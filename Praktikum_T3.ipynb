{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g0yFQwReM5p"
      },
      "source": [
        "#Creating Directories and Unzipping Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b5erE2TeHDt"
      },
      "outputs": [],
      "source": [
        "!mkdir dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEoWDg5DeMi0"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/drive/MyDrive/Praktikum/train_phase2\" -d \"/content/content_data/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgS2oemXeSxy"
      },
      "source": [
        "#Installing Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWUY2JVJeUvT"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUVO_b2MeaTT",
        "outputId": "4df3953e-10e4-46d2-9f92-1f03e15417ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Collecting pydicom\n",
            "  Downloading pydicom-2.4.4-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.19.3)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.34.1)\n",
            "Collecting pylibjpeg\n",
            "  Downloading pylibjpeg-2.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting pylibjpeg-libjpeg\n",
            "  Downloading pylibjpeg_libjpeg-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pylibjpeg-openjpeg\n",
            "  Downloading pylibjpeg_openjpeg-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.3)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (10.3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2024.5.22)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Installing collected packages: pylibjpeg-openjpeg, pylibjpeg-libjpeg, pylibjpeg, pydicom\n",
            "Successfully installed pydicom-2.4.4 pylibjpeg-2.0.0 pylibjpeg-libjpeg-2.1.0 pylibjpeg-openjpeg-2.2.1\n"
          ]
        }
      ],
      "source": [
        "# Install the required packages\n",
        "!pip install numpy pandas pydicom scikit-image imageio pylibjpeg pylibjpeg-libjpeg pylibjpeg-openjpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2p3JqDS9Inb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6Aj6YBd8JZQ"
      },
      "source": [
        "#Prep of the Folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WeOoXaR9jbW"
      },
      "source": [
        "ZIPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EZ8zshL9bQ8"
      },
      "outputs": [],
      "source": [
        "# import zipfile\n",
        "# import os\n",
        "# from google.colab import files\n",
        "# from typing import List\n",
        "\n",
        "# def combine_and_extract_zip_files(zip_files: List[str], extract_to: str) -> List[str]:\n",
        "#     \"\"\"\n",
        "#     Upload and extract multiple ZIP files into a single directory.\n",
        "#     Collect all DICOM file paths from the extracted directories.\n",
        "\n",
        "#     Parameters:\n",
        "#     - zip_files: List of ZIP file paths to be extracted.\n",
        "#     - extract_to: Directory where the ZIP files will be extracted.\n",
        "\n",
        "#     Returns:\n",
        "#     - List of all DICOM file paths.\n",
        "#     \"\"\"\n",
        "#     all_dicom_files = []\n",
        "\n",
        "#     # Extract each ZIP file\n",
        "#     for zip_file in zip_files:\n",
        "#         with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "#             zip_ref.extractall(extract_to)\n",
        "\n",
        "#     # Collect all DICOM files from the extracted directories\n",
        "#     for subdir, _, files in os.walk(extract_to):\n",
        "#         for file in files:\n",
        "#             if file.lower().endswith('.dcm'):\n",
        "#                 all_dicom_files.append(os.path.join(subdir, file))\n",
        "\n",
        "#     return all_dicom_files\n",
        "\n",
        "# # Upload the two ZIP files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# # Define the directory to extract the ZIP files to\n",
        "# extract_to = '/content/Breast-Cancer-Screening-DBT'\n",
        "\n",
        "# # Get the list of uploaded ZIP file paths\n",
        "# zip_file_paths = list(uploaded.keys())\n",
        "\n",
        "# # Combine and extract the ZIP files, and collect all DICOM file paths\n",
        "# dicom_files = combine_and_extract_zip_files(zip_file_paths, extract_to)\n",
        "\n",
        "# # Print the number of DICOM files found\n",
        "# print(f\"Found {len(dicom_files)} DICOM files.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-6p0ueD9h17"
      },
      "source": [
        "1 ZIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyqS8URN8I2u"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# import zipfile\n",
        "# import os\n",
        "\n",
        "# # Upload ZIP file\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# # Extract ZIP file\n",
        "# for filename in uploaded.keys():\n",
        "#     with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "#         zip_ref.extractall('/content/Breast-Cancer-Screening-DBT')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cNxQdAGBE4C"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNJJrNMg8URp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def collect_dicom_files(root_dir: str) -> List[str]:\n",
        "    \"\"\"Collect all DICOM file paths from the directory structure.\"\"\"\n",
        "    dicom_files = []\n",
        "    for subdir, _, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.dcm'):\n",
        "                dicom_files.append(os.path.join(subdir, file))\n",
        "    return dicom_files\n",
        "\n",
        "# Collect all DICOM file paths\n",
        "dicom_files = collect_dicom_files('/content/Breast-Cancer-Screening-DBT')\n",
        "print(f\"Found {len(dicom_files)} DICOM files.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw1ivKuV8YUr"
      },
      "source": [
        "#Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwsiEBtp8XhK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "d0cbb7a5-4d45-4213-8fdc-ea343260573c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-3-43041724f6be>, line 35)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-43041724f6be>\"\u001b[0;36m, line \u001b[0;32m35\u001b[0m\n\u001b[0;31m    if not all([key in df_boxes.columns for key in primary_key]):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "from typing import AnyStr, BinaryIO, Dict, List, NamedTuple, Optional, Union\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydicom as dicom\n",
        "from skimage.exposure import rescale_intensity\n",
        "import imageio\n",
        "\n",
        "def dcmread_image(fp: Union[str, os.PathLike[AnyStr], BinaryIO], view: str, index: Optional[np.uint] = None) -> np.ndarray:\n",
        "    \"\"\"Read pixel array from DBT DICOM file.\"\"\"\n",
        "    ds = dicom.dcmread(fp)\n",
        "    ds.decompress(handler_name=\"pylibjpeg\")  # Use pylibjpeg for decompression\n",
        "    pixel_array = ds.pixel_array\n",
        "    view_laterality = view[0].upper()\n",
        "    image_laterality = _get_image_laterality(pixel_array[index or 0])\n",
        "\n",
        "    if index is not None:\n",
        "        pixel_array = pixel_array[index]\n",
        "\n",
        "    if not image_laterality == view_laterality:\n",
        "        pixel_array = np.flip(pixel_array, axis=(-1, -2))\n",
        "\n",
        "    window_center = _get_window_center(ds)\n",
        "    window_width = _get_window_width(ds)\n",
        "    low = (2 * window_center - window_width) / 2\n",
        "    high = (2 * window_center + window_width) / 2\n",
        "    pixel_array = rescale_intensity(pixel_array, in_range=(low, high), out_range=\"dtype\")\n",
        "\n",
        "    return pixel_array\n",
        "\n",
        "def read_boxes(boxes_fp: str, filepaths_fp: str) -> pd.DataFrame:\n",
        "    \"\"\"Read pandas DataFrame with bounding boxes joined with file paths.\"\"\"\n",
        "    df_boxes = pd.read_csv(boxes_fp)\n",
        "    df_filepaths = pd.read_csv(filepaths_fp)\n",
        "    primary_key = (\"PatientID\", \"StudyUID\", \"View\")\n",
        "        if not all([key in df_boxes.columns for key in primary_key]):\n",
        "        raise AssertionError(f\"Not all primary key columns {primary_key} are present in bounding boxes columns {df_boxes.columns}\")\n",
        "\n",
        "    if not all([key in df_filepaths.columns for key in primary_key]):\n",
        "        raise AssertionError(f\"Not all primary key columns {primary_key} are present in file paths columns {df_filepaths.columns}\")\n",
        "\n",
        "    return pd.merge(df_boxes, df_filepaths, on=primary_key)\n",
        "\n",
        "def draw_box(image: np.ndarray, x: int, y: int, width: int, height: int, color: Optional[Union[int, tuple]] = None, lw=4) -> np.ndarray:\n",
        "    \"\"\"Draw bounding box on the image.\"\"\"\n",
        "    x = min(max(x, 0), image.shape[1] - 1)\n",
        "    y = min(max(y, 0), image.shape[0] - 1)\n",
        "\n",
        "    if color is None:\n",
        "        color = 255\n",
        "\n",
        "    if len(image.shape) > 2 and not hasattr(color, \"__len__\"):\n",
        "        color = (color,) + (0,) * (image.shape[-1] - 1)\n",
        "\n",
        "    image[y : y + lw, x : x + width] = color\n",
        "    image[y + height - lw : y + height, x : x + width] = color\n",
        "    image[y : y + height, x : x + lw] = color\n",
        "    image[y : y + height, x + width - lw : x + width] = color\n",
        "\n",
        "    return image\n",
        "\n",
        "def evaluate(labels_fp: str, boxes_fp: str, predictions_fp: str) -> Dict[str, float]:\n",
        "    \"\"\"Evaluate predictions.\"\"\"\n",
        "    df_labels = pd.read_csv(labels_fp)\n",
        "    df_boxes = pd.read_csv(boxes_fp, dtype={\"VolumeSlices\": float})\n",
        "    df_pred = pd.read_csv(predictions_fp, dtype={\"Score\": float})\n",
        "\n",
        "    df_labels = df_labels.reset_index().set_index([\"StudyUID\", \"View\"]).sort_index()\n",
        "    df_boxes = df_boxes.reset_index().set_index([\"StudyUID\", \"View\"]).sort_index()\n",
        "    df_pred = df_pred.reset_index().set_index([\"StudyUID\", \"View\"]).sort_index()\n",
        "\n",
        "    df_pred[\"TP\"] = 0\n",
        "    df_pred[\"GTID\"] = -1\n",
        "\n",
        "    thresholds = [df_pred[\"Score\"].max() + 1.0]\n",
        "\n",
        "    for box_pred in df_pred.itertuples():\n",
        "        if box_pred.Index not in df_boxes.index:\n",
        "            continue\n",
        "\n",
        "        df_boxes_view = df_boxes.loc[[box_pred.Index]]\n",
        "        view_slice_offset = df_boxes.loc[[box_pred.Index], \"VolumeSlices\"].iloc[0] / 4\n",
        "        tp_boxes = [b for b in df_boxes_view.itertuples() if _is_tp(box_pred, b, slice_offset=view_slice_offset)]\n",
        "\n",
        "        if len(tp_boxes) > 1:\n",
        "            tp_distances = [_distance(box_pred, b) for b in tp_boxes]\n",
        "            tp_boxes = [tp_boxes[np.argmin(tp_distances)]]\n",
        "\n",
        "        if len(tp_boxes) > 0:\n",
        "            tp_i = tp_boxes[0].index\n",
        "            df_pred.loc[df_pred[\"index\"] == box_pred.index, (\"TP\", \"GTID\")] = (1, tp_i)\n",
        "            thresholds.append(box_pred.Score)\n",
        "\n",
        "    thresholds.append(df_pred[\"Score\"].min() - 1.0)\n",
        "\n",
        "    evaluation_fps_all = (2.0,)\n",
        "    tpr_all = _froc(df_pred=df_pred, thresholds=thresholds, n_volumes=len(df_labels), n_boxes=len(df_boxes), evaluation_fps=evaluation_fps_all)\n",
        "    result = {f\"sensitivity_at_2_fps_all\": tpr_all[0]}\n",
        "\n",
        "    df_pred = df_pred[df_pred.index.isin(df_boxes.index)]\n",
        "    df_labels = df_labels[df_labels.index.isin(df_boxes.index)]\n",
        "    evaluation_fps_positive = (1.0, 2.0, 3.0, 4.0)\n",
        "    tpr_positive = _froc(df_pred=df_pred, thresholds=thresholds, n_volumes=len(df_labels), n_boxes=len(df_boxes), evaluation_fps=evaluation_fps_positive)\n",
        "\n",
        "    result.update(dict((f\"sensitivity_at_{int(x)}_fps_positive\", y) for x, y in zip(evaluation_fps_positive, tpr_positive)))\n",
        "    result.update({\"mean_sensitivity_positive\": np.mean(tpr_positive)})\n",
        "\n",
        "    return result\n",
        "\n",
        "def _froc(df_pred: pd.DataFrame, thresholds: List[float], n_volumes: int, n_boxes: int, evaluation_fps: tuple) -> List[float]:\n",
        "    \"\"\"Free-response receiver operating characteristic (FROC) calculation.\"\"\"\n",
        "    tpr = []\n",
        "    fps = []\n",
        "\n",
        "    for th in sorted(thresholds, reverse=True):\n",
        "        df_th = df_pred.loc[df_pred[\"Score\"] >= th]\n",
        "        df_th_unique_tp = df_th.reset_index().drop_duplicates(subset=[\"StudyUID\", \"View\", \"TP\", \"GTID\"])\n",
        "        n_tps_th = float(sum(df_th_unique_tp[\"TP\"]))\n",
        "        tpr_th = n_tps_th / n_boxes\n",
        "        n_fps_th = float(len(df_th[df_th[\"TP\"] == 0]))\n",
        "        fps_th = n_fps_th / n_volumes\n",
        "        tpr.append(tpr_th)\n",
        "        fps.append(fps_th)\n",
        "\n",
        "        if fps_th > max(evaluation_fps):\n",
        "            break\n",
        "\n",
        "    return [np.interp(x, fps, tpr) for x in evaluation_fps]\n",
        "\n",
        "def _is_tp(box_pred: NamedTuple, box_true: NamedTuple, slice_offset: int, min_dist: int = 100) -> bool:\n",
        "    \"\"\"Determine if a prediction is a true positive.\"\"\"\n",
        "    pred_y = box_pred.Y + box_pred.Height / 2\n",
        "    pred_x = box_pred.X + box_pred.Width / 2\n",
        "    pred_z = box_pred.Z + box_pred.Depth / 2\n",
        "    true_y = box_true.Y + box_true.Height / 2\n",
        "    true_x = box_true.X + box_true.Width / 2\n",
        "    true_z = box_true.Slice\n",
        "\n",
        "    dist = np.linalg.norm((pred_x - true_x, pred_y - true_y))\n",
        "    dist_threshold = np.sqrt(box_true.Width ** 2 + box_true.Height ** 2) / 2.0\n",
        "    dist_threshold = max(dist_threshold, min_dist)\n",
        "    slice_diff = np.abs(pred_z - true_z)\n",
        "\n",
        "    return dist <= dist_threshold and slice_diff <= slice_offset\n",
        "\n",
        "def _distance(box_pred: NamedTuple, box_true: NamedTuple) -> float:\n",
        "    \"\"\"Calculate the Euclidean distance between predicted and true box centers.\"\"\"\n",
        "    pred_y = box_pred.Y + box_pred.Height / 2\n",
        "    pred_x = box_pred.X + box_pred.Width / 2\n",
        "    pred_z = box_pred.Z + box_pred.Depth / 2\n",
        "    true_y = box_true.Y + box_true.Height / 2\n",
        "    true_x = box_true.X + box_true.Width / 2\n",
        "    true_z = box_true.Slice\n",
        "\n",
        "    return np.linalg.norm((pred_x - true_x, pred_y - true_y, pred_z - true_z))\n",
        "\n",
        "def _get_dicom_laterality(ds: dicom.dataset.FileDataset) -> str:\n",
        "    \"\"\"Get laterality from DICOM metadata (unreliable).\"\"\"\n",
        "    return ds[0x5200, 0x9229][0][0x0020, 0x9071][0][0x0020, 0x9072].value\n",
        "\n",
        "def _get_image_laterality(pixel_array: np.ndarray) -> str:\n",
        "    \"\"\"Determine laterality based on image pixel intensity.\"\"\"\n",
        "    left_edge = np.sum(pixel_array[:, 0])  # sum of left edge pixels\n",
        "    right_edge = np.sum(pixel_array[:, -1])  # sum of right edge pixels\n",
        "    return \"R\" if left_edge < right_edge else \"L\"\n",
        "\n",
        "def _get_window_center(ds: dicom.dataset.FileDataset) -> np.float32:\n",
        "    \"\"\"Get window center from DICOM metadata.\"\"\"\n",
        "    return np.float32(ds[0x5200, 0x9229][0][0x0028, 0x9132][0][0x0028, 0x1050].value)\n",
        "\n",
        "def _get_window_width(ds: dicom.dataset.FileDataset) -> np.float32:\n",
        "    \"\"\"Get window width from DICOM metadata.\"\"\"\n",
        "    return np.float32(ds[0x5200, 0x9229][0][0x0028, 0x9132][0][0x0028, 0x1051].value)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c37OZyuG8g7-"
      },
      "source": [
        "#Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2WRbVQH8f_K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def process_dicom_files(dicom_files: List[str], boxes_csv_path: str, filepaths_csv_path: str, labels_csv_path: str, predictions_csv_path: str):\n",
        "    # Read bounding boxes and file paths\n",
        "    df = read_boxes(boxes_csv_path, filepaths_csv_path)\n",
        "\n",
        "    for dicom_file_path in dicom_files:\n",
        "        # Extract the view from the file path or use a default view\n",
        "        view = 'LCC'  # Example view; adjust as needed\n",
        "\n",
        "        # Read and process the DICOM file\n",
        "        image_array = dcmread_image(dicom_file_path, view)\n",
        "\n",
        "        # Draw a bounding box on the image (example coordinates)\n",
        "        x, y, width, height = 50, 50, 100, 100  # Replace with actual coordinates if available\n",
        "        image_with_box = draw_box(image_array, x, y, width, height)\n",
        "\n",
        "        # Save the image with the bounding box\n",
        "        output_image_path = os.path.splitext(dicom_file_path)[0] + '_output.png'\n",
        "        imageio.imwrite(output_image_path, image_with_box)\n",
        "\n",
        "        # Evaluate predictions\n",
        "        if labels_csv_path and predictions_csv_path:\n",
        "            evaluation_results = evaluate(labels_csv_path, boxes_csv_path, predictions_csv_path)\n",
        "            print(f\"Evaluation results for {dicom_file_path}: {evaluation_results}\")\n",
        "\n",
        "# Define file paths\n",
        "boxes_csv_path = '/content/Breast-Cancer-Screening-DBT/boxes.csv'\n",
        "filepaths_csv_path = '/content/Breast-Cancer-Screening-DBT/filepaths.csv'\n",
        "labels_csv_path = '/content/Breast-Cancer-Screening-DBT/labels.csv'\n",
        "predictions_csv_path = '/content/Breast-Cancer-Screening-DBT/predictions.csv'\n",
        "\n",
        "# Process the DICOM files\n",
        "process_dicom_files(dicom_files, boxes_csv_path, filepaths_csv_path, labels_csv_path, predictions_csv_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLYbtnX1Bvx5"
      },
      "source": [
        "#Combined Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfBVlEV7BtE6",
        "outputId": "e5d9dec2-e986-44d1-819b-7459629abb72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 200 DICOM files.\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the function to combine and extract ZIP files\n",
        "import zipfile\n",
        "import os\n",
        "from typing import List\n",
        "\n",
        "def combine_and_extract_zip_files(zip_files: List[str], extract_to: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Extract multiple ZIP files into a single directory.\n",
        "    Collect all DICOM file paths from the extracted directories.\n",
        "\n",
        "    Parameters:\n",
        "    - zip_files: List of ZIP file paths to be extracted.\n",
        "    - extract_to: Directory where the ZIP files will be extracted.\n",
        "\n",
        "    Returns:\n",
        "    - List of all DICOM file paths.\n",
        "    \"\"\"\n",
        "    all_dicom_files = []\n",
        "\n",
        "    # Extract each ZIP file\n",
        "    for zip_file in zip_files:\n",
        "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "\n",
        "    # Collect all DICOM files from the extracted directories\n",
        "    for subdir, _, files in os.walk(extract_to):\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.dcm'):\n",
        "                all_dicom_files.append(os.path.join(subdir, file))\n",
        "\n",
        "    return all_dicom_files\n",
        "\n",
        "\n",
        "# Define the directory to extract the ZIP files to\n",
        "extract_to = '/content/dataset/'\n",
        "\n",
        "# List of ZIP file paths on Google Drive\n",
        "zip_file_paths = [\n",
        "    '/content/drive/MyDrive/Praktikum/Breast-Cancer-Screening-DBT_before4000.zip',\n",
        "    '/content/drive/MyDrive/Praktikum/Breast-Cancer-Screening-DBT.zip'\n",
        "]\n",
        "\n",
        "# Combine and extract the ZIP files, and collect all DICOM file paths\n",
        "dicom_files = combine_and_extract_zip_files(zip_file_paths, extract_to)\n",
        "\n",
        "# Print the number of DICOM files found\n",
        "print(f\"Found {len(dicom_files)} DICOM files.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMrdNr5qHcmo"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define DICOM processing functions\n",
        "from typing import AnyStr, BinaryIO, Dict, List, NamedTuple, Optional, Union\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydicom as dicom\n",
        "from skimage.exposure import rescale_intensity\n",
        "import imageio\n",
        "\n",
        "def dcmread_image(fp: Union[str, os.PathLike[AnyStr], BinaryIO], view: str, index: Optional[np.uint] = None) -> np.ndarray:\n",
        "    \"\"\"Read pixel array from DBT DICOM file.\"\"\"\n",
        "    ds = dicom.dcmread(fp)\n",
        "    ds.decompress(handler_name=\"pylibjpeg\")  # Use pylibjpeg for decompression\n",
        "    pixel_array = ds.pixel_array\n",
        "    view_laterality = view[0].upper()\n",
        "    image_laterality = _get_image_laterality(pixel_array[index or 0])\n",
        "\n",
        "    if index is not None:\n",
        "        pixel_array = pixel_array[index]\n",
        "\n",
        "    if not image_laterality == view_laterality:\n",
        "        pixel_array = np.flip(pixel_array, axis=(-1, -2))\n",
        "\n",
        "    window_center = _get_window_center(ds)\n",
        "    window_width = _get_window_width(ds)\n",
        "    low = (2 * window_center - window_width) / 2\n",
        "    high = (2 * window_center + window_width) / 2\n",
        "    pixel_array = rescale_intensity(pixel_array, in_range=(low, high), out_range=\"dtype\")\n",
        "\n",
        "    return pixel_array\n",
        "\n",
        "def read_boxes(boxes_fp: str, filepaths_fp: str) -> pd.DataFrame:\n",
        "    \"\"\"Read pandas DataFrame with bounding boxes joined with file paths.\"\"\"\n",
        "    df_boxes = pd.read_csv(boxes_fp)\n",
        "    df_filepaths = pd.read_csv(filepaths_fp)\n",
        "    primary_key = (\"PatientID\", \"StudyUID\", \"View\")\n",
        "\n",
        "    if not all([key in df_boxes.columns for key in primary_key]):\n",
        "        raise AssertionError(f\"Not all primary key columns {primary_key} are present in bounding boxes columns {df_boxes.columns}\")\n",
        "\n",
        "    if not all([key in df_filepaths.columns for key in primary_key]):\n",
        "        raise AssertionError(f\"Not all primary key columns {primary_key} are present in file paths columns {df_filepaths.columns}\")\n",
        "\n",
        "    return pd.merge(df_boxes, df_filepaths, on=primary_key)\n",
        "\n",
        "def draw_box(image: np.ndarray, x: int, y: int, width: int, height: int, color: Optional[Union[int, tuple]] = None, lw=4) -> np.ndarray:\n",
        "    \"\"\"Draw bounding box on the image.\"\"\"\n",
        "    x = min(max(x, 0), image.shape[1] - 1)\n",
        "    y = min(max(y, 0), image.shape[0] - 1)\n",
        "\n",
        "    if color is None:\n",
        "        color = 255\n",
        "\n",
        "    if len(image.shape) > 2 and not hasattr(color, \"__len__\"):\n",
        "        color = (color,) + (0,) * (image.shape[-1] - 1)\n",
        "\n",
        "    image[y : y + lw, x : x + width] = color\n",
        "    image[y + height - lw : y + height, x : x + width] = color\n",
        "    image[y : y + height, x : x + lw] = color\n",
        "    image[y : y + height, x + width - lw : x + width] = color\n",
        "\n",
        "    return image\n",
        "\n",
        "def evaluate(labels_fp: str, boxes_fp: str, predictions_fp: str) -> Dict[str, float]:\n",
        "    \"\"\"Evaluate predictions.\"\"\"\n",
        "    df_labels = pd.read_csv(labels_fp)\n",
        "    df_boxes = pd.read_csv(boxes_fp, dtype={\"VolumeSlices\": float})\n",
        "    df_pred = pd.read_csv(predictions_fp, dtype={\"Score\": float})\n",
        "\n",
        "    df_labels = df_labels.reset_index().set_index([\"StudyUID\", \"View\"]).sort_index()\n",
        "    df_boxes = df_boxes.reset_index().set_index([\"StudyUID\", \"View\"]).sort_index()\n",
        "    df_pred = df_pred.reset_index().set_index([\"StudyUID\", \"View\"]).sort_index()\n",
        "\n",
        "    df_pred[\"TP\"] = 0\n",
        "    df_pred[\"GTID\"] = -1\n",
        "\n",
        "    thresholds = [df_pred[\"Score\"].max() + 1.0]\n",
        "\n",
        "    for box_pred in df_pred.itertuples():\n",
        "        if box_pred.Index not in df_boxes.index:\n",
        "            continue\n",
        "\n",
        "        df_boxes_view = df_boxes.loc[[box_pred.Index]]\n",
        "        view_slice_offset = df_boxes.loc[[box_pred.Index], \"VolumeSlices\"].iloc[0] / 4\n",
        "        tp_boxes = [b for b in df_boxes_view.itertuples() if _is_tp(box_pred, b, slice_offset=view_slice_offset)]\n",
        "\n",
        "        if len(tp_boxes) > 1:\n",
        "            tp_distances = [_distance(box_pred, b) for b in tp_boxes]\n",
        "            tp_boxes = [tp_boxes[np.argmin(tp_distances)]]\n",
        "\n",
        "        if len(tp_boxes) > 0:\n",
        "            tp_i = tp_boxes[0].index\n",
        "            df_pred.loc[df_pred[\"index\"] == box_pred.index, (\"TP\", \"GTID\")] = (1, tp_i)\n",
        "            thresholds.append(box_pred.Score)\n",
        "\n",
        "    thresholds.append(df_pred[\"Score\"].min() - 1.0)\n",
        "\n",
        "    evaluation_fps_all = (2.0,)\n",
        "    tpr_all = _froc(df_pred=df_pred, thresholds=thresholds, n_volumes=len(df_labels), n_boxes=len(df_boxes), evaluation_fps=evaluation_fps_all)\n",
        "    result = {f\"sensitivity_at_2_fps_all\": tpr_all[0]}\n",
        "\n",
        "    df_pred = df_pred[df_pred.index.isin(df_boxes.index)]\n",
        "    df_labels = df_labels[df_labels.index.isin(df_boxes.index)]\n",
        "    evaluation_fps_positive = (1.0, 2.0, 3.0, 4.0)\n",
        "    tpr_positive = _froc(df_pred=df_pred, thresholds=thresholds, n_volumes=len(df_labels), n_boxes=len(df_boxes), evaluation_fps=evaluation_fps_positive)\n",
        "\n",
        "    result.update(dict((f\"sensitivity_at_{int(x)}_fps_positive\", y) for x, y in zip(evaluation_fps_positive, tpr_positive)))\n",
        "    result.update({\"mean_sensitivity_positive\": np.mean(tpr_positive)})\n",
        "\n",
        "    return result\n",
        "\n",
        "def _froc(df_pred: pd.DataFrame, thresholds: List[float], n_volumes: int, n_boxes: int, evaluation_fps: tuple) -> List[float]:\n",
        "    \"\"\"Free-response receiver operating characteristic (FROC) calculation.\"\"\"\n",
        "    tpr = []\n",
        "    fps = []\n",
        "\n",
        "    for th in sorted(thresholds, reverse=True):\n",
        "        df_th = df_pred.loc[df_pred[\"Score\"] >= th]\n",
        "        df_th_unique_tp = df_th.reset_index().drop_duplicates(subset=[\"StudyUID\", \"View\", \"TP\", \"GTID\"])\n",
        "        n_tps_th = float(sum(df_th_unique_tp[\"TP\"]))\n",
        "        tpr_th = n_tps_th / n_boxes\n",
        "        n_fps_th = float(len(df_th[df_th[\"TP\"] == 0]))\n",
        "        fps_th = n_fps_th / n_volumes\n",
        "        tpr.append(tpr_th)\n",
        "        fps.append(fps_th)\n",
        "\n",
        "        if fps_th > max(evaluation_fps):\n",
        "            break\n",
        "\n",
        "    return [np.interp(x, fps, tpr) for x in evaluation_fps]\n",
        "\n",
        "def _is_tp(box_pred: NamedTuple, box_true: NamedTuple, slice_offset: int, min_dist: int = 100) -> bool:\n",
        "    \"\"\"Determine if a prediction is a true positive.\"\"\"\n",
        "    pred_y = box_pred.Y + box_pred.Height / 2\n",
        "    pred_x = box_pred.X + box_pred.Width / 2\n",
        "    pred_z = box_pred.Z + box_pred.Depth / 2\n",
        "    true_y = box_true.Y + box_true.Height / 2\n",
        "    true_x = box_true.X + box_true.Width / 2\n",
        "    true_z = box_true.Slice\n",
        "\n",
        "    dist = np.linalg.norm((pred_x - true_x, pred_y - true_y))\n",
        "    dist_threshold = np.sqrt(box_true.Width ** 2 + box_true.Height ** 2) / 2.0\n",
        "    dist_threshold = max(dist_threshold, min_dist)\n",
        "    slice_diff = np.abs(pred_z - true_z)\n",
        "\n",
        "    return dist <= dist_threshold and slice_diff <= slice_offset\n",
        "\n",
        "def _distance(box_pred: NamedTuple, box_true: NamedTuple) -> float:\n",
        "    \"\"\"Calculate the Euclidean distance between predicted and true box centers.\"\"\"\n",
        "    pred_y = box_pred.Y + box_pred.Height / 2\n",
        "    pred_x = box_pred.X + box_pred.Width / 2\n",
        "    pred_z = box_pred.Z + box_pred.Depth / 2\n",
        "    true_y = box_true.Y + box_true.Height / 2\n",
        "    true_x = box_true.X + box_true.Width / 2\n",
        "    true_z = box_true.Slice\n",
        "\n",
        "    return np.linalg.norm((pred_x - true_x, pred_y - true_y, pred_z - true_z))\n",
        "\n",
        "def _get_dicom_laterality(ds: dicom.dataset.FileDataset) -> str:\n",
        "    \"\"\"Get laterality from DICOM metadata (unreliable).\"\"\"\n",
        "    return ds[0x5200, 0x9229][0][0x0020, 0x9071][0][0x0020, 0x9072].value\n",
        "\n",
        "def _get_image_laterality(pixel_array: np.ndarray) -> str:\n",
        "    \"\"\"Determine laterality based on image pixel intensity.\"\"\"\n",
        "    left_edge = np.sum(pixel_array[:, 0])  # sum of left edge pixels\n",
        "    right_edge = np.sum(pixel_array[:, -1])  # sum of right edge pixels\n",
        "    if left_edge < right_edge:\n",
        "      return \"R\"\n",
        "    else:\n",
        "      return \"L\"\n",
        "\n",
        "def _get_window_center(ds: dicom.dataset.FileDataset) -> np.float32:\n",
        "    \"\"\"Get window center from DICOM metadata.\"\"\"\n",
        "    return np.float32(ds[0x5200, 0x9229][0][0x0028, 0x9132][0][0x0028, 0x1050].value)\n",
        "\n",
        "def _get_window_width(ds: dicom.dataset.FileDataset) -> np.float32:\n",
        "    \"\"\"Get window width from DICOM metadata.\"\"\"\n",
        "    return np.float32(ds[0x5200, 0x9229][0][0x0028, 0x9132][0][0x0028, 0x1051].value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TixjkuRrD0KB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "53b6e9a2-ab21-477c-8e25-e94c5d10dd31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/openjpeg/utils.py:314: UserWarning: The (0028,0101) Bits Stored value '10' in the dataset does not match the component precision value '16' found in the JPEG 2000 data. It's recommended that you change the Bits Stored value to produce the correct output\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b49573bbd5b3>\u001b[0m in \u001b[0;36m<cell line: 212>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;31m# Process the DICOM files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m \u001b[0mprocess_dicom_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdicom_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes_csv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepaths_csv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_csv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_csv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-b49573bbd5b3>\u001b[0m in \u001b[0;36mprocess_dicom_files\u001b[0;34m(dicom_files, boxes_csv_path, filepaths_csv_path, labels_csv_path, predictions_csv_path)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# Read and process the DICOM file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mimage_array_3d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdcmread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdicom_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# Select a slice (e.g., the middle slice)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-b49573bbd5b3>\u001b[0m in \u001b[0;36mdcmread_image\u001b[0;34m(fp, view, index)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mlow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwindow_center\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwindow_width\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mhigh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwindow_center\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwindow_width\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mpixel_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrescale_intensity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpixel_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skimage/exposure/exposure.py\u001b[0m in \u001b[0;36mrescale_intensity\u001b[0;34m(image, in_range, out_range)\u001b[0m\n\u001b[1;32m    592\u001b[0m         )\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimin\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mimax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2168\u001b[0m     \"\"\"\n\u001b[0;32m-> 2169\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_clip\u001b[0;34m(a, min, max, out, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from typing import AnyStr, BinaryIO, Dict, List, NamedTuple, Optional, Union\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydicom as dicom\n",
        "from skimage.exposure import rescale_intensity\n",
        "import imageio\n",
        "\n",
        "def dcmread_image(fp: Union[str, os.PathLike[AnyStr], BinaryIO], view: str, index: Optional[np.uint] = None) -> np.ndarray:\n",
        "    \"\"\"Read pixel array from DBT DICOM file.\"\"\"\n",
        "    ds = dicom.dcmread(fp)\n",
        "    ds.decompress(handler_name=\"pylibjpeg\")  # Use pylibjpeg for decompression\n",
        "    pixel_array = ds.pixel_array\n",
        "    view_laterality = view[0].upper()\n",
        "    image_laterality = _get_image_laterality(pixel_array[0])\n",
        "\n",
        "    if index is not None:\n",
        "        pixel_array = pixel_array[index]\n",
        "\n",
        "    if not image_laterality == view_laterality:\n",
        "        pixel_array = np.flip(pixel_array, axis=(-1, -2))\n",
        "\n",
        "    window_center = _get_window_center(ds)\n",
        "    window_width = _get_window_width(ds)\n",
        "    low = (2 * window_center - window_width) / 2\n",
        "    high = (2 * window_center + window_width) / 2\n",
        "    pixel_array = rescale_intensity(pixel_array, in_range=(low, high), out_range=\"dtype\")\n",
        "\n",
        "    return pixel_array\n",
        "\n",
        "def read_boxes(boxes_fp: str, filepaths_fp: str) -> pd.DataFrame:\n",
        "    \"\"\"Read pandas DataFrame with bounding boxes joined with file paths.\"\"\"\n",
        "    df_boxes = pd.read_csv(boxes_fp)\n",
        "    df_filepaths = pd.read_csv(filepaths_fp)\n",
        "    primary_key = (\"PatientID\", \"StudyUID\", \"View\")\n",
        "\n",
        "    if not all([key in df_boxes.columns for key in primary_key]):\n",
        "        raise AssertionError(f\"Not all primary key columns {primary_key} are present in bounding boxes columns {df_boxes.columns}\")\n",
        "\n",
        "    if not all([key in df_filepaths.columns for key in primary_key]):\n",
        "        raise AssertionError(f\"Not all primary key columns {primary_key} are present in file paths columns {df_filepaths.columns}\")\n",
        "\n",
        "    return pd.merge(df_boxes, df_filepaths, on=primary_key)\n",
        "\n",
        "def draw_box(image: np.ndarray, x: int, y: int, width: int, height: int, color: Optional[Union[int, tuple]] = None, lw=4) -> np.ndarray:\n",
        "    \"\"\"Draw bounding box on the image.\"\"\"\n",
        "    x = min(max(x, 0), image.shape[1] - 1)\n",
        "    y = min(max(y, 0), image.shape[0] - 1)\n",
        "\n",
        "    if color is None:\n",
        "        color = 255\n",
        "\n",
        "    if len(image.shape) > 2 and not hasattr(color, \"__len__\"):\n",
        "        color = (color,) + (0,) * (image.shape[-1] - 1)\n",
        "\n",
        "    image[y : y + lw, x : x + width] = color\n",
        "    image[y + height - lw : y + height, x : x + width] = color\n",
        "    image[y : y + height, x : x + lw] = color\n",
        "    image[y : y + height, x + width - lw : x + width] = color\n",
        "\n",
        "    return image\n",
        "\n",
        "def evaluate(labels_fp: str, boxes_fp: str, predictions_fp: str) -> Dict[str, float]:\n",
        "    \"\"\"Evaluate predictions.\"\"\"\n",
        "    df_labels = pd.read_csv(labels_fp)\n",
        "    df_boxes = pd.read_csv(boxes_fp, dtype={\"VolumeSlices\": float})\n",
        "    df_pred = pd.read_csv(predictions_fp, dtype={\"Score\": float})\n",
        "\n",
        "    df_labels = df_labels.reset_index().set_index([\"StudyUID\", \"View\"]).sort_index()\n",
        "    df_boxes = df_boxes.reset_index().set_index([\"StudyUID\", \"View\"]).sort_index()\n",
        "    df_pred = df_pred.reset_index().set_index([\"StudyUID\", \"View\"]).sort_index()\n",
        "\n",
        "    df_pred[\"TP\"] = 0\n",
        "    df_pred[\"GTID\"] = -1\n",
        "\n",
        "    thresholds = [df_pred[\"Score\"].max() + 1.0]\n",
        "\n",
        "    for box_pred in df_pred.itertuples():\n",
        "        if box_pred.Index not in df_boxes.index:\n",
        "            continue\n",
        "\n",
        "        df_boxes_view = df_boxes.loc[[box_pred.Index]]\n",
        "        view_slice_offset = df_boxes.loc[[box_pred.Index], \"VolumeSlices\"].iloc[0] / 4\n",
        "        tp_boxes = [b for b in df_boxes_view.itertuples() if _is_tp(box_pred, b, slice_offset=view_slice_offset)]\n",
        "\n",
        "        if len(tp_boxes) > 1:\n",
        "            tp_distances = [_distance(box_pred, b) for b in tp_boxes]\n",
        "            tp_boxes = [tp_boxes[np.argmin(tp_distances)]]\n",
        "\n",
        "        if len(tp_boxes) > 0:\n",
        "            tp_i = tp_boxes[0].index\n",
        "            df_pred.loc[df_pred[\"index\"] == box_pred.index, (\"TP\", \"GTID\")] = (1, tp_i)\n",
        "            thresholds.append(box_pred.Score)\n",
        "\n",
        "    thresholds.append(df_pred[\"Score\"].min() - 1.0)\n",
        "\n",
        "    evaluation_fps_all = (2.0,)\n",
        "    tpr_all = _froc(df_pred=df_pred, thresholds=thresholds, n_volumes=len(df_labels), n_boxes=len(df_boxes), evaluation_fps=evaluation_fps_all)\n",
        "    result = {f\"sensitivity_at_2_fps_all\": tpr_all[0]}\n",
        "\n",
        "    df_pred = df_pred[df_pred.index.isin(df_boxes.index)]\n",
        "    df_labels = df_labels[df_labels.index.isin(df_boxes.index)]\n",
        "    evaluation_fps_positive = (1.0, 2.0, 3.0, 4.0)\n",
        "    tpr_positive = _froc(df_pred=df_pred, thresholds=thresholds, n_volumes=len(df_labels), n_boxes=len(df_boxes), evaluation_fps=evaluation_fps_positive)\n",
        "\n",
        "    result.update(dict((f\"sensitivity_at_{int(x)}_fps_positive\", y) for x, y in zip(evaluation_fps_positive, tpr_positive)))\n",
        "    result.update({\"mean_sensitivity_positive\": np.mean(tpr_positive)})\n",
        "\n",
        "    return result\n",
        "\n",
        "def _froc(df_pred: pd.DataFrame, thresholds: List[float], n_volumes: int, n_boxes: int, evaluation_fps: tuple) -> List[float]:\n",
        "    \"\"\"Free-response receiver operating characteristic (FROC) calculation.\"\"\"\n",
        "    tpr = []\n",
        "    fps = []\n",
        "\n",
        "    for th in sorted(thresholds, reverse=True):\n",
        "        df_th = df_pred.loc[df_pred[\"Score\"] >= th]\n",
        "        df_th_unique_tp = df_th.reset_index().drop_duplicates(subset=[\"StudyUID\", \"View\", \"TP\", \"GTID\"])\n",
        "        n_tps_th = float(sum(df_th_unique_tp[\"TP\"]))\n",
        "        tpr_th = n_tps_th / n_boxes\n",
        "        n_fps_th = float(len(df_th[df_th[\"TP\"] == 0]))\n",
        "        fps_th = n_fps_th / n_volumes\n",
        "        tpr.append(tpr_th)\n",
        "        fps.append(fps_th)\n",
        "\n",
        "        if fps_th > max(evaluation_fps):\n",
        "            break\n",
        "\n",
        "    return [np.interp(x, fps, tpr) for x in evaluation_fps]\n",
        "\n",
        "def _is_tp(box_pred: NamedTuple, box_true: NamedTuple, slice_offset: int, min_dist: int = 100) -> bool:\n",
        "    \"\"\"Determine if a prediction is a true positive.\"\"\"\n",
        "    pred_y = box_pred.Y + box_pred.Height / 2\n",
        "    pred_x = box_pred.X + box_pred.Width / 2\n",
        "    pred_z = box_pred.Z + box_pred.Depth / 2\n",
        "    true_y = box_true.Y + box_true.Height / 2\n",
        "    true_x = box_true.X + box_true.Width / 2\n",
        "    true_z = box_true.Slice\n",
        "\n",
        "    dist = np.linalg.norm((pred_x - true_x, pred_y - true_y))\n",
        "    dist_threshold = np.sqrt(box_true.Width ** 2 + box_true.Height ** 2) / 2.0\n",
        "    dist_threshold = max(dist_threshold, min_dist)\n",
        "    slice_diff = np.abs(pred_z - true_z)\n",
        "\n",
        "    return dist <= dist_threshold and slice_diff <= slice_offset\n",
        "\n",
        "def _distance(box_pred: NamedTuple, box_true: NamedTuple) -> float:\n",
        "    \"\"\"Calculate the Euclidean distance between predicted and true box centers.\"\"\"\n",
        "    pred_y = box_pred.Y + box_pred.Height / 2\n",
        "    pred_x = box_pred.X + box_pred.Width / 2\n",
        "    pred_z = box_pred.Z + box_pred.Depth / 2\n",
        "    true_y = box_true.Y + box_true.Height / 2\n",
        "    true_x = box_true.X + box_true.Width / 2\n",
        "    true_z = box_true.Slice\n",
        "\n",
        "    return np.linalg.norm((pred_x - true_x, pred_y - true_y, pred_z - true_z))\n",
        "\n",
        "def _get_dicom_laterality(ds: dicom.dataset.FileDataset) -> str:\n",
        "    \"\"\"Get laterality from DICOM metadata (unreliable).\"\"\"\n",
        "    return ds[0x5200, 0x9229][0][0x0020, 0x9071][0][0x0020, 0x9072].value\n",
        "\n",
        "def _get_image_laterality(pixel_array: np.ndarray) -> str:\n",
        "    \"\"\"Determine laterality based on image pixel intensity.\"\"\"\n",
        "    left_edge = np.sum(pixel_array[:, 0])  # sum of left edge pixels\n",
        "    right_edge = np.sum(pixel_array[:, -1])  # sum of right edge pixels\n",
        "    return \"R\" if left_edge < right_edge else \"L\"\n",
        "\n",
        "def _get_window_center(ds: dicom.dataset.FileDataset) -> np.float32:\n",
        "    \"\"\"Get window center from DICOM metadata.\"\"\"\n",
        "    return np.float32(ds[0x5200, 0x9229][0][0x0028, 0x9132][0][0x0028, 0x1050].value)\n",
        "\n",
        "def _get_window_width(ds: dicom.dataset.FileDataset) -> np.float32:\n",
        "    \"\"\"Get window width from DICOM metadata.\"\"\"\n",
        "    return np.float32(ds[0x5200, 0x9229][0][0x0028, 0x9132][0][0x0028, 0x1051].value)\n",
        "\n",
        "def process_dicom_files(dicom_files: List[str], boxes_csv_path: str, filepaths_csv_path: str, labels_csv_path: str, predictions_csv_path: str):\n",
        "    # Read bounding boxes and file paths\n",
        "    df = read_boxes(boxes_csv_path, filepaths_csv_path)\n",
        "\n",
        "    for dicom_file_path in dicom_files:\n",
        "        # Extract the view from the file path or use a default view\n",
        "        view = 'LCC'  # Example view; adjust as needed\n",
        "\n",
        "        # Read and process the DICOM file\n",
        "        image_array_3d = dcmread_image(dicom_file_path, view)\n",
        "\n",
        "        # Select a slice (e.g., the middle slice)\n",
        "        slice_index = image_array_3d.shape[0] // 2\n",
        "        image_array = image_array_3d[slice_index]\n",
        "\n",
        "        # Draw bounding boxes on the image\n",
        "        for index, row in df.iterrows():\n",
        "            if row['descriptive_path'] in dicom_file_path:\n",
        "                x, y, width, height = row['X'], row['Y'], row['Width'], row['Height']\n",
        "                image_with_box = draw_box(image_array, x, y, width, height)\n",
        "\n",
        "                # Save the image with the bounding box\n",
        "                output_image_path = os.path.splitext(dicom_file_path)[0] + '_output.png'\n",
        "                imageio.imwrite(output_image_path, image_with_box)\n",
        "                print(f\"Saved {output_image_path}\")\n",
        "\n",
        "    # Evaluate predictions\n",
        "    evaluation_results = evaluate(labels_csv_path, boxes_csv_path, predictions_csv_path)\n",
        "    print(f\"Evaluation results: {evaluation_results}\")\n",
        "\n",
        "# Define file paths for CSV files\n",
        "boxes_csv_path = '/content/drive/MyDrive/Praktikum/boxes.csv'\n",
        "filepaths_csv_path = '/content/drive/MyDrive/Praktikum/filepaths.csv'\n",
        "labels_csv_path = '/content/drive/MyDrive/Praktikum/labels.csv'\n",
        "predictions_csv_path = '/content/drive/MyDrive/Praktikum/predictions.csv'\n",
        "\n",
        "# Process the DICOM files\n",
        "process_dicom_files(dicom_files, boxes_csv_path, filepaths_csv_path, labels_csv_path, predictions_csv_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfBoCOy8xwMH",
        "outputId": "805623fd-f0f9-4930-d680-7990d97572df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03816/01-01-2000-DBT-S03888-MAMMO screening digital bilateral-27568/9767.000000-14592/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03816/01-01-2000-DBT-S03888-MAMMO screening digital bilateral-27568/9766.000000-53332/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00107/01-01-2000-DBT-S05365-MAMMO screening digital bilateral-34793/20399.000000-54881/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00107/01-01-2000-DBT-S05365-MAMMO screening digital bilateral-34793/20398.000000-13219/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04818/01-01-2000-DBT-S02975-MAMMO SCREENING BREAST TOMOSYNTHESIS-18472/6592.000000-78720/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04818/01-01-2000-DBT-S02975-MAMMO SCREENING BREAST TOMOSYNTHESIS-18472/6591.000000-47379/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00784/01-01-2000-DBT-S05205-MAMMO diagnostic digital bilateral-23673/19377.000000-55830/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00784/01-01-2000-DBT-S05205-MAMMO diagnostic digital bilateral-23673/19378.000000-97495/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01712/01-01-2000-DBT-S02941-MAMMO screening digital bilateral-35152/9194.000000-43456/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01712/01-01-2000-DBT-S02941-MAMMO screening digital bilateral-35152/9195.000000-84984/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00060/01-01-2000-DBT-S00787-MAMMO diagnostic digital bilateral-48574/10132.000000-67888/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01751/01-01-2000-DBT-S01332-MAMMO DIAGNOSTIC DIGITAL BILATERAL-61641/16074.000000-65721/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01181/01-01-2000-DBT-S04901-MAMMO diagnostic digital bilateral-55675/13017.000000-68484/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01181/01-01-2000-DBT-S04901-MAMMO diagnostic digital bilateral-55675/13018.000000-54172/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03085/01-01-2000-DBT-S00863-MAMMO DIAGNOSTIC DIGITAL BILATERAL-02053/6322.000000-23801/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03085/01-01-2000-DBT-S00863-MAMMO DIAGNOSTIC DIGITAL BILATERAL-02053/6321.000000-93226/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03748/01-01-2000-DBT-S02094-MAMMO DIAGNOSTIC DIGITAL BILATERAL-55993/6240.000000-66637/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03748/01-01-2000-DBT-S02094-MAMMO DIAGNOSTIC DIGITAL BILATERAL-55993/6239.000000-95462/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01282/01-01-2000-DBT-S01508-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-20750/1738.000000-68814/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01282/01-01-2000-DBT-S01508-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-20750/1737.000000-14534/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04326/01-01-2000-DBT-S03750-MAMMO diagnostic digital bilateral-13946/3012.000000-18261/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04326/01-01-2000-DBT-S03750-MAMMO diagnostic digital bilateral-13946/3011.000000-56685/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00361/01-01-2000-DBT-S00216-MAMMO DIAGNOSTIC DIGITAL BILATERAL-73658/14082.000000-27110/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00361/01-01-2000-DBT-S00216-MAMMO DIAGNOSTIC DIGITAL BILATERAL-73658/14083.000000-68728/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02588/01-01-2000-DBT-S04431-MAMMO diagnostic digital bilateral-49548/7753.000000-88180/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02588/01-01-2000-DBT-S04431-MAMMO diagnostic digital bilateral-49548/7754.000000-32853/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02171/01-01-2000-DBT-S04537-MAMMO diagnostic digital bilateral-59773/11007.000000-09231/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02171/01-01-2000-DBT-S04537-MAMMO diagnostic digital bilateral-59773/11006.000000-62096/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02471/01-01-2000-DBT-S03894-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-95681/2157.000000-57395/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02471/01-01-2000-DBT-S03894-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-95681/2158.000000-82866/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01587/01-01-2000-DBT-S04115-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-24348/13052.000000-97181/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01587/01-01-2000-DBT-S04115-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-24348/13051.000000-06426/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00684/01-01-2000-DBT-S02691-MAMMO diagnostic digital bilateral-61437/4773.000000-24524/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00684/01-01-2000-DBT-S02691-MAMMO diagnostic digital bilateral-61437/4774.000000-46750/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01817/01-01-2000-DBT-S01841-MAMMO diagnostic digital bilateral-51322/8611.000000-33236/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01817/01-01-2000-DBT-S01841-MAMMO diagnostic digital bilateral-51322/8610.000000-57707/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03212/01-01-2000-DBT-S02198-MAMMO screening digital bilateral-48437/813.000000-78802/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03212/01-01-2000-DBT-S02198-MAMMO screening digital bilateral-48437/814.000000-18674/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03009/01-01-2000-DBT-S01465-MAMMO diagnostic digital bilateral-66600/776.000000-20059/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03009/01-01-2000-DBT-S01465-MAMMO diagnostic digital bilateral-66600/775.000000-87362/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P05056/01-01-2000-DBT-S01839-MAMMO screening digital bilateral-55454/6449.000000-98631/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P05056/01-01-2000-DBT-S01839-MAMMO screening digital bilateral-55454/6450.000000-95404/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03017/01-01-2000-DBT-S04280-MAMMO screening digital bilateral-72222/18354.000000-29486/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03017/01-01-2000-DBT-S04280-MAMMO screening digital bilateral-72222/18355.000000-80217/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01241/01-01-2000-DBT-S03180-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-25883/4700.000000-31181/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01241/01-01-2000-DBT-S03180-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-25883/4699.000000-79043/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04901/01-01-2000-DBT-S05032-MAMMO SCREENING BREAST TOMOSYNTHESIS-63983/6847.000000-71680/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04901/01-01-2000-DBT-S05032-MAMMO SCREENING BREAST TOMOSYNTHESIS-63983/6848.000000-91538/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03073/01-01-2000-DBT-S04591-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-40461/18136.000000-16998/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03073/01-01-2000-DBT-S04591-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-40461/18135.000000-84983/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04750/01-01-2000-DBT-S00052-MAMMO SCREENING DIGITAL BILATERAL-50612/8343.000000-89535/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04429/01-01-2000-DBT-S00568-MAMMO diagnostic digital bilateral-85158/612.000000-67937/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04429/01-01-2000-DBT-S00568-MAMMO diagnostic digital bilateral-85158/611.000000-61388/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03677/01-01-2000-DBT-S00709-MAMMO diagnostic digital bilateral-19578/22042.000000-47599/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03677/01-01-2000-DBT-S00709-MAMMO diagnostic digital bilateral-19578/22040.000000-98488/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03677/01-01-2000-DBT-S00709-MAMMO diagnostic digital bilateral-19578/22041.000000-86822/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03677/01-01-2000-DBT-S00709-MAMMO diagnostic digital bilateral-19578/22039.000000-97674/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02100/01-01-2000-DBT-S02965-MAMMO diagnostic digital right-33374/17125.000000-87246/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03218/01-01-2000-DBT-S04050-MAMMO DIAGNOSTIC DIGITAL BILATERAL-07561/2180.000000-85433/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03218/01-01-2000-DBT-S04050-MAMMO DIAGNOSTIC DIGITAL BILATERAL-07561/2179.000000-40865/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01112/01-01-2000-DBT-S04216-MAMMO SCREENING BREAST TOMOSYNTHESIS-55399/710.000000-65963/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01112/01-01-2000-DBT-S04216-MAMMO SCREENING BREAST TOMOSYNTHESIS-55399/709.000000-85307/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01626/01-01-2000-DBT-S02188-MAMMO DIAGNOSTIC DIGITAL BILATERAL-66565/1030.000000-12792/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01626/01-01-2000-DBT-S02188-MAMMO DIAGNOSTIC DIGITAL BILATERAL-66565/1029.000000-41286/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04116/01-01-2000-DBT-S03961-MAMMO diagnostic digital bilateral-96363/16777.000000-25503/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04116/01-01-2000-DBT-S03961-MAMMO diagnostic digital bilateral-96363/16778.000000-84689/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02510/01-01-2000-DBT-S04417-MAMMO diagnostic digital bilateral-56440/699.000000-76341/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02510/01-01-2000-DBT-S04417-MAMMO diagnostic digital bilateral-56440/700.000000-98034/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P05014/01-01-2000-DBT-S04931-MAMMO diagnostic digital bilateral-87401/15945.000000-39087/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P05014/01-01-2000-DBT-S04931-MAMMO diagnostic digital bilateral-87401/15944.000000-40231/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00583/01-01-2000-DBT-S00852-MAMMO screening digital bilateral-57589/5975.000000-91338/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00583/01-01-2000-DBT-S00852-MAMMO screening digital bilateral-57589/5976.000000-98891/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04372/01-01-2000-DBT-S04281-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-14777/1597.000000-95077/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04372/01-01-2000-DBT-S04281-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-14777/1598.000000-16943/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01839/01-01-2000-DBT-S03748-MAMMO diagnostic digital bilateral-99828/5989.000000-67466/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01839/01-01-2000-DBT-S03748-MAMMO diagnostic digital bilateral-99828/5990.000000-14913/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01718/01-01-2000-DBT-S01120-MAMMO diagnostic digital bilateral-54799/6813.000000-32583/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01718/01-01-2000-DBT-S01120-MAMMO diagnostic digital bilateral-54799/6814.000000-90142/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03915/01-01-2000-DBT-S05004-MAMMO DIAGNOSTIC DIGITAL BILATERAL-26756/14226.000000-44972/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01461/01-01-2000-DBT-S00251-MAMMO SCREENING DIGITAL BILATERAL-51291/20284.000000-24547/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01461/01-01-2000-DBT-S00251-MAMMO SCREENING DIGITAL BILATERAL-51291/20285.000000-73584/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01493/01-01-2000-DBT-S00432-MAMMO diagnostic digital bilateral-67139/3537.000000-41887/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01493/01-01-2000-DBT-S00432-MAMMO diagnostic digital bilateral-67139/3538.000000-07093/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02532/01-01-2000-DBT-S02099-MAMMO diagnostic digital bilateral-95101/778.000000-46954/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02532/01-01-2000-DBT-S02099-MAMMO diagnostic digital bilateral-95101/777.000000-85323/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00538/01-01-2000-DBT-S01986-MAMMO screening digital bilateral-71001/14155.000000-36016/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00538/01-01-2000-DBT-S01986-MAMMO screening digital bilateral-71001/14154.000000-69041/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00538/01-01-2000-DBT-S01986-MAMMO screening digital bilateral-71001/14156.000000-08630/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00538/01-01-2000-DBT-S01986-MAMMO screening digital bilateral-71001/14157.000000-86898/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01539/01-01-2000-DBT-S03623-MAMMO DIAGNOSTIC DIGITAL BILATERAL-92414/14136.000000-76114/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01539/01-01-2000-DBT-S03623-MAMMO DIAGNOSTIC DIGITAL BILATERAL-92414/14137.000000-96589/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03539/01-01-2000-DBT-S03755-MAMMO diagnostic digital bilateral-24568/3642.000000-27221/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03539/01-01-2000-DBT-S03755-MAMMO diagnostic digital bilateral-24568/3643.000000-26186/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01439/01-01-2000-DBT-S01096-MAMMO SCREENING BREAST TOMOSYNTHESIS-85652/5189.000000-33192/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00024/01-01-2000-DBT-S03255-MAMMO SCREENING DIGITAL BILATERAL-57165/14783.000000-53289/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00024/01-01-2000-DBT-S03255-MAMMO SCREENING DIGITAL BILATERAL-57165/14782.000000-91029/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02380/01-01-2000-DBT-S01233-MAMMO screening digital bilateral-08173/17469.000000-76593/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02380/01-01-2000-DBT-S01233-MAMMO screening digital bilateral-08173/17470.000000-18206/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03176/01-01-2000-DBT-S03730-MAMMO diagnostic digital bilateral-11702/17051.000000-09862/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03176/01-01-2000-DBT-S03730-MAMMO diagnostic digital bilateral-11702/17052.000000-96687/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03458/01-01-2000-DBT-S03411-MAMMO screening digital bilateral-02962/4435.000000-56229/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P05030/01-01-2000-DBT-S05569-MAMMO diagnostic digital bilateral-78633/862.000000-27865/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P05030/01-01-2000-DBT-S05569-MAMMO diagnostic digital bilateral-78633/861.000000-35432/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01753/01-01-2000-DBT-S04069-MAMMO screening digital bilateral-91798/8978.000000-75393/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01753/01-01-2000-DBT-S04069-MAMMO screening digital bilateral-91798/8979.000000-56601/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02843/01-01-2000-DBT-S01347-MAMMO DIAGNOSTIC DIGITAL BILATERAL-11631/38.000000-63845/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02843/01-01-2000-DBT-S01347-MAMMO DIAGNOSTIC DIGITAL BILATERAL-11631/37.000000-09477/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01110/01-01-2000-DBT-S04109-MAMMO SCREENING BREAST TOMOSYNTHESIS-51562/4229.000000-70503/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01110/01-01-2000-DBT-S04109-MAMMO SCREENING BREAST TOMOSYNTHESIS-51562/4230.000000-43825/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02935/01-01-2000-DBT-S00614-MAMMO screening digital bilateral-64474/10088.000000-57519/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02935/01-01-2000-DBT-S00614-MAMMO screening digital bilateral-64474/10089.000000-31126/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01130/01-01-2000-DBT-S01935-MAMMO DIAGNOSTIC DIGITAL BILATERAL-93564/11600.000000-83445/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01130/01-01-2000-DBT-S01935-MAMMO DIAGNOSTIC DIGITAL BILATERAL-93564/11601.000000-70253/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01745/01-01-2000-DBT-S03916-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-75698/15690.000000-44302/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01745/01-01-2000-DBT-S03916-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-75698/15691.000000-81114/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01826/01-01-2000-DBT-S00710-MAMMO diagnostic digital bilateral-74920/12960.000000-14639/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02493/01-01-2000-DBT-S03027-MAMMO SCREENING DIGITAL BILATERAL-94575/16242.000000-09917/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02493/01-01-2000-DBT-S03027-MAMMO SCREENING DIGITAL BILATERAL-94575/16241.000000-53198/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00654/01-01-2000-DBT-S02571-MAMMO diagnostic digital bilateral-18402/18364.000000-69903/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03292/01-01-2000-DBT-S03262-MAMMO screening digital bilateral-53098/21811.000000-96704/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03292/01-01-2000-DBT-S03262-MAMMO screening digital bilateral-53098/21812.000000-11324/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04026/01-01-2000-DBT-S01650-MAMMO diagnostic digital bilateral-69325/5033.000000-81759/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04026/01-01-2000-DBT-S01650-MAMMO diagnostic digital bilateral-69325/5034.000000-41098/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02579/01-01-2000-DBT-S00929-MAMMO diagnostic digital bilateral-16324/1019.000000-88977/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02579/01-01-2000-DBT-S00929-MAMMO diagnostic digital bilateral-16324/1020.000000-90943/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01347/01-01-2000-DBT-S02864-MAMMO screening digital bilateral-38847/3990.000000-95139/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01347/01-01-2000-DBT-S02864-MAMMO screening digital bilateral-38847/3992.000000-13434/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01347/01-01-2000-DBT-S02864-MAMMO screening digital bilateral-38847/3991.000000-47293/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03658/01-01-2000-DBT-S05241-MAMMO SCREENING BREAST TOMOSYNTHESIS-51351/21827.000000-92512/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03658/01-01-2000-DBT-S05241-MAMMO SCREENING BREAST TOMOSYNTHESIS-51351/21826.000000-22789/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03658/01-01-2000-DBT-S05241-MAMMO SCREENING BREAST TOMOSYNTHESIS-51351/21828.000000-11399/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00194/01-01-2000-DBT-S00645-MAMMO diagnostic digital bilateral-60236/14293.000000-96482/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02065/01-01-2000-DBT-S04959-MAMMO DIAGNOSTIC DIGITAL BILATERAL-50888/11032.000000-30225/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02065/01-01-2000-DBT-S04959-MAMMO DIAGNOSTIC DIGITAL BILATERAL-50888/11033.000000-71824/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00013/01-01-2000-DBT-S00163-MAMMO DIAGNOSTIC DIGITAL BILATERAL-56865/20566.000000-32081/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02227/01-01-2000-DBT-S05014-MAMMO diagnostic digital bilateral-24559/1431.000000-59384/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02227/01-01-2000-DBT-S05014-MAMMO diagnostic digital bilateral-24559/1432.000000-28211/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00303/01-01-2000-DBT-S02436-MAMMO diagnostic digital bilateral-33835/14595.000000-79329/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00303/01-01-2000-DBT-S02436-MAMMO diagnostic digital bilateral-33835/14594.000000-25446/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02750/01-01-2000-DBT-S00905-MAMMO screening digital bilateral-91605/9298.000000-03332/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02750/01-01-2000-DBT-S00905-MAMMO screening digital bilateral-91605/9299.000000-67056/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02919/01-01-2000-DBT-S01259-MAMMO screening digital bilateral-68030/12895.000000-12883/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02919/01-01-2000-DBT-S01259-MAMMO screening digital bilateral-68030/12894.000000-34445/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02919/01-01-2000-DBT-S01259-MAMMO screening digital bilateral-68030/12892.000000-81829/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02919/01-01-2000-DBT-S01259-MAMMO screening digital bilateral-68030/12893.000000-24570/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02133/01-01-2000-DBT-S02082-MAMMO diagnostic digital right-74674/15119.000000-35020/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02133/01-01-2000-DBT-S02082-MAMMO diagnostic digital right-74674/15118.000000-48075/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01139/01-01-2000-DBT-S01170-MAMMO screening digital bilateral-99674/9512.000000-56979/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01139/01-01-2000-DBT-S01170-MAMMO screening digital bilateral-99674/9513.000000-83090/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00818/01-01-2000-DBT-S02315-MAMMO DIAGNOSTIC DIGITAL BILATERAL-14000/16600.000000-18711/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00818/01-01-2000-DBT-S02315-MAMMO DIAGNOSTIC DIGITAL BILATERAL-14000/16599.000000-35539/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02736/01-01-2000-DBT-S03542-MAMMO diagnostic digital bilateral-28356/15841.000000-62055/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02736/01-01-2000-DBT-S03542-MAMMO diagnostic digital bilateral-28356/15840.000000-43309/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02738/01-01-2000-DBT-S01258-MAMMO diagnostic digital bilateral-71538/14275.000000-97275/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02738/01-01-2000-DBT-S01258-MAMMO diagnostic digital bilateral-71538/14276.000000-15205/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04858/01-01-2000-DBT-S04555-MAMMO SCREENING BREAST TOMOSYNTHESIS-89444/5243.000000-08718/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04858/01-01-2000-DBT-S04555-MAMMO SCREENING BREAST TOMOSYNTHESIS-89444/5242.000000-38850/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00225/01-01-2000-DBT-S02346-MAMMO DIAGNOSTIC DIGITAL BILATERAL-28434/6189.000000-66774/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00225/01-01-2000-DBT-S02346-MAMMO DIAGNOSTIC DIGITAL BILATERAL-28434/6190.000000-67794/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04710/01-01-2000-DBT-S03227-MAMMO SCREENING BREAST TOMOSYNTHESIS-24983/4960.000000-86967/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04710/01-01-2000-DBT-S03227-MAMMO SCREENING BREAST TOMOSYNTHESIS-24983/4959.000000-38406/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02176/01-01-2000-DBT-S03078-MAMMO screening digital bilateral-69304/19230.000000-75419/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02176/01-01-2000-DBT-S03078-MAMMO screening digital bilateral-69304/19229.000000-39737/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03203/01-01-2000-DBT-S04862-MAMMO diagnostic digital bilateral-92733/14304.000000-59631/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03203/01-01-2000-DBT-S04862-MAMMO diagnostic digital bilateral-92733/14305.000000-63473/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P05047/01-01-2000-DBT-S05588-MAMMO SCREENING DIGITAL BILATERAL-88035/20309.000000-18632/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P05047/01-01-2000-DBT-S05588-MAMMO SCREENING DIGITAL BILATERAL-88035/20308.000000-79110/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03222/01-01-2000-DBT-S00931-MAMMO diagnostic digital bilateral-57777/17348.000000-97916/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03222/01-01-2000-DBT-S00931-MAMMO diagnostic digital bilateral-57777/17347.000000-50713/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03978/01-01-2000-DBT-S00442-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-62824/12449.000000-68028/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03978/01-01-2000-DBT-S00442-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-62824/12448.000000-52179/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02582/01-01-2000-DBT-S01997-MAMMO diagnostic digital bilateral-98922/3865.000000-48384/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02582/01-01-2000-DBT-S01997-MAMMO diagnostic digital bilateral-98922/3864.000000-27277/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01801/01-01-2000-DBT-S04999-MAMMO diagnostic digital right-47139/840.000000-69817/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01801/01-01-2000-DBT-S04999-MAMMO diagnostic digital right-47139/839.000000-76312/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04721/01-01-2000-DBT-S01833-MAMMO screening digital bilateral-47309/2889.000000-50432/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04721/01-01-2000-DBT-S01833-MAMMO screening digital bilateral-47309/2890.000000-83373/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01673/01-01-2000-DBT-S04347-MAMMO diagnostic digital bilateral-30029/12558.000000-63058/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01673/01-01-2000-DBT-S04347-MAMMO diagnostic digital bilateral-30029/12559.000000-24133/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01624/01-01-2000-DBT-S03518-MAMMO screening digital bilateral-19315/5179.000000-07086/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01624/01-01-2000-DBT-S03518-MAMMO screening digital bilateral-19315/5180.000000-11459/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04631/01-01-2000-DBT-S05515-MAMMO diagnostic digital bilateral-84562/2789.000000-47217/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04631/01-01-2000-DBT-S05515-MAMMO diagnostic digital bilateral-84562/2790.000000-75744/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00827/01-01-2000-DBT-S01731-MAMMO screening digital bilateral-65653/779.000000-59014/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P00827/01-01-2000-DBT-S01731-MAMMO screening digital bilateral-65653/780.000000-58797/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03423/01-01-2000-DBT-S05305-MAMMO diagnostic digital bilateral-27904/1071.000000-36189/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P03423/01-01-2000-DBT-S05305-MAMMO diagnostic digital bilateral-27904/1072.000000-36946/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01262/01-01-2000-DBT-S00537-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-88617/5360.000000-70054/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01262/01-01-2000-DBT-S00537-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-88617/5361.000000-47955/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01267/01-01-2000-DBT-S03415-MAMMO diagnostic digital bilateral-33927/19688.000000-16007/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01267/01-01-2000-DBT-S03415-MAMMO diagnostic digital bilateral-33927/19689.000000-24374/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02798/01-01-2000-DBT-S01770-MAMMO diagnostic digital bilateral-60802/4654.000000-88875/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02798/01-01-2000-DBT-S01770-MAMMO diagnostic digital bilateral-60802/4653.000000-73932/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P02798/01-01-2000-DBT-S01770-MAMMO diagnostic digital bilateral-60802/4652.000000-73193/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01488/01-01-2000-DBT-S00017-MAMMO SCREENING DIGITAL BILATERAL-90520/9196.000000-13599/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P01488/01-01-2000-DBT-S00017-MAMMO SCREENING DIGITAL BILATERAL-90520/9197.000000-04594/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04090/01-01-2000-DBT-S01718-MAMMO diagnostic digital bilateral-51968/3137.000000-95562/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P04090/01-01-2000-DBT-S01718-MAMMO diagnostic digital bilateral-51968/3138.000000-94838/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P05022/01-01-2000-DBT-S05195-MAMMO diagnostic digital bilateral-02818/10582.000000-26511/1-1.dcm', '/content/dataset/Breast-Cancer-Screening-DBT/DBT-P05022/01-01-2000-DBT-S05195-MAMMO diagnostic digital bilateral-02818/10583.000000-11197/1-1.dcm']\n"
          ]
        }
      ],
      "source": [
        "print(dicom_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA8im0GhsFWO"
      },
      "source": [
        "#3D Dataset Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dufPUJ3ksIS_",
        "outputId": "c3e1a28a-d8d8-4333-8125-2a1c061ad391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patient ID: DBT-P05022\n",
            "Study ID: 1.2.826.0.1.3680043.8.498.68850109392698203596997730928918402818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/200 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/pydicom/pixel_data_handlers/pillow_handler.py:238: UserWarning: The (0028,0101) 'Bits Stored' value (10-bit) doesn't match the JPEG 2000 data (16-bit). It's recommended that you change the 'Bits Stored' value\n",
            "  warnings.warn(\n",
            "  0%|          | 1/200 [00:49<2:44:00, 49.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03816/01-01-2000-DBT-S03888-MAMMO screening digital bilateral-27568/9767.000000-14592/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 2/200 [01:54<3:12:49, 58.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03816/01-01-2000-DBT-S03888-MAMMO screening digital bilateral-27568/9766.000000-53332/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 3/200 [02:30<2:38:34, 48.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00107/01-01-2000-DBT-S05365-MAMMO screening digital bilateral-34793/20399.000000-54881/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 4/200 [02:53<2:05:23, 38.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00107/01-01-2000-DBT-S05365-MAMMO screening digital bilateral-34793/20398.000000-13219/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▎         | 5/200 [03:35<2:08:58, 39.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04818/01-01-2000-DBT-S02975-MAMMO SCREENING BREAST TOMOSYNTHESIS-18472/6592.000000-78720/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 6/200 [04:14<2:07:43, 39.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04818/01-01-2000-DBT-S02975-MAMMO SCREENING BREAST TOMOSYNTHESIS-18472/6591.000000-47379/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▎         | 7/200 [04:42<1:54:48, 35.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00784/01-01-2000-DBT-S05205-MAMMO diagnostic digital bilateral-23673/19377.000000-55830/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 8/200 [05:04<1:40:21, 31.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00784/01-01-2000-DBT-S05205-MAMMO diagnostic digital bilateral-23673/19378.000000-97495/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 9/200 [05:30<1:34:32, 29.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01712/01-01-2000-DBT-S02941-MAMMO screening digital bilateral-35152/9194.000000-43456/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 10/200 [06:06<1:39:44, 31.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01712/01-01-2000-DBT-S02941-MAMMO screening digital bilateral-35152/9195.000000-84984/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 11/200 [06:40<1:41:25, 32.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00060/01-01-2000-DBT-S00787-MAMMO diagnostic digital bilateral-48574/10132.000000-67888/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 12/200 [07:00<1:29:37, 28.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01751/01-01-2000-DBT-S01332-MAMMO DIAGNOSTIC DIGITAL BILATERAL-61641/16074.000000-65721/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▋         | 13/200 [07:36<1:35:45, 30.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01181/01-01-2000-DBT-S04901-MAMMO diagnostic digital bilateral-55675/13017.000000-68484/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 14/200 [08:22<1:49:55, 35.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01181/01-01-2000-DBT-S04901-MAMMO diagnostic digital bilateral-55675/13018.000000-54172/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 15/200 [08:58<1:50:14, 35.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03085/01-01-2000-DBT-S00863-MAMMO DIAGNOSTIC DIGITAL BILATERAL-02053/6322.000000-23801/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 16/200 [09:39<1:54:06, 37.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03085/01-01-2000-DBT-S00863-MAMMO DIAGNOSTIC DIGITAL BILATERAL-02053/6321.000000-93226/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 17/200 [10:22<1:58:43, 38.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03748/01-01-2000-DBT-S02094-MAMMO DIAGNOSTIC DIGITAL BILATERAL-55993/6240.000000-66637/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 18/200 [11:07<2:03:19, 40.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03748/01-01-2000-DBT-S02094-MAMMO DIAGNOSTIC DIGITAL BILATERAL-55993/6239.000000-95462/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|▉         | 19/200 [11:42<1:57:41, 39.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01282/01-01-2000-DBT-S01508-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-20750/1738.000000-68814/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 20/200 [12:11<1:48:20, 36.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01282/01-01-2000-DBT-S01508-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-20750/1737.000000-14534/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 21/200 [12:36<1:37:50, 32.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04326/01-01-2000-DBT-S03750-MAMMO diagnostic digital bilateral-13946/3012.000000-18261/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 22/200 [13:07<1:35:33, 32.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04326/01-01-2000-DBT-S03750-MAMMO diagnostic digital bilateral-13946/3011.000000-56685/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 23/200 [13:34<1:30:14, 30.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00361/01-01-2000-DBT-S00216-MAMMO DIAGNOSTIC DIGITAL BILATERAL-73658/14082.000000-27110/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 24/200 [13:58<1:24:15, 28.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00361/01-01-2000-DBT-S00216-MAMMO DIAGNOSTIC DIGITAL BILATERAL-73658/14083.000000-68728/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▎        | 25/200 [14:31<1:27:41, 30.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02588/01-01-2000-DBT-S04431-MAMMO diagnostic digital bilateral-49548/7753.000000-88180/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 26/200 [14:59<1:25:12, 29.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02588/01-01-2000-DBT-S04431-MAMMO diagnostic digital bilateral-49548/7754.000000-32853/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▎        | 27/200 [15:44<1:38:10, 34.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02171/01-01-2000-DBT-S04537-MAMMO diagnostic digital bilateral-59773/11007.000000-09231/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 28/200 [16:20<1:39:15, 34.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02171/01-01-2000-DBT-S04537-MAMMO diagnostic digital bilateral-59773/11006.000000-62096/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 29/200 [17:15<1:56:15, 40.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02471/01-01-2000-DBT-S03894-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-95681/2157.000000-57395/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 30/200 [17:59<1:57:52, 41.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02471/01-01-2000-DBT-S03894-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-95681/2158.000000-82866/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 31/200 [18:31<1:49:11, 38.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01587/01-01-2000-DBT-S04115-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-24348/13052.000000-97181/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 32/200 [19:08<1:46:45, 38.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01587/01-01-2000-DBT-S04115-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-24348/13051.000000-06426/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▋        | 33/200 [19:39<1:40:33, 36.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00684/01-01-2000-DBT-S02691-MAMMO diagnostic digital bilateral-61437/4773.000000-24524/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 34/200 [20:13<1:38:10, 35.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00684/01-01-2000-DBT-S02691-MAMMO diagnostic digital bilateral-61437/4774.000000-46750/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 35/200 [21:16<2:00:25, 43.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01817/01-01-2000-DBT-S01841-MAMMO diagnostic digital bilateral-51322/8611.000000-33236/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 36/200 [22:06<2:04:17, 45.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01817/01-01-2000-DBT-S01841-MAMMO diagnostic digital bilateral-51322/8610.000000-57707/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 37/200 [22:32<1:48:17, 39.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03212/01-01-2000-DBT-S02198-MAMMO screening digital bilateral-48437/813.000000-78802/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▉        | 38/200 [23:05<1:41:51, 37.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03212/01-01-2000-DBT-S02198-MAMMO screening digital bilateral-48437/814.000000-18674/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|█▉        | 39/200 [23:55<1:51:08, 41.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03009/01-01-2000-DBT-S01465-MAMMO diagnostic digital bilateral-66600/776.000000-20059/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 40/200 [24:45<1:57:25, 44.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03009/01-01-2000-DBT-S01465-MAMMO diagnostic digital bilateral-66600/775.000000-87362/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 41/200 [25:23<1:51:26, 42.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P05056/01-01-2000-DBT-S01839-MAMMO screening digital bilateral-55454/6449.000000-98631/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██        | 42/200 [26:04<1:50:12, 41.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P05056/01-01-2000-DBT-S01839-MAMMO screening digital bilateral-55454/6450.000000-95404/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 43/200 [26:40<1:44:52, 40.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03017/01-01-2000-DBT-S04280-MAMMO screening digital bilateral-72222/18354.000000-29486/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 44/200 [27:23<1:46:16, 40.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03017/01-01-2000-DBT-S04280-MAMMO screening digital bilateral-72222/18355.000000-80217/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▎       | 45/200 [28:21<1:59:05, 46.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01241/01-01-2000-DBT-S03180-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-25883/4700.000000-31181/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 46/200 [29:05<1:56:54, 45.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01241/01-01-2000-DBT-S03180-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-25883/4699.000000-79043/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▎       | 47/200 [29:40<1:47:45, 42.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04901/01-01-2000-DBT-S05032-MAMMO SCREENING BREAST TOMOSYNTHESIS-63983/6847.000000-71680/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 48/200 [30:05<1:33:50, 37.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04901/01-01-2000-DBT-S05032-MAMMO SCREENING BREAST TOMOSYNTHESIS-63983/6848.000000-91538/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 49/200 [30:46<1:36:47, 38.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03073/01-01-2000-DBT-S04591-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-40461/18136.000000-16998/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 50/200 [31:34<1:43:10, 41.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03073/01-01-2000-DBT-S04591-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-40461/18135.000000-84983/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 51/200 [32:05<1:34:52, 38.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04750/01-01-2000-DBT-S00052-MAMMO SCREENING DIGITAL BILATERAL-50612/8343.000000-89535/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 52/200 [32:47<1:37:01, 39.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04429/01-01-2000-DBT-S00568-MAMMO diagnostic digital bilateral-85158/612.000000-67937/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▋       | 53/200 [33:38<1:44:49, 42.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04429/01-01-2000-DBT-S00568-MAMMO diagnostic digital bilateral-85158/611.000000-61388/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 54/200 [34:14<1:39:02, 40.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03677/01-01-2000-DBT-S00709-MAMMO diagnostic digital bilateral-19578/22042.000000-47599/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 55/200 [34:40<1:27:48, 36.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03677/01-01-2000-DBT-S00709-MAMMO diagnostic digital bilateral-19578/22040.000000-98488/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 56/200 [35:17<1:27:28, 36.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03677/01-01-2000-DBT-S00709-MAMMO diagnostic digital bilateral-19578/22041.000000-86822/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 57/200 [35:48<1:22:59, 34.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03677/01-01-2000-DBT-S00709-MAMMO diagnostic digital bilateral-19578/22039.000000-97674/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▉       | 58/200 [36:23<1:22:46, 34.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02100/01-01-2000-DBT-S02965-MAMMO diagnostic digital right-33374/17125.000000-87246/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|██▉       | 59/200 [37:03<1:25:38, 36.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03218/01-01-2000-DBT-S04050-MAMMO DIAGNOSTIC DIGITAL BILATERAL-07561/2180.000000-85433/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 60/200 [37:56<1:36:51, 41.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03218/01-01-2000-DBT-S04050-MAMMO DIAGNOSTIC DIGITAL BILATERAL-07561/2179.000000-40865/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 61/200 [38:47<1:42:34, 44.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01112/01-01-2000-DBT-S04216-MAMMO SCREENING BREAST TOMOSYNTHESIS-55399/710.000000-65963/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███       | 62/200 [39:31<1:41:30, 44.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01112/01-01-2000-DBT-S04216-MAMMO SCREENING BREAST TOMOSYNTHESIS-55399/709.000000-85307/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 63/200 [40:02<1:31:41, 40.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01626/01-01-2000-DBT-S02188-MAMMO DIAGNOSTIC DIGITAL BILATERAL-66565/1030.000000-12792/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 64/200 [40:40<1:29:31, 39.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01626/01-01-2000-DBT-S02188-MAMMO DIAGNOSTIC DIGITAL BILATERAL-66565/1029.000000-41286/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▎      | 65/200 [41:16<1:26:25, 38.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04116/01-01-2000-DBT-S03961-MAMMO diagnostic digital bilateral-96363/16777.000000-25503/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 66/200 [42:02<1:31:05, 40.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04116/01-01-2000-DBT-S03961-MAMMO diagnostic digital bilateral-96363/16778.000000-84689/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▎      | 67/200 [42:34<1:24:37, 38.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02510/01-01-2000-DBT-S04417-MAMMO diagnostic digital bilateral-56440/699.000000-76341/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▍      | 68/200 [43:20<1:29:03, 40.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02510/01-01-2000-DBT-S04417-MAMMO diagnostic digital bilateral-56440/700.000000-98034/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▍      | 69/200 [44:15<1:37:56, 44.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P05014/01-01-2000-DBT-S04931-MAMMO diagnostic digital bilateral-87401/15945.000000-39087/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 70/200 [45:05<1:40:39, 46.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P05014/01-01-2000-DBT-S04931-MAMMO diagnostic digital bilateral-87401/15944.000000-40231/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 71/200 [45:43<1:34:22, 43.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00583/01-01-2000-DBT-S00852-MAMMO screening digital bilateral-57589/5975.000000-91338/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 72/200 [46:24<1:31:25, 42.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00583/01-01-2000-DBT-S00852-MAMMO screening digital bilateral-57589/5976.000000-98891/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▋      | 73/200 [47:18<1:38:08, 46.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04372/01-01-2000-DBT-S04281-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-14777/1597.000000-95077/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 74/200 [48:33<1:55:24, 54.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04372/01-01-2000-DBT-S04281-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-14777/1598.000000-16943/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 75/200 [49:46<2:05:26, 60.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01839/01-01-2000-DBT-S03748-MAMMO diagnostic digital bilateral-99828/5989.000000-67466/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 76/200 [50:43<2:02:29, 59.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01839/01-01-2000-DBT-S03748-MAMMO diagnostic digital bilateral-99828/5990.000000-14913/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 77/200 [51:23<1:50:07, 53.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01718/01-01-2000-DBT-S01120-MAMMO diagnostic digital bilateral-54799/6813.000000-32583/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39%|███▉      | 78/200 [51:53<1:34:47, 46.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01718/01-01-2000-DBT-S01120-MAMMO diagnostic digital bilateral-54799/6814.000000-90142/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|███▉      | 79/200 [52:28<1:26:26, 42.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03915/01-01-2000-DBT-S05004-MAMMO DIAGNOSTIC DIGITAL BILATERAL-26756/14226.000000-44972/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 80/200 [52:54<1:15:59, 37.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01461/01-01-2000-DBT-S00251-MAMMO SCREENING DIGITAL BILATERAL-51291/20284.000000-24547/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 81/200 [53:26<1:11:26, 36.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01461/01-01-2000-DBT-S00251-MAMMO SCREENING DIGITAL BILATERAL-51291/20285.000000-73584/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 41%|████      | 82/200 [53:36<55:49, 28.39s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01493/01-01-2000-DBT-S00432-MAMMO diagnostic digital bilateral-67139/3537.000000-41887/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 83/200 [53:50<46:58, 24.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01493/01-01-2000-DBT-S00432-MAMMO diagnostic digital bilateral-67139/3538.000000-07093/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 84/200 [54:27<54:04, 27.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02532/01-01-2000-DBT-S02099-MAMMO diagnostic digital bilateral-95101/778.000000-46954/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▎     | 85/200 [55:14<1:04:25, 33.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02532/01-01-2000-DBT-S02099-MAMMO diagnostic digital bilateral-95101/777.000000-85323/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 86/200 [55:55<1:08:10, 35.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00538/01-01-2000-DBT-S01986-MAMMO screening digital bilateral-71001/14155.000000-36016/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▎     | 87/200 [56:32<1:07:47, 35.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00538/01-01-2000-DBT-S01986-MAMMO screening digital bilateral-71001/14154.000000-69041/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 88/200 [57:17<1:12:33, 38.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00538/01-01-2000-DBT-S01986-MAMMO screening digital bilateral-71001/14156.000000-08630/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 89/200 [57:49<1:07:58, 36.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00538/01-01-2000-DBT-S01986-MAMMO screening digital bilateral-71001/14157.000000-86898/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 90/200 [58:59<1:25:48, 46.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01539/01-01-2000-DBT-S03623-MAMMO DIAGNOSTIC DIGITAL BILATERAL-92414/14136.000000-76114/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 91/200 [1:00:16<1:41:28, 55.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01539/01-01-2000-DBT-S03623-MAMMO DIAGNOSTIC DIGITAL BILATERAL-92414/14137.000000-96589/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 92/200 [1:00:58<1:32:46, 51.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03539/01-01-2000-DBT-S03755-MAMMO diagnostic digital bilateral-24568/3642.000000-27221/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▋     | 93/200 [1:01:46<1:30:28, 50.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03539/01-01-2000-DBT-S03755-MAMMO diagnostic digital bilateral-24568/3643.000000-26186/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 94/200 [1:02:21<1:20:51, 45.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01439/01-01-2000-DBT-S01096-MAMMO SCREENING BREAST TOMOSYNTHESIS-85652/5189.000000-33192/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 95/200 [1:03:19<1:26:45, 49.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00024/01-01-2000-DBT-S03255-MAMMO SCREENING DIGITAL BILATERAL-57165/14783.000000-53289/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 96/200 [1:04:04<1:23:17, 48.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00024/01-01-2000-DBT-S03255-MAMMO SCREENING DIGITAL BILATERAL-57165/14782.000000-91029/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 97/200 [1:04:27<1:09:40, 40.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02380/01-01-2000-DBT-S01233-MAMMO screening digital bilateral-08173/17469.000000-76593/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 49%|████▉     | 98/200 [1:04:53<1:01:30, 36.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02380/01-01-2000-DBT-S01233-MAMMO screening digital bilateral-08173/17470.000000-18206/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|████▉     | 99/200 [1:05:27<59:56, 35.60s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03176/01-01-2000-DBT-S03730-MAMMO diagnostic digital bilateral-11702/17051.000000-09862/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 100/200 [1:05:57<56:33, 33.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03176/01-01-2000-DBT-S03730-MAMMO diagnostic digital bilateral-11702/17052.000000-96687/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 101/200 [1:06:33<56:49, 34.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03458/01-01-2000-DBT-S03411-MAMMO screening digital bilateral-02962/4435.000000-56229/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 51%|█████     | 102/200 [1:07:18<1:01:44, 37.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P05030/01-01-2000-DBT-S05569-MAMMO diagnostic digital bilateral-78633/862.000000-27865/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 103/200 [1:08:10<1:07:54, 42.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P05030/01-01-2000-DBT-S05569-MAMMO diagnostic digital bilateral-78633/861.000000-35432/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 104/200 [1:09:19<1:20:04, 50.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01753/01-01-2000-DBT-S04069-MAMMO screening digital bilateral-91798/8978.000000-75393/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▎    | 105/200 [1:10:18<1:23:22, 52.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01753/01-01-2000-DBT-S04069-MAMMO screening digital bilateral-91798/8979.000000-56601/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 106/200 [1:11:17<1:25:42, 54.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02843/01-01-2000-DBT-S01347-MAMMO DIAGNOSTIC DIGITAL BILATERAL-11631/38.000000-63845/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▎    | 107/200 [1:12:10<1:24:10, 54.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02843/01-01-2000-DBT-S01347-MAMMO DIAGNOSTIC DIGITAL BILATERAL-11631/37.000000-09477/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 108/200 [1:12:55<1:18:40, 51.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01110/01-01-2000-DBT-S04109-MAMMO SCREENING BREAST TOMOSYNTHESIS-51562/4229.000000-70503/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▍    | 109/200 [1:13:33<1:11:38, 47.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01110/01-01-2000-DBT-S04109-MAMMO SCREENING BREAST TOMOSYNTHESIS-51562/4230.000000-43825/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 110/200 [1:14:23<1:12:07, 48.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02935/01-01-2000-DBT-S00614-MAMMO screening digital bilateral-64474/10088.000000-57519/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 111/200 [1:15:04<1:08:26, 46.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02935/01-01-2000-DBT-S00614-MAMMO screening digital bilateral-64474/10089.000000-31126/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 112/200 [1:15:26<57:01, 38.88s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01130/01-01-2000-DBT-S01935-MAMMO DIAGNOSTIC DIGITAL BILATERAL-93564/11600.000000-83445/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▋    | 113/200 [1:15:41<46:00, 31.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01130/01-01-2000-DBT-S01935-MAMMO DIAGNOSTIC DIGITAL BILATERAL-93564/11601.000000-70253/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 114/200 [1:16:10<44:17, 30.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01745/01-01-2000-DBT-S03916-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-75698/15690.000000-44302/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▊    | 115/200 [1:16:45<45:20, 32.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01745/01-01-2000-DBT-S03916-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-75698/15691.000000-81114/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 116/200 [1:17:17<44:57, 32.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01826/01-01-2000-DBT-S00710-MAMMO diagnostic digital bilateral-74920/12960.000000-14639/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 117/200 [1:17:54<46:15, 33.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02493/01-01-2000-DBT-S03027-MAMMO SCREENING DIGITAL BILATERAL-94575/16242.000000-09917/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 59%|█████▉    | 118/200 [1:18:42<51:36, 37.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02493/01-01-2000-DBT-S03027-MAMMO SCREENING DIGITAL BILATERAL-94575/16241.000000-53198/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|█████▉    | 119/200 [1:19:25<53:20, 39.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00654/01-01-2000-DBT-S02571-MAMMO diagnostic digital bilateral-18402/18364.000000-69903/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 120/200 [1:20:08<54:05, 40.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03292/01-01-2000-DBT-S03262-MAMMO screening digital bilateral-53098/21811.000000-96704/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 121/200 [1:20:59<57:21, 43.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03292/01-01-2000-DBT-S03262-MAMMO screening digital bilateral-53098/21812.000000-11324/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61%|██████    | 122/200 [1:21:24<49:28, 38.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04026/01-01-2000-DBT-S01650-MAMMO diagnostic digital bilateral-69325/5033.000000-81759/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 123/200 [1:21:53<45:23, 35.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04026/01-01-2000-DBT-S01650-MAMMO diagnostic digital bilateral-69325/5034.000000-41098/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 124/200 [1:22:19<41:06, 32.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02579/01-01-2000-DBT-S00929-MAMMO diagnostic digital bilateral-16324/1019.000000-88977/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▎   | 125/200 [1:22:43<37:32, 30.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02579/01-01-2000-DBT-S00929-MAMMO diagnostic digital bilateral-16324/1020.000000-90943/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63%|██████▎   | 126/200 [1:23:19<39:23, 31.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01347/01-01-2000-DBT-S02864-MAMMO screening digital bilateral-38847/3990.000000-95139/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▎   | 127/200 [1:23:53<39:24, 32.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01347/01-01-2000-DBT-S02864-MAMMO screening digital bilateral-38847/3992.000000-13434/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 128/200 [1:24:37<43:04, 35.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01347/01-01-2000-DBT-S02864-MAMMO screening digital bilateral-38847/3991.000000-47293/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 129/200 [1:25:34<50:05, 42.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03658/01-01-2000-DBT-S05241-MAMMO SCREENING BREAST TOMOSYNTHESIS-51351/21827.000000-92512/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 130/200 [1:26:22<51:23, 44.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03658/01-01-2000-DBT-S05241-MAMMO SCREENING BREAST TOMOSYNTHESIS-51351/21826.000000-22789/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▌   | 131/200 [1:27:08<51:03, 44.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03658/01-01-2000-DBT-S05241-MAMMO SCREENING BREAST TOMOSYNTHESIS-51351/21828.000000-11399/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▌   | 132/200 [1:28:08<55:44, 49.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00194/01-01-2000-DBT-S00645-MAMMO diagnostic digital bilateral-60236/14293.000000-96482/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▋   | 133/200 [1:28:49<52:11, 46.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02065/01-01-2000-DBT-S04959-MAMMO DIAGNOSTIC DIGITAL BILATERAL-50888/11032.000000-30225/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 134/200 [1:29:23<47:15, 42.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02065/01-01-2000-DBT-S04959-MAMMO DIAGNOSTIC DIGITAL BILATERAL-50888/11033.000000-71824/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 135/200 [1:29:49<41:04, 37.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00013/01-01-2000-DBT-S00163-MAMMO DIAGNOSTIC DIGITAL BILATERAL-56865/20566.000000-32081/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 136/200 [1:30:38<43:58, 41.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02227/01-01-2000-DBT-S05014-MAMMO diagnostic digital bilateral-24559/1431.000000-59384/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 137/200 [1:31:21<43:41, 41.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02227/01-01-2000-DBT-S05014-MAMMO diagnostic digital bilateral-24559/1432.000000-28211/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69%|██████▉   | 138/200 [1:31:50<39:04, 37.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00303/01-01-2000-DBT-S02436-MAMMO diagnostic digital bilateral-33835/14595.000000-79329/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|██████▉   | 139/200 [1:32:22<36:48, 36.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00303/01-01-2000-DBT-S02436-MAMMO diagnostic digital bilateral-33835/14594.000000-25446/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 140/200 [1:33:02<37:15, 37.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02750/01-01-2000-DBT-S00905-MAMMO screening digital bilateral-91605/9298.000000-03332/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 141/200 [1:33:44<38:05, 38.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02750/01-01-2000-DBT-S00905-MAMMO screening digital bilateral-91605/9299.000000-67056/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████   | 142/200 [1:34:57<47:15, 48.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02919/01-01-2000-DBT-S01259-MAMMO screening digital bilateral-68030/12895.000000-12883/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 143/200 [1:35:53<48:42, 51.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02919/01-01-2000-DBT-S01259-MAMMO screening digital bilateral-68030/12894.000000-34445/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 144/200 [1:37:00<52:02, 55.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02919/01-01-2000-DBT-S01259-MAMMO screening digital bilateral-68030/12892.000000-81829/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▎  | 145/200 [1:38:17<57:10, 62.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02919/01-01-2000-DBT-S01259-MAMMO screening digital bilateral-68030/12893.000000-24570/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 146/200 [1:39:04<51:50, 57.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02133/01-01-2000-DBT-S02082-MAMMO diagnostic digital right-74674/15119.000000-35020/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▎  | 147/200 [1:39:43<46:05, 52.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02133/01-01-2000-DBT-S02082-MAMMO diagnostic digital right-74674/15118.000000-48075/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 148/200 [1:40:29<43:34, 50.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01139/01-01-2000-DBT-S01170-MAMMO screening digital bilateral-99674/9512.000000-56979/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 149/200 [1:41:28<44:51, 52.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01139/01-01-2000-DBT-S01170-MAMMO screening digital bilateral-99674/9513.000000-83090/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 150/200 [1:42:40<48:48, 58.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00818/01-01-2000-DBT-S02315-MAMMO DIAGNOSTIC DIGITAL BILATERAL-14000/16600.000000-18711/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 151/200 [1:43:42<48:45, 59.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00818/01-01-2000-DBT-S02315-MAMMO DIAGNOSTIC DIGITAL BILATERAL-14000/16599.000000-35539/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 152/200 [1:44:26<43:57, 54.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02736/01-01-2000-DBT-S03542-MAMMO diagnostic digital bilateral-28356/15841.000000-62055/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▋  | 153/200 [1:45:03<38:42, 49.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02736/01-01-2000-DBT-S03542-MAMMO diagnostic digital bilateral-28356/15840.000000-43309/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 77%|███████▋  | 154/200 [1:45:53<38:04, 49.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02738/01-01-2000-DBT-S01258-MAMMO diagnostic digital bilateral-71538/14275.000000-97275/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 155/200 [1:46:33<35:07, 46.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02738/01-01-2000-DBT-S01258-MAMMO diagnostic digital bilateral-71538/14276.000000-15205/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 156/200 [1:47:19<34:08, 46.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04858/01-01-2000-DBT-S04555-MAMMO SCREENING BREAST TOMOSYNTHESIS-89444/5243.000000-08718/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 157/200 [1:47:56<31:15, 43.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04858/01-01-2000-DBT-S04555-MAMMO SCREENING BREAST TOMOSYNTHESIS-89444/5242.000000-38850/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 79%|███████▉  | 158/200 [1:48:47<32:01, 45.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00225/01-01-2000-DBT-S02346-MAMMO DIAGNOSTIC DIGITAL BILATERAL-28434/6189.000000-66774/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|███████▉  | 159/200 [1:49:45<33:52, 49.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00225/01-01-2000-DBT-S02346-MAMMO DIAGNOSTIC DIGITAL BILATERAL-28434/6190.000000-67794/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 160/200 [1:50:28<31:41, 47.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04710/01-01-2000-DBT-S03227-MAMMO SCREENING BREAST TOMOSYNTHESIS-24983/4960.000000-86967/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 161/200 [1:51:19<31:35, 48.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04710/01-01-2000-DBT-S03227-MAMMO SCREENING BREAST TOMOSYNTHESIS-24983/4959.000000-38406/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81%|████████  | 162/200 [1:52:28<34:37, 54.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02176/01-01-2000-DBT-S03078-MAMMO screening digital bilateral-69304/19230.000000-75419/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 163/200 [1:53:32<35:31, 57.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02176/01-01-2000-DBT-S03078-MAMMO screening digital bilateral-69304/19229.000000-39737/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 164/200 [1:54:06<30:16, 50.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03203/01-01-2000-DBT-S04862-MAMMO diagnostic digital bilateral-92733/14304.000000-59631/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▎ | 165/200 [1:54:48<27:56, 47.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03203/01-01-2000-DBT-S04862-MAMMO diagnostic digital bilateral-92733/14305.000000-63473/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 166/200 [1:55:33<26:43, 47.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P05047/01-01-2000-DBT-S05588-MAMMO SCREENING DIGITAL BILATERAL-88035/20309.000000-18632/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▎ | 167/200 [1:56:26<26:48, 48.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P05047/01-01-2000-DBT-S05588-MAMMO SCREENING DIGITAL BILATERAL-88035/20308.000000-79110/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 168/200 [1:56:51<22:13, 41.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03222/01-01-2000-DBT-S00931-MAMMO diagnostic digital bilateral-57777/17348.000000-97916/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 169/200 [1:57:16<19:00, 36.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03222/01-01-2000-DBT-S00931-MAMMO diagnostic digital bilateral-57777/17347.000000-50713/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 170/200 [1:57:40<16:24, 32.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03978/01-01-2000-DBT-S00442-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-62824/12449.000000-68028/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 171/200 [1:58:00<14:00, 28.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03978/01-01-2000-DBT-S00442-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-62824/12448.000000-52179/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 172/200 [1:58:32<13:59, 29.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02582/01-01-2000-DBT-S01997-MAMMO diagnostic digital bilateral-98922/3865.000000-48384/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▋ | 173/200 [1:58:57<12:49, 28.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02582/01-01-2000-DBT-S01997-MAMMO diagnostic digital bilateral-98922/3864.000000-27277/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 174/200 [2:00:18<19:05, 44.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01801/01-01-2000-DBT-S04999-MAMMO diagnostic digital right-47139/840.000000-69817/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 175/200 [2:01:30<21:53, 52.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01801/01-01-2000-DBT-S04999-MAMMO diagnostic digital right-47139/839.000000-76312/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 176/200 [2:02:07<19:09, 47.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04721/01-01-2000-DBT-S01833-MAMMO screening digital bilateral-47309/2889.000000-50432/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 177/200 [2:02:34<15:57, 41.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04721/01-01-2000-DBT-S01833-MAMMO screening digital bilateral-47309/2890.000000-83373/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89%|████████▉ | 178/200 [2:03:06<14:11, 38.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01673/01-01-2000-DBT-S04347-MAMMO diagnostic digital bilateral-30029/12558.000000-63058/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|████████▉ | 179/200 [2:03:39<12:57, 37.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01673/01-01-2000-DBT-S04347-MAMMO diagnostic digital bilateral-30029/12559.000000-24133/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 180/200 [2:04:26<13:22, 40.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01624/01-01-2000-DBT-S03518-MAMMO screening digital bilateral-19315/5179.000000-07086/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 181/200 [2:05:04<12:28, 39.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01624/01-01-2000-DBT-S03518-MAMMO screening digital bilateral-19315/5180.000000-11459/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%|█████████ | 182/200 [2:05:39<11:25, 38.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04631/01-01-2000-DBT-S05515-MAMMO diagnostic digital bilateral-84562/2789.000000-47217/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 183/200 [2:06:35<12:18, 43.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04631/01-01-2000-DBT-S05515-MAMMO diagnostic digital bilateral-84562/2790.000000-75744/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 184/200 [2:06:59<10:01, 37.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00827/01-01-2000-DBT-S01731-MAMMO screening digital bilateral-65653/779.000000-59014/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▎| 185/200 [2:07:25<08:33, 34.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P00827/01-01-2000-DBT-S01731-MAMMO screening digital bilateral-65653/780.000000-58797/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 186/200 [2:07:50<07:19, 31.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03423/01-01-2000-DBT-S05305-MAMMO diagnostic digital bilateral-27904/1071.000000-36189/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▎| 187/200 [2:08:12<06:10, 28.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P03423/01-01-2000-DBT-S05305-MAMMO diagnostic digital bilateral-27904/1072.000000-36946/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 188/200 [2:08:45<05:56, 29.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01262/01-01-2000-DBT-S00537-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-88617/5360.000000-70054/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 189/200 [2:09:24<06:00, 32.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01262/01-01-2000-DBT-S00537-MAMMO SCREEN BREAST TOMOSYNTHESIS BILATERAL-88617/5361.000000-47955/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 190/200 [2:09:53<05:15, 31.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01267/01-01-2000-DBT-S03415-MAMMO diagnostic digital bilateral-33927/19688.000000-16007/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 191/200 [2:10:18<04:26, 29.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01267/01-01-2000-DBT-S03415-MAMMO diagnostic digital bilateral-33927/19689.000000-24374/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 192/200 [2:11:46<06:17, 47.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02798/01-01-2000-DBT-S01770-MAMMO diagnostic digital bilateral-60802/4654.000000-88875/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▋| 193/200 [2:13:41<07:51, 67.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02798/01-01-2000-DBT-S01770-MAMMO diagnostic digital bilateral-60802/4653.000000-73932/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 194/200 [2:15:04<07:12, 72.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P02798/01-01-2000-DBT-S01770-MAMMO diagnostic digital bilateral-60802/4652.000000-73193/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 195/200 [2:15:33<04:56, 59.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01488/01-01-2000-DBT-S00017-MAMMO SCREENING DIGITAL BILATERAL-90520/9196.000000-13599/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 196/200 [2:16:09<03:29, 52.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P01488/01-01-2000-DBT-S00017-MAMMO SCREENING DIGITAL BILATERAL-90520/9197.000000-04594/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 197/200 [2:16:54<02:29, 49.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04090/01-01-2000-DBT-S01718-MAMMO diagnostic digital bilateral-51968/3137.000000-95562/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 99%|█████████▉| 198/200 [2:17:42<01:39, 49.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P04090/01-01-2000-DBT-S01718-MAMMO diagnostic digital bilateral-51968/3138.000000-94838/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|█████████▉| 199/200 [2:18:19<00:45, 45.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P05022/01-01-2000-DBT-S05195-MAMMO diagnostic digital bilateral-02818/10582.000000-26511/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [2:19:01<00:00, 41.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/dataset/Breast-Cancer-Screening-DBT/DBT-P05022/01-01-2000-DBT-S05195-MAMMO diagnostic digital bilateral-02818/10583.000000-11197/1-1.dcm: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pydicom\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skimage.exposure import rescale_intensity\n",
        "import nibabel as nib\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import random\n",
        "\n",
        "def load_dicom_volume(dicom_file):\n",
        "    ds = pydicom.dcmread(dicom_file)\n",
        "    volume = ds.pixel_array\n",
        "    volume = rescale_intensity(volume, out_range=(0, 255)).astype(np.uint8)\n",
        "    return volume\n",
        "\n",
        "def extract_3d_patch(volume, center, patch_size=(32, 32, 32)):\n",
        "    z, y, x = center\n",
        "    d, h, w = patch_size\n",
        "    z_start, z_end = max(0, z - d // 2), min(volume.shape[0], z + d // 2)\n",
        "    y_start, y_end = max(0, y - h // 2), min(volume.shape[1], y + h // 2)\n",
        "    x_start, x_end = max(0, x - w // 2), min(volume.shape[2], x + w // 2)\n",
        "    patch = volume[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    return patch\n",
        "\n",
        "import pydicom\n",
        "\n",
        "def get_patient_and_study_id(dicom_file_path):\n",
        "    \"\"\"\n",
        "    Extract Patient ID and Study ID from a DICOM file.\n",
        "\n",
        "    Parameters:\n",
        "    - dicom_file_path: Path to the DICOM file.\n",
        "\n",
        "    Returns:\n",
        "    - patient_id: The Patient ID extracted from the DICOM file.\n",
        "    - study_id: The Study ID extracted from the DICOM file.\n",
        "    \"\"\"\n",
        "    ds = pydicom.dcmread(dicom_file_path)\n",
        "    patient_id = ds.PatientID\n",
        "    study_uid = ds.StudyInstanceUID\n",
        "    return patient_id, study_uid\n",
        "\n",
        "# Example DICOM file path\n",
        "dicom_file_path = \"/content/dataset/Breast-Cancer-Screening-DBT/DBT-P05022/01-01-2000-DBT-S05195-MAMMO diagnostic digital bilateral-02818/10583.000000-11197/1-1.dcm\"\n",
        "\n",
        "# Get Patient ID and Study ID\n",
        "patient_id, study_uid = get_patient_and_study_id(dicom_file_path)\n",
        "\n",
        "print(f\"Patient ID: {patient_id}\")\n",
        "print(f\"Study ID: {study_uid}\")\n",
        "\n",
        "\n",
        "# Define path to save 3D patches\n",
        "save_path = '/content/3d_patches_nii_2/'\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Load the CSV files\n",
        "path_table = pd.read_csv('/content/drive/MyDrive/Praktikum/filepaths.csv')\n",
        "box_table = pd.read_csv('/content/drive/MyDrive/Praktikum/boxes.csv')\n",
        "label_list = pd.read_csv('/content/drive/MyDrive/Praktikum/labels.csv')\n",
        "\n",
        "# Create a DataFrame to save metadata\n",
        "df = pd.DataFrame(columns=['PatientID', 'StudyUID', 'view', 'img_path', 'Normal', 'Actionable', 'Benign', 'Cancer'])\n",
        "\n",
        "# Define patch size\n",
        "patch_size = (32, 32, 32)\n",
        "\n",
        "# Process each DICOM file\n",
        "for dicom_file in tqdm(dicom_files):\n",
        "    try:\n",
        "        base_name = os.path.basename(dicom_file).replace('.dcm', '')\n",
        "        matching_row = path_table[path_table['descriptive_path'].str.contains(base_name)]\n",
        "\n",
        "        if not matching_row.empty:\n",
        "            patient_id, study_uid = get_patient_and_study_id(dicom_file)\n",
        "            view = matching_row['View'].values[0]\n",
        "            side = view[0]\n",
        "\n",
        "            volume = load_dicom_volume(dicom_file)\n",
        "\n",
        "            col1 = box_table['StudyUID'] == study_uid\n",
        "            col2 = box_table['View'] == view\n",
        "            col_final = box_table[col1 & col2]\n",
        "\n",
        "            if len(col_final) == 0:  # No nodule\n",
        "                center_slice = volume.shape[0] // 2\n",
        "                z = center_slice\n",
        "                y = volume.shape[1] // 2\n",
        "                x = volume.shape[2] // 2\n",
        "                center = (z, y, x)\n",
        "                random_angle = random.randint(0, 360)\n",
        "                patch = extract_3d_patch(volume, center, patch_size)\n",
        "                slice_name = f\"{patient_id}_{study_uid}_{view}_{center_slice}_angle_{random_angle}.nii.gz\"\n",
        "                patch_nifti = nib.Nifti1Image(patch, np.eye(4))\n",
        "                nib.save(patch_nifti, os.path.join(save_path, slice_name))\n",
        "                df.loc[len(df)] = [patient_id, study_uid, view, os.path.join(save_path, slice_name), label_list.loc[col1 & col2, 'Normal'].values[0], label_list.loc[col1 & col2, 'Actionable'].values[0], label_list.loc[col1 & col2, 'Benign'].values[0], label_list.loc[col1 & col2, 'Cancer'].values[0]]\n",
        "            else:  # Nodule present\n",
        "                for _, row in col_final.iterrows():\n",
        "                    z = row['Slice']\n",
        "                    y = row['Y']\n",
        "                    x = row['X']\n",
        "                    center = (z, y, x)\n",
        "                    random_angle = random.randint(0, 360)\n",
        "                    patch = extract_3d_patch(volume, center, patch_size)\n",
        "                    slice_name = f\"{patient_id}_{study_uid}_{view}_{z}_angle_{random_angle}.nii.gz\"\n",
        "                    patch_nifti = nib.Nifti1Image(patch, np.eye(4))\n",
        "                    nib.save(patch_nifti, os.path.join(save_path, slice_name))\n",
        "                    df.loc[len(df)] = [patient_id, study_uid, view, os.path.join(save_path, slice_name), label_list.loc[col1 & col2, 'Normal'].values[0], label_list.loc[col1 & col2, 'Actionable'].values[0], label_list.loc[col1 & col2, 'Benign'].values[0], label_list.loc[col1 & col2, 'Cancer'].values[0]]\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {dicom_file}: {e}\")\n",
        "    gc.collect()\n",
        "\n",
        "!cp -r /content/3d_patches_nii_2 /content/drive/MyDrive/Praktikum/3d_dataset_3/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Phase"
      ],
      "metadata": {
        "id": "OMovuOihDubV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install monai torch torchvision nibabel tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwKeHvknKb5W",
        "outputId": "2c37bd23-9088-445b-9d7d-7222cdae4a74"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting monai\n",
            "  Downloading monai-1.3.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cpu)\n",
            "Collecting nibabel\n",
            "  Downloading nibabel-5.2.1-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from monai) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: packaging>=17 in /usr/local/lib/python3.10/dist-packages (from nibabel) (24.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nibabel, monai\n",
            "Successfully installed monai-1.3.1 nibabel-5.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Without metrics saving"
      ],
      "metadata": {
        "id": "fhRS2yfNVG_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_df = pd.read_csv('/content/drive/MyDrive/Praktikum/labels.csv')"
      ],
      "metadata": {
        "id": "4CSvSsewOZ6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import monai\n",
        "from monai.data import ImageDataset, DataLoader\n",
        "from monai.transforms import EnsureChannelFirst, Compose, RandRotate90, Resize, ScaleIntensity\n",
        "import pandas as pd\n",
        "\n",
        "def load_image_paths_and_labels(data_dir, labels_csv):\n",
        "    \"\"\"\n",
        "    Load image paths and corresponding labels from the directory and CSV file.\n",
        "\n",
        "    Args:\n",
        "    - data_dir (str): Directory containing the image files.\n",
        "    - labels_csv (str): Path to the CSV file containing labels.\n",
        "\n",
        "    Returns:\n",
        "    - image_paths (List[str]): List of image file paths.\n",
        "    - labels (List[int]): List of labels corresponding to the image file paths.\n",
        "    \"\"\"\n",
        "    # Read labels CSV\n",
        "    labels_df = pd.read_csv(labels_csv)\n",
        "\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    for _, row in labels_df.iterrows():\n",
        "        patient_id = row['PatientID']\n",
        "        label = row['Cancer']\n",
        "        # Find all image files for the patient\n",
        "        patient_images = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.startswith(patient_id)]\n",
        "        image_paths.extend(patient_images)\n",
        "        labels.extend([label] * len(patient_images))\n",
        "\n",
        "    return image_paths, labels\n",
        "\n",
        "def main():\n",
        "    monai.config.print_config()\n",
        "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "\n",
        "    # Define data paths\n",
        "    data_dir = '/content/3d_patches_nii/'  # Directory containing the image files\n",
        "    labels_csv = '/content/drive/MyDrive/Praktikum/labels.csv'  # Path to the CSV file containing labels\n",
        "\n",
        "    # Load image paths and labels\n",
        "    images, labels = load_image_paths_and_labels(data_dir, labels_csv)\n",
        "    labels = np.array(labels, dtype=np.int64)\n",
        "\n",
        "    # Define transforms\n",
        "    train_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(), Resize((96, 96, 96)), RandRotate90()])\n",
        "    val_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(), Resize((96, 96, 96))])\n",
        "\n",
        "    # Define image dataset, data loader\n",
        "    check_ds = ImageDataset(image_files=images, labels=labels, transform=train_transforms)\n",
        "    check_loader = DataLoader(check_ds, batch_size=2, num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "    im, label = monai.utils.misc.first(check_loader)\n",
        "    print(type(im), im.shape, label)\n",
        "\n",
        "    # Split dataset into training and validation sets\n",
        "    train_size = int(0.8 * len(images))\n",
        "    val_size = len(images) - train_size\n",
        "    train_images, val_images = images[:train_size], images[train_size:]\n",
        "    train_labels, val_labels = labels[:train_size], labels[train_size:]\n",
        "\n",
        "    # create a training data loader\n",
        "    train_ds = ImageDataset(image_files=train_images, labels=train_labels, transform=train_transforms)\n",
        "    train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "    # create a validation data loader\n",
        "    val_ds = ImageDataset(image_files=val_images, labels=val_labels, transform=val_transforms)\n",
        "    val_loader = DataLoader(val_ds, batch_size=2, num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "    # Create DenseNet121, CrossEntropyLoss and Adam optimizer\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = monai.networks.nets.DenseNet121(spatial_dims=3, in_channels=1, out_channels=2).to(device)\n",
        "    loss_function = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), 1e-5)\n",
        "\n",
        "    # start a typical PyTorch training\n",
        "    val_interval = 2\n",
        "    best_metric = -1\n",
        "    epoch_loss_values = list()\n",
        "    metric_values = list()\n",
        "    writer = SummaryWriter()\n",
        "    for epoch in range(5):\n",
        "        print(\"-\" * 10)\n",
        "        print(f\"epoch {epoch + 1}/{5}\")\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        step = 0\n",
        "        for batch_data in train_loader:\n",
        "            step += 1\n",
        "            inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_len = len(train_ds) // train_loader.batch_size\n",
        "            print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
        "            writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
        "        epoch_loss /= step\n",
        "        epoch_loss_values.append(epoch_loss)\n",
        "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        if (epoch + 1) % val_interval == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                num_correct = 0.0\n",
        "                metric_count = 0\n",
        "                for val_data in val_loader:\n",
        "                    val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
        "                    val_outputs = model(val_images)\n",
        "                    value = torch.eq(val_outputs.argmax(dim=1), val_labels)\n",
        "                    metric_count += len(value)\n",
        "                    num_correct += value.sum().item()\n",
        "                metric = num_correct / metric_count\n",
        "                metric_values.append(metric)\n",
        "                if metric > best_metric:\n",
        "                    best_metric = metric\n",
        "                    best_metric_epoch = epoch + 1\n",
        "                    torch.save(model.state_dict(), \"best_metric_model_classification3d_array.pth\")\n",
        "                    print(\"saved new best metric model\")\n",
        "                print(\n",
        "                    \"current epoch: {} current accuracy: {:.4f} best accuracy: {:.4f} at epoch {}\".format(\n",
        "                        epoch + 1, metric, best_metric, best_metric_epoch\n",
        "                    )\n",
        "                )\n",
        "                writer.add_scalar(\"val_accuracy\", metric, epoch + 1)\n",
        "    print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
        "    writer.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ZjF3o3HgPWM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##With Metrics Saving"
      ],
      "metadata": {
        "id": "FstwCK1MVJkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import monai\n",
        "from monai.data import ImageDataset, DataLoader\n",
        "from monai.transforms import EnsureChannelFirst, Compose, RandRotate90, Resize, ScaleIntensity\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, f1_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_image_paths_and_labels(data_dir, labels_csv):\n",
        "    \"\"\"\n",
        "    Load image paths and corresponding labels from the directory and CSV file.\n",
        "\n",
        "    Args:\n",
        "    - data_dir (str): Directory containing the image files.\n",
        "    - labels_csv (str): Path to the CSV file containing labels.\n",
        "\n",
        "    Returns:\n",
        "    - image_paths (List[str]): List of image file paths.\n",
        "    - labels (List[int]): List of labels corresponding to the image file paths.\n",
        "    \"\"\"\n",
        "    # Read labels CSV\n",
        "    labels_df = pd.read_csv(labels_csv)\n",
        "\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    for _, row in labels_df.iterrows():\n",
        "        patient_id = row['PatientID']\n",
        "        label = row['Cancer']\n",
        "        # Find all image files for the patient\n",
        "        patient_images = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.startswith(patient_id)]\n",
        "        image_paths.extend(patient_images)\n",
        "        labels.extend([label] * len(patient_images))\n",
        "\n",
        "    return image_paths, labels\n",
        "\n",
        "def plot_and_save_metrics(epochs, train_metrics, val_metrics, metric_name, save_path):\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, train_metrics, label=f'Train {metric_name}')\n",
        "    plt.plot(epochs, val_metrics, label=f'Validation {metric_name}')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.legend()\n",
        "    plt.title(f'Train and Validation {metric_name}')\n",
        "    plt.savefig(os.path.join(save_path, f'{metric_name}.png'))\n",
        "\n",
        "def main():\n",
        "    monai.config.print_config()\n",
        "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "\n",
        "    # Define data paths\n",
        "    data_dir = '/content/drive/MyDrive/Praktikum/3d_dataset_2/3d_patches_nii/'  # Directory containing the image files\n",
        "    labels_csv = '/content/drive/MyDrive/Praktikum/labels.csv'  # Path to the CSV file containing labels\n",
        "\n",
        "    # Load image paths and labels\n",
        "    images, labels = load_image_paths_and_labels(data_dir, labels_csv)\n",
        "    labels = np.array(labels, dtype=np.int64)\n",
        "\n",
        "    # Debugging: Print number of images and labels\n",
        "    print(f\"Number of images: {len(images)}, Number of labels: {len(labels)}\")\n",
        "    print(f\"Labels distribution: {np.bincount(labels)}\")\n",
        "\n",
        "    # Define transforms\n",
        "    train_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(), Resize((96, 96, 96)), RandRotate90()])\n",
        "    val_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(), Resize((96, 96, 96))])\n",
        "\n",
        "    # Initialize metrics storage\n",
        "    metrics = {\n",
        "        'accuracy': [],\n",
        "        'recall': [],\n",
        "        'auc': [],\n",
        "        'f1': [],\n",
        "        'mcc': []\n",
        "    }\n",
        "\n",
        "    # Perform k-fold cross-validation\n",
        "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold = 0\n",
        "\n",
        "    for train_indices, val_indices in kfold.split(images):\n",
        "        fold += 1\n",
        "        print(f'Fold {fold}')\n",
        "\n",
        "        # Create datasets and dataloaders for the current fold\n",
        "        train_images = [images[i] for i in train_indices]\n",
        "        val_images = [images[i] for i in val_indices]\n",
        "        train_labels = labels[train_indices]\n",
        "        val_labels = labels[val_indices]\n",
        "\n",
        "        # Debugging: Print train and validation label distributions\n",
        "        print(f\"Train labels distribution: {np.bincount(train_labels)}\")\n",
        "        print(f\"Validation labels distribution: {np.bincount(val_labels)}\")\n",
        "\n",
        "        train_ds = ImageDataset(image_files=train_images, labels=train_labels, transform=train_transforms)\n",
        "        val_ds = ImageDataset(image_files=val_images, labels=val_labels, transform=val_transforms)\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "        val_loader = DataLoader(val_ds, batch_size=2, num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "        # Create DenseNet121, CrossEntropyLoss and Adam optimizer\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = monai.networks.nets.DenseNet121(spatial_dims=3, in_channels=1, out_channels=2).to(device)\n",
        "        loss_function = torch.nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), 1e-5)\n",
        "\n",
        "        val_interval = 2\n",
        "        best_metric = -1\n",
        "        epoch_loss_values = list()\n",
        "        epoch_accuracy_values = list()\n",
        "        epoch_recall_values = list()\n",
        "        epoch_auc_values = list()\n",
        "        epoch_f1_values = list()\n",
        "        epoch_mcc_values = list()\n",
        "        writer = SummaryWriter()\n",
        "\n",
        "        for epoch in range(50):\n",
        "            print(\"-\" * 10)\n",
        "            print(f\"epoch {epoch + 1}/{5}\")\n",
        "            model.train()\n",
        "            epoch_loss = 0\n",
        "            step = 0\n",
        "            for batch_data in train_loader:\n",
        "                step += 1\n",
        "                inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = loss_function(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "                epoch_len = len(train_ds) // train_loader.batch_size\n",
        "                print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
        "                writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
        "            epoch_loss /= step\n",
        "            epoch_loss_values.append(epoch_loss)\n",
        "            print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
        "\n",
        "            if (epoch + 1) % val_interval == 0:\n",
        "                model.eval()\n",
        "                val_preds = []\n",
        "                val_true = []\n",
        "                with torch.no_grad():\n",
        "                    num_correct = 0.0\n",
        "                    metric_count = 0\n",
        "                    for val_data in val_loader:\n",
        "                        val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
        "                        val_outputs = model(val_images)\n",
        "                        val_preds.extend(val_outputs.argmax(dim=1).cpu().numpy())\n",
        "                        val_true.extend(val_labels.cpu().numpy())\n",
        "                        value = torch.eq(val_outputs.argmax(dim=1), val_labels)\n",
        "                        metric_count += len(value)\n",
        "                        num_correct += value.sum().item()\n",
        "                accuracy = accuracy_score(val_true, val_preds)\n",
        "                recall = recall_score(val_true, val_preds)\n",
        "                auc = roc_auc_score(val_true, val_preds)\n",
        "                f1 = f1_score(val_true, val_preds)\n",
        "                mcc = matthews_corrcoef(val_true, val_preds)\n",
        "\n",
        "                epoch_accuracy_values.append(accuracy)\n",
        "                epoch_recall_values.append(recall)\n",
        "                epoch_auc_values.append(auc)\n",
        "                epoch_f1_values.append(f1)\n",
        "                epoch_mcc_values.append(mcc)\n",
        "\n",
        "                if accuracy > best_metric:\n",
        "                    best_metric = accuracy\n",
        "                    best_metric_epoch = epoch + 1\n",
        "                    torch.save(model.state_dict(), f\"best_metric_model_classification3d_fold_50E_{fold}.pth\")\n",
        "                    print(\"saved new best metric model\")\n",
        "                print(\n",
        "                    f\"current epoch: {epoch + 1} current accuracy: {accuracy:.4f} best accuracy: {best_metric:.4f} at epoch {best_metric_epoch}\"\n",
        "                )\n",
        "                writer.add_scalar(\"val_accuracy\", accuracy, epoch + 1)\n",
        "\n",
        "        metrics['accuracy'].append(epoch_accuracy_values)\n",
        "        metrics['recall'].append(epoch_recall_values)\n",
        "        metrics['auc'].append(epoch_auc_values)\n",
        "        metrics['f1'].append(epoch_f1_values)\n",
        "        metrics['mcc'].append(epoch_mcc_values)\n",
        "\n",
        "    # Save metrics as PNG\n",
        "    for metric_name, metric_values in metrics.items():\n",
        "        plot_and_save_metrics(list(range(1, 6)), metric_values, metric_name, '.')\n",
        "\n",
        "    print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
        "    writer.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_p7FSKWTVMEZ",
        "outputId": "e2eba540-a974-40b1-ddbd-c3a3b7a05f34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MONAI version: 1.3.1\n",
            "Numpy version: 1.25.2\n",
            "Pytorch version: 2.3.0+cpu\n",
            "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
            "MONAI rev id: 96bfda00c6bd290297f5e3514ea227c6be4d08b4\n",
            "MONAI __file__: /usr/local/lib/python3.10/dist-packages/monai/__init__.py\n",
            "\n",
            "Optional dependencies:\n",
            "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "Nibabel version: 5.2.1\n",
            "scikit-image version: 0.19.3\n",
            "scipy version: 1.11.4\n",
            "Pillow version: 10.3.0\n",
            "Tensorboard version: 2.15.2\n",
            "gdown version: 4.7.3\n",
            "TorchVision version: 0.18.0+cpu\n",
            "tqdm version: 4.66.4\n",
            "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "psutil version: 5.9.5\n",
            "pandas version: 2.0.3\n",
            "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "transformers version: 4.41.2\n",
            "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "\n",
            "For details about installing the optional dependencies, please visit:\n",
            "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
            "\n",
            "Number of images: 422, Number of labels: 422\n",
            "Labels distribution: [264 158]\n",
            "Fold 1\n",
            "Train labels distribution: [215 122]\n",
            "Validation labels distribution: [49 36]\n",
            "----------\n",
            "epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.7038\n",
            "2/168, train_loss: 0.8605\n",
            "3/168, train_loss: 0.7368\n",
            "4/168, train_loss: 0.6559\n",
            "5/168, train_loss: 0.6631\n",
            "6/168, train_loss: 0.6993\n",
            "7/168, train_loss: 0.6534\n",
            "8/168, train_loss: 0.7336\n",
            "9/168, train_loss: 0.7342\n",
            "10/168, train_loss: 0.6573\n",
            "11/168, train_loss: 0.7426\n",
            "12/168, train_loss: 0.7298\n",
            "13/168, train_loss: 0.7326\n",
            "14/168, train_loss: 0.7159\n",
            "15/168, train_loss: 0.6479\n",
            "16/168, train_loss: 0.7942\n",
            "17/168, train_loss: 0.7081\n",
            "18/168, train_loss: 0.6928\n",
            "19/168, train_loss: 0.6700\n",
            "20/168, train_loss: 0.7038\n",
            "21/168, train_loss: 0.7069\n",
            "22/168, train_loss: 0.7055\n",
            "23/168, train_loss: 0.6321\n",
            "24/168, train_loss: 0.7119\n",
            "25/168, train_loss: 0.7021\n",
            "26/168, train_loss: 0.6633\n",
            "27/168, train_loss: 0.5073\n",
            "28/168, train_loss: 0.6927\n",
            "29/168, train_loss: 0.7406\n",
            "30/168, train_loss: 0.7163\n",
            "31/168, train_loss: 0.7262\n",
            "32/168, train_loss: 0.7152\n",
            "33/168, train_loss: 0.6828\n",
            "34/168, train_loss: 0.7029\n",
            "35/168, train_loss: 0.9339\n",
            "36/168, train_loss: 0.7061\n",
            "37/168, train_loss: 0.5071\n",
            "38/168, train_loss: 0.6900\n",
            "39/168, train_loss: 0.6846\n",
            "40/168, train_loss: 0.6808\n",
            "41/168, train_loss: 0.6935\n",
            "42/168, train_loss: 0.5140\n",
            "43/168, train_loss: 0.6724\n",
            "44/168, train_loss: 0.7700\n",
            "45/168, train_loss: 0.7192\n",
            "46/168, train_loss: 0.6736\n",
            "47/168, train_loss: 0.6937\n",
            "48/168, train_loss: 0.6528\n",
            "49/168, train_loss: 0.7281\n",
            "50/168, train_loss: 0.7304\n",
            "51/168, train_loss: 0.6575\n",
            "52/168, train_loss: 0.9458\n",
            "53/168, train_loss: 0.6940\n",
            "54/168, train_loss: 0.6352\n",
            "55/168, train_loss: 0.6283\n",
            "56/168, train_loss: 0.6473\n",
            "57/168, train_loss: 0.6945\n",
            "58/168, train_loss: 0.6614\n",
            "59/168, train_loss: 0.6164\n",
            "60/168, train_loss: 0.7541\n",
            "61/168, train_loss: 0.7507\n",
            "62/168, train_loss: 0.7483\n",
            "63/168, train_loss: 0.6409\n",
            "64/168, train_loss: 0.6568\n",
            "65/168, train_loss: 0.8049\n",
            "66/168, train_loss: 0.6475\n",
            "67/168, train_loss: 0.7754\n",
            "68/168, train_loss: 0.6193\n",
            "69/168, train_loss: 0.6344\n",
            "70/168, train_loss: 0.6076\n",
            "71/168, train_loss: 0.6947\n",
            "72/168, train_loss: 0.6249\n",
            "73/168, train_loss: 0.7783\n",
            "74/168, train_loss: 0.6392\n",
            "75/168, train_loss: 0.7514\n",
            "76/168, train_loss: 0.7980\n",
            "77/168, train_loss: 0.6274\n",
            "78/168, train_loss: 0.6266\n",
            "79/168, train_loss: 0.7926\n",
            "80/168, train_loss: 0.6261\n",
            "81/168, train_loss: 0.7777\n",
            "82/168, train_loss: 0.5998\n",
            "83/168, train_loss: 0.7640\n",
            "84/168, train_loss: 0.6277\n",
            "85/168, train_loss: 0.7661\n",
            "86/168, train_loss: 0.7074\n",
            "87/168, train_loss: 0.6136\n",
            "88/168, train_loss: 0.7709\n",
            "89/168, train_loss: 0.6115\n",
            "90/168, train_loss: 0.6361\n",
            "91/168, train_loss: 0.6752\n",
            "92/168, train_loss: 0.6986\n",
            "93/168, train_loss: 0.6021\n",
            "94/168, train_loss: 0.6737\n",
            "95/168, train_loss: 0.6843\n",
            "96/168, train_loss: 0.7025\n",
            "97/168, train_loss: 0.6952\n",
            "98/168, train_loss: 0.6948\n",
            "99/168, train_loss: 0.6373\n",
            "100/168, train_loss: 0.6028\n",
            "101/168, train_loss: 0.6191\n",
            "102/168, train_loss: 0.8085\n",
            "103/168, train_loss: 0.6569\n",
            "104/168, train_loss: 0.6604\n",
            "105/168, train_loss: 0.6974\n",
            "106/168, train_loss: 0.8044\n",
            "107/168, train_loss: 0.6827\n",
            "108/168, train_loss: 0.5905\n",
            "109/168, train_loss: 0.6115\n",
            "110/168, train_loss: 0.5909\n",
            "111/168, train_loss: 0.7783\n",
            "112/168, train_loss: 0.7637\n",
            "113/168, train_loss: 0.7541\n",
            "114/168, train_loss: 0.5712\n",
            "115/168, train_loss: 0.6120\n",
            "116/168, train_loss: 0.6815\n",
            "117/168, train_loss: 0.6001\n",
            "118/168, train_loss: 0.7918\n",
            "119/168, train_loss: 0.6599\n",
            "120/168, train_loss: 0.6921\n",
            "121/168, train_loss: 0.7874\n",
            "122/168, train_loss: 0.6164\n",
            "123/168, train_loss: 0.6102\n",
            "124/168, train_loss: 0.5988\n",
            "125/168, train_loss: 0.7894\n",
            "126/168, train_loss: 0.6182\n",
            "127/168, train_loss: 0.6155\n",
            "128/168, train_loss: 0.5751\n",
            "129/168, train_loss: 0.7639\n",
            "130/168, train_loss: 0.6589\n",
            "131/168, train_loss: 0.7950\n",
            "132/168, train_loss: 0.6093\n",
            "133/168, train_loss: 0.5858\n",
            "134/168, train_loss: 0.5849\n",
            "135/168, train_loss: 0.8217\n",
            "136/168, train_loss: 0.6116\n",
            "137/168, train_loss: 0.5795\n",
            "138/168, train_loss: 0.5793\n",
            "139/168, train_loss: 0.7539\n",
            "140/168, train_loss: 0.7131\n",
            "141/168, train_loss: 0.5702\n",
            "142/168, train_loss: 0.7437\n",
            "143/168, train_loss: 0.7420\n",
            "144/168, train_loss: 0.5985\n",
            "145/168, train_loss: 0.6214\n",
            "146/168, train_loss: 0.5717\n",
            "147/168, train_loss: 0.6994\n",
            "148/168, train_loss: 0.6419\n",
            "149/168, train_loss: 0.7490\n",
            "150/168, train_loss: 0.8307\n",
            "151/168, train_loss: 0.7033\n",
            "152/168, train_loss: 0.6990\n",
            "153/168, train_loss: 0.8117\n",
            "154/168, train_loss: 0.6069\n",
            "155/168, train_loss: 0.6980\n",
            "156/168, train_loss: 0.7790\n",
            "157/168, train_loss: 0.6971\n",
            "158/168, train_loss: 0.5835\n",
            "159/168, train_loss: 0.5828\n",
            "160/168, train_loss: 0.5880\n",
            "161/168, train_loss: 0.5775\n",
            "162/168, train_loss: 0.5791\n",
            "163/168, train_loss: 0.6026\n",
            "164/168, train_loss: 0.6131\n",
            "165/168, train_loss: 0.6992\n",
            "166/168, train_loss: 0.5807\n",
            "167/168, train_loss: 0.6851\n",
            "168/168, train_loss: 0.5688\n",
            "169/168, train_loss: 0.5690\n",
            "epoch 1 average loss: 0.6811\n",
            "----------\n",
            "epoch 2/5\n",
            "1/168, train_loss: 0.6103\n",
            "2/168, train_loss: 0.5530\n",
            "3/168, train_loss: 0.5594\n",
            "4/168, train_loss: 0.6197\n",
            "5/168, train_loss: 0.8365\n",
            "6/168, train_loss: 0.5451\n",
            "7/168, train_loss: 0.8509\n",
            "8/168, train_loss: 0.8364\n",
            "9/168, train_loss: 0.7571\n",
            "10/168, train_loss: 0.7814\n",
            "11/168, train_loss: 0.8539\n",
            "12/168, train_loss: 0.5472\n",
            "13/168, train_loss: 0.5624\n",
            "14/168, train_loss: 0.8291\n",
            "15/168, train_loss: 0.7229\n",
            "16/168, train_loss: 0.5618\n",
            "17/168, train_loss: 0.5399\n",
            "18/168, train_loss: 0.7367\n",
            "19/168, train_loss: 0.7061\n",
            "20/168, train_loss: 0.5604\n",
            "21/168, train_loss: 0.5299\n",
            "22/168, train_loss: 0.8937\n",
            "23/168, train_loss: 0.6295\n",
            "24/168, train_loss: 0.5369\n",
            "25/168, train_loss: 0.7589\n",
            "26/168, train_loss: 0.6555\n",
            "27/168, train_loss: 0.6220\n",
            "28/168, train_loss: 0.8473\n",
            "29/168, train_loss: 0.7039\n",
            "30/168, train_loss: 0.8701\n",
            "31/168, train_loss: 0.5541\n",
            "32/168, train_loss: 0.5551\n",
            "33/168, train_loss: 0.7021\n",
            "34/168, train_loss: 0.5519\n",
            "35/168, train_loss: 0.7222\n",
            "36/168, train_loss: 0.6729\n",
            "37/168, train_loss: 0.8661\n",
            "38/168, train_loss: 0.6513\n",
            "39/168, train_loss: 0.5983\n",
            "40/168, train_loss: 0.5731\n",
            "41/168, train_loss: 0.8817\n",
            "42/168, train_loss: 0.5728\n",
            "43/168, train_loss: 0.5621\n",
            "44/168, train_loss: 0.5568\n",
            "45/168, train_loss: 0.5598\n",
            "46/168, train_loss: 0.8512\n",
            "47/168, train_loss: 0.5693\n",
            "48/168, train_loss: 0.5531\n",
            "49/168, train_loss: 0.7064\n",
            "50/168, train_loss: 0.5557\n",
            "51/168, train_loss: 0.5464\n",
            "52/168, train_loss: 0.5719\n",
            "53/168, train_loss: 0.5296\n",
            "54/168, train_loss: 0.6264\n",
            "55/168, train_loss: 0.5571\n",
            "56/168, train_loss: 0.6129\n",
            "57/168, train_loss: 0.8724\n",
            "58/168, train_loss: 0.8139\n",
            "59/168, train_loss: 0.5359\n",
            "60/168, train_loss: 0.5488\n",
            "61/168, train_loss: 0.5771\n",
            "62/168, train_loss: 0.5408\n",
            "63/168, train_loss: 0.5817\n",
            "64/168, train_loss: 0.6336\n",
            "65/168, train_loss: 0.8761\n",
            "66/168, train_loss: 0.5386\n",
            "67/168, train_loss: 0.6168\n",
            "68/168, train_loss: 0.5226\n",
            "69/168, train_loss: 0.7137\n",
            "70/168, train_loss: 1.0051\n",
            "71/168, train_loss: 0.5871\n",
            "72/168, train_loss: 0.5337\n",
            "73/168, train_loss: 0.5805\n",
            "74/168, train_loss: 0.6001\n",
            "75/168, train_loss: 0.8860\n",
            "76/168, train_loss: 0.5247\n",
            "77/168, train_loss: 0.7124\n",
            "78/168, train_loss: 0.5675\n",
            "79/168, train_loss: 0.4829\n",
            "80/168, train_loss: 0.7109\n",
            "81/168, train_loss: 0.6484\n",
            "82/168, train_loss: 0.5495\n",
            "83/168, train_loss: 0.5378\n",
            "84/168, train_loss: 0.5643\n",
            "85/168, train_loss: 0.5332\n",
            "86/168, train_loss: 0.5363\n",
            "87/168, train_loss: 0.5233\n",
            "88/168, train_loss: 0.9226\n",
            "89/168, train_loss: 0.5037\n",
            "90/168, train_loss: 0.5122\n",
            "91/168, train_loss: 0.5363\n",
            "92/168, train_loss: 0.7128\n",
            "93/168, train_loss: 0.5032\n",
            "94/168, train_loss: 0.5141\n",
            "95/168, train_loss: 0.5906\n",
            "96/168, train_loss: 0.5482\n",
            "97/168, train_loss: 0.8466\n",
            "98/168, train_loss: 0.5355\n",
            "99/168, train_loss: 0.8236\n",
            "100/168, train_loss: 0.7830\n",
            "101/168, train_loss: 0.4283\n",
            "102/168, train_loss: 0.5673\n",
            "103/168, train_loss: 0.5143\n",
            "104/168, train_loss: 0.5508\n",
            "105/168, train_loss: 0.7692\n",
            "106/168, train_loss: 0.9312\n",
            "107/168, train_loss: 0.5163\n",
            "108/168, train_loss: 0.4960\n",
            "109/168, train_loss: 0.5164\n",
            "110/168, train_loss: 0.5918\n",
            "111/168, train_loss: 0.5039\n",
            "112/168, train_loss: 0.8187\n",
            "113/168, train_loss: 0.5060\n",
            "114/168, train_loss: 0.9102\n",
            "115/168, train_loss: 0.9583\n",
            "116/168, train_loss: 0.7160\n",
            "117/168, train_loss: 0.5185\n",
            "118/168, train_loss: 0.9479\n",
            "119/168, train_loss: 0.5285\n",
            "120/168, train_loss: 0.5684\n",
            "121/168, train_loss: 0.9151\n",
            "122/168, train_loss: 0.7083\n",
            "123/168, train_loss: 0.5197\n",
            "124/168, train_loss: 0.5056\n",
            "125/168, train_loss: 0.5395\n",
            "126/168, train_loss: 0.6064\n",
            "127/168, train_loss: 0.5064\n",
            "128/168, train_loss: 0.7125\n",
            "129/168, train_loss: 0.5310\n",
            "130/168, train_loss: 0.6267\n",
            "131/168, train_loss: 0.5430\n",
            "132/168, train_loss: 0.9260\n",
            "133/168, train_loss: 0.5317\n",
            "134/168, train_loss: 0.4543\n",
            "135/168, train_loss: 0.5204\n",
            "136/168, train_loss: 0.5635\n",
            "137/168, train_loss: 0.5106\n",
            "138/168, train_loss: 0.5117\n",
            "139/168, train_loss: 0.5053\n",
            "140/168, train_loss: 0.7129\n",
            "141/168, train_loss: 0.6683\n",
            "142/168, train_loss: 0.5082\n",
            "143/168, train_loss: 0.5198\n",
            "144/168, train_loss: 0.5130\n",
            "145/168, train_loss: 0.5024\n",
            "146/168, train_loss: 0.6000\n",
            "147/168, train_loss: 0.4956\n",
            "148/168, train_loss: 0.4771\n",
            "149/168, train_loss: 0.9544\n",
            "150/168, train_loss: 0.6198\n",
            "151/168, train_loss: 0.5361\n",
            "152/168, train_loss: 0.6506\n",
            "153/168, train_loss: 0.8749\n",
            "154/168, train_loss: 0.9486\n",
            "155/168, train_loss: 0.5456\n",
            "156/168, train_loss: 0.4164\n",
            "157/168, train_loss: 0.9261\n",
            "158/168, train_loss: 0.4983\n",
            "159/168, train_loss: 0.4713\n",
            "160/168, train_loss: 0.6717\n",
            "161/168, train_loss: 0.5747\n",
            "162/168, train_loss: 0.6426\n",
            "163/168, train_loss: 0.4817\n",
            "164/168, train_loss: 0.4805\n",
            "165/168, train_loss: 0.5377\n",
            "166/168, train_loss: 0.4880\n",
            "167/168, train_loss: 0.9562\n",
            "168/168, train_loss: 0.9771\n",
            "169/168, train_loss: 0.4949\n",
            "epoch 2 average loss: 0.6361\n",
            "saved new best metric model\n",
            "current epoch: 2 current accuracy: 0.4471 best accuracy: 0.4471 at epoch 2\n",
            "----------\n",
            "epoch 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.6287\n",
            "2/168, train_loss: 0.7485\n",
            "3/168, train_loss: 0.4693\n",
            "4/168, train_loss: 0.7761\n",
            "5/168, train_loss: 0.4701\n",
            "6/168, train_loss: 0.4837\n",
            "7/168, train_loss: 0.4675\n",
            "8/168, train_loss: 0.8688\n",
            "9/168, train_loss: 0.6611\n",
            "10/168, train_loss: 0.4733\n",
            "11/168, train_loss: 0.4894\n",
            "12/168, train_loss: 0.4731\n",
            "13/168, train_loss: 0.4617\n",
            "14/168, train_loss: 0.4470\n",
            "15/168, train_loss: 1.0049\n",
            "16/168, train_loss: 0.6678\n",
            "17/168, train_loss: 0.7296\n",
            "18/168, train_loss: 0.7341\n",
            "19/168, train_loss: 0.4911\n",
            "20/168, train_loss: 0.4751\n",
            "21/168, train_loss: 0.6727\n",
            "22/168, train_loss: 0.6751\n",
            "23/168, train_loss: 1.0314\n",
            "24/168, train_loss: 0.4601\n",
            "25/168, train_loss: 0.4583\n",
            "26/168, train_loss: 1.0515\n",
            "27/168, train_loss: 0.7496\n",
            "28/168, train_loss: 1.0045\n",
            "29/168, train_loss: 0.4818\n",
            "30/168, train_loss: 0.7167\n",
            "31/168, train_loss: 1.0005\n",
            "32/168, train_loss: 0.4709\n",
            "33/168, train_loss: 0.5207\n",
            "34/168, train_loss: 1.0217\n",
            "35/168, train_loss: 0.4761\n",
            "36/168, train_loss: 0.4643\n",
            "37/168, train_loss: 0.7202\n",
            "38/168, train_loss: 0.4886\n",
            "39/168, train_loss: 0.4735\n",
            "40/168, train_loss: 0.9897\n",
            "41/168, train_loss: 0.4934\n",
            "42/168, train_loss: 0.4619\n",
            "43/168, train_loss: 0.4705\n",
            "44/168, train_loss: 0.4767\n",
            "45/168, train_loss: 0.7269\n",
            "46/168, train_loss: 0.6619\n",
            "47/168, train_loss: 1.0285\n",
            "48/168, train_loss: 0.7398\n",
            "49/168, train_loss: 0.5942\n",
            "50/168, train_loss: 0.6519\n",
            "51/168, train_loss: 0.6433\n",
            "52/168, train_loss: 0.7229\n",
            "53/168, train_loss: 0.7996\n",
            "54/168, train_loss: 0.9847\n",
            "55/168, train_loss: 0.4853\n",
            "56/168, train_loss: 0.5349\n",
            "57/168, train_loss: 0.4557\n",
            "58/168, train_loss: 0.5904\n",
            "59/168, train_loss: 0.4916\n",
            "60/168, train_loss: 0.5893\n",
            "61/168, train_loss: 0.4824\n",
            "62/168, train_loss: 0.5348\n",
            "63/168, train_loss: 1.0074\n",
            "64/168, train_loss: 0.5248\n",
            "65/168, train_loss: 0.4615\n",
            "66/168, train_loss: 0.4932\n",
            "67/168, train_loss: 0.4649\n",
            "68/168, train_loss: 0.5011\n",
            "69/168, train_loss: 0.5777\n",
            "70/168, train_loss: 0.4573\n",
            "71/168, train_loss: 0.4760\n",
            "72/168, train_loss: 0.4580\n",
            "73/168, train_loss: 1.0116\n",
            "74/168, train_loss: 0.5408\n",
            "75/168, train_loss: 0.6252\n",
            "76/168, train_loss: 0.9615\n",
            "77/168, train_loss: 0.4320\n",
            "78/168, train_loss: 0.3907\n",
            "79/168, train_loss: 0.8135\n",
            "80/168, train_loss: 0.4637\n",
            "81/168, train_loss: 1.0039\n",
            "82/168, train_loss: 0.4692\n",
            "83/168, train_loss: 0.4978\n",
            "84/168, train_loss: 0.8680\n",
            "85/168, train_loss: 0.3598\n",
            "86/168, train_loss: 1.0222\n",
            "87/168, train_loss: 0.5684\n",
            "88/168, train_loss: 0.7288\n",
            "89/168, train_loss: 0.6717\n",
            "90/168, train_loss: 0.4620\n",
            "91/168, train_loss: 0.7330\n",
            "92/168, train_loss: 0.9882\n",
            "93/168, train_loss: 0.7313\n",
            "94/168, train_loss: 0.4618\n",
            "95/168, train_loss: 0.9702\n",
            "96/168, train_loss: 0.4723\n",
            "97/168, train_loss: 0.4590\n",
            "98/168, train_loss: 0.5202\n",
            "99/168, train_loss: 0.5045\n",
            "100/168, train_loss: 0.6756\n",
            "101/168, train_loss: 0.5142\n",
            "102/168, train_loss: 0.9801\n",
            "103/168, train_loss: 0.4704\n",
            "104/168, train_loss: 0.4862\n",
            "105/168, train_loss: 0.9857\n",
            "106/168, train_loss: 0.7202\n",
            "107/168, train_loss: 0.4756\n",
            "108/168, train_loss: 0.7078\n",
            "109/168, train_loss: 0.4876\n",
            "110/168, train_loss: 0.7309\n",
            "111/168, train_loss: 0.4733\n",
            "112/168, train_loss: 0.8379\n",
            "113/168, train_loss: 0.7314\n",
            "114/168, train_loss: 0.8362\n",
            "115/168, train_loss: 0.4769\n",
            "116/168, train_loss: 0.4598\n",
            "117/168, train_loss: 0.4821\n",
            "118/168, train_loss: 0.4678\n",
            "119/168, train_loss: 0.9968\n",
            "120/168, train_loss: 0.6869\n",
            "121/168, train_loss: 0.6439\n",
            "122/168, train_loss: 0.5844\n",
            "123/168, train_loss: 0.8620\n",
            "124/168, train_loss: 1.0181\n",
            "125/168, train_loss: 0.9828\n",
            "126/168, train_loss: 0.4555\n",
            "127/168, train_loss: 0.4382\n",
            "128/168, train_loss: 0.7365\n",
            "129/168, train_loss: 0.7883\n",
            "130/168, train_loss: 0.4382\n",
            "131/168, train_loss: 0.7516\n",
            "132/168, train_loss: 0.8675\n",
            "133/168, train_loss: 0.8264\n",
            "134/168, train_loss: 0.4638\n",
            "135/168, train_loss: 0.4634\n",
            "136/168, train_loss: 0.5107\n",
            "137/168, train_loss: 0.7383\n",
            "138/168, train_loss: 0.4702\n",
            "139/168, train_loss: 0.5839\n",
            "140/168, train_loss: 0.7394\n",
            "141/168, train_loss: 0.4702\n",
            "142/168, train_loss: 0.4745\n",
            "143/168, train_loss: 1.0337\n",
            "144/168, train_loss: 0.7322\n",
            "145/168, train_loss: 0.4493\n",
            "146/168, train_loss: 0.4544\n",
            "147/168, train_loss: 0.4654\n",
            "148/168, train_loss: 0.4501\n",
            "149/168, train_loss: 0.6136\n",
            "150/168, train_loss: 0.9783\n",
            "151/168, train_loss: 0.9993\n",
            "152/168, train_loss: 0.4689\n",
            "153/168, train_loss: 0.9698\n",
            "154/168, train_loss: 0.4639\n",
            "155/168, train_loss: 0.4614\n",
            "156/168, train_loss: 0.8481\n",
            "157/168, train_loss: 0.6142\n",
            "158/168, train_loss: 0.9953\n",
            "159/168, train_loss: 0.4516\n",
            "160/168, train_loss: 0.4456\n",
            "161/168, train_loss: 0.6936\n",
            "162/168, train_loss: 0.4698\n",
            "163/168, train_loss: 0.4541\n",
            "164/168, train_loss: 0.9855\n",
            "165/168, train_loss: 1.0186\n",
            "166/168, train_loss: 0.6726\n",
            "167/168, train_loss: 0.4495\n",
            "168/168, train_loss: 0.6905\n",
            "169/168, train_loss: 0.4551\n",
            "epoch 3 average loss: 0.6420\n",
            "----------\n",
            "epoch 4/5\n",
            "1/168, train_loss: 0.6567\n",
            "2/168, train_loss: 0.4597\n",
            "3/168, train_loss: 0.7295\n",
            "4/168, train_loss: 0.6407\n",
            "5/168, train_loss: 0.4724\n",
            "6/168, train_loss: 0.6443\n",
            "7/168, train_loss: 0.4650\n",
            "8/168, train_loss: 0.4735\n",
            "9/168, train_loss: 0.4411\n",
            "10/168, train_loss: 0.9854\n",
            "11/168, train_loss: 0.4593\n",
            "12/168, train_loss: 1.0715\n",
            "13/168, train_loss: 0.4557\n",
            "14/168, train_loss: 0.4473\n",
            "15/168, train_loss: 0.5125\n",
            "16/168, train_loss: 0.9981\n",
            "17/168, train_loss: 0.7392\n",
            "18/168, train_loss: 0.6085\n",
            "19/168, train_loss: 0.7782\n",
            "20/168, train_loss: 0.4564\n",
            "21/168, train_loss: 0.5783\n",
            "22/168, train_loss: 1.0020\n",
            "23/168, train_loss: 0.4788\n",
            "24/168, train_loss: 0.6847\n",
            "25/168, train_loss: 0.5184\n",
            "26/168, train_loss: 0.4881\n",
            "27/168, train_loss: 0.4417\n",
            "28/168, train_loss: 0.4576\n",
            "29/168, train_loss: 0.7087\n",
            "30/168, train_loss: 1.0256\n",
            "31/168, train_loss: 0.7340\n",
            "32/168, train_loss: 0.9920\n",
            "33/168, train_loss: 0.4618\n",
            "34/168, train_loss: 1.0838\n",
            "35/168, train_loss: 0.4721\n",
            "36/168, train_loss: 0.4579\n",
            "37/168, train_loss: 0.7248\n",
            "38/168, train_loss: 0.5069\n",
            "39/168, train_loss: 0.4583\n",
            "40/168, train_loss: 0.4704\n",
            "41/168, train_loss: 0.4852\n",
            "42/168, train_loss: 0.7273\n",
            "43/168, train_loss: 0.5438\n",
            "44/168, train_loss: 0.4695\n",
            "45/168, train_loss: 0.5459\n",
            "46/168, train_loss: 0.4497\n",
            "47/168, train_loss: 0.4490\n",
            "48/168, train_loss: 0.5640\n",
            "49/168, train_loss: 0.9031\n",
            "50/168, train_loss: 0.9161\n",
            "51/168, train_loss: 1.0126\n",
            "52/168, train_loss: 0.4643\n",
            "53/168, train_loss: 0.4517\n",
            "54/168, train_loss: 0.4655\n",
            "55/168, train_loss: 0.4455\n",
            "56/168, train_loss: 0.7060\n",
            "57/168, train_loss: 0.7419\n",
            "58/168, train_loss: 1.0393\n",
            "59/168, train_loss: 1.0393\n",
            "60/168, train_loss: 0.5358\n",
            "61/168, train_loss: 0.7832\n",
            "62/168, train_loss: 1.0071\n",
            "63/168, train_loss: 1.0117\n",
            "64/168, train_loss: 0.4615\n",
            "65/168, train_loss: 1.0227\n",
            "66/168, train_loss: 0.4493\n",
            "67/168, train_loss: 0.4561\n",
            "68/168, train_loss: 0.6842\n",
            "69/168, train_loss: 0.8357\n",
            "70/168, train_loss: 0.4921\n",
            "71/168, train_loss: 0.4845\n",
            "72/168, train_loss: 0.9978\n",
            "73/168, train_loss: 0.4909\n",
            "74/168, train_loss: 1.0407\n",
            "75/168, train_loss: 0.7330\n",
            "76/168, train_loss: 0.9731\n",
            "77/168, train_loss: 0.9456\n",
            "78/168, train_loss: 0.4624\n",
            "79/168, train_loss: 0.4516\n",
            "80/168, train_loss: 0.4096\n",
            "81/168, train_loss: 0.4730\n",
            "82/168, train_loss: 0.9823\n",
            "83/168, train_loss: 0.4909\n",
            "84/168, train_loss: 0.5961\n",
            "85/168, train_loss: 0.5825\n",
            "86/168, train_loss: 0.4546\n",
            "87/168, train_loss: 0.4705\n",
            "88/168, train_loss: 0.4733\n",
            "89/168, train_loss: 1.0092\n",
            "90/168, train_loss: 0.4698\n",
            "91/168, train_loss: 0.6286\n",
            "92/168, train_loss: 0.7320\n",
            "93/168, train_loss: 0.9773\n",
            "94/168, train_loss: 0.7260\n",
            "95/168, train_loss: 0.4674\n",
            "96/168, train_loss: 0.4675\n",
            "97/168, train_loss: 0.4770\n",
            "98/168, train_loss: 0.5313\n",
            "99/168, train_loss: 0.5942\n",
            "100/168, train_loss: 0.5029\n",
            "101/168, train_loss: 0.4516\n",
            "102/168, train_loss: 0.9434\n",
            "103/168, train_loss: 0.9607\n",
            "104/168, train_loss: 0.5240\n",
            "105/168, train_loss: 0.7225\n",
            "106/168, train_loss: 0.5144\n",
            "107/168, train_loss: 0.4716\n",
            "108/168, train_loss: 0.7559\n",
            "109/168, train_loss: 0.7555\n",
            "110/168, train_loss: 0.7601\n",
            "111/168, train_loss: 1.1025\n",
            "112/168, train_loss: 0.4845\n",
            "113/168, train_loss: 0.4787\n",
            "114/168, train_loss: 0.5117\n",
            "115/168, train_loss: 0.5027\n",
            "116/168, train_loss: 0.4844\n",
            "117/168, train_loss: 0.4873\n",
            "118/168, train_loss: 0.7002\n",
            "119/168, train_loss: 0.4780\n",
            "120/168, train_loss: 0.6051\n",
            "121/168, train_loss: 0.7191\n",
            "122/168, train_loss: 0.4962\n",
            "123/168, train_loss: 0.4753\n",
            "124/168, train_loss: 0.8887\n",
            "125/168, train_loss: 0.5741\n",
            "126/168, train_loss: 0.9999\n",
            "127/168, train_loss: 0.9929\n",
            "128/168, train_loss: 0.4640\n",
            "129/168, train_loss: 0.4725\n",
            "130/168, train_loss: 0.8815\n",
            "131/168, train_loss: 0.7619\n",
            "132/168, train_loss: 0.4585\n",
            "133/168, train_loss: 0.4418\n",
            "134/168, train_loss: 0.4445\n",
            "135/168, train_loss: 0.4623\n",
            "136/168, train_loss: 0.4908\n",
            "137/168, train_loss: 0.4642\n",
            "138/168, train_loss: 0.7454\n",
            "139/168, train_loss: 0.4498\n",
            "140/168, train_loss: 0.7394\n",
            "141/168, train_loss: 0.4635\n",
            "142/168, train_loss: 0.4538\n",
            "143/168, train_loss: 0.6848\n",
            "144/168, train_loss: 1.0139\n",
            "145/168, train_loss: 0.4439\n",
            "146/168, train_loss: 0.4463\n",
            "147/168, train_loss: 0.4730\n",
            "148/168, train_loss: 1.0239\n",
            "149/168, train_loss: 0.4476\n",
            "150/168, train_loss: 0.5548\n",
            "151/168, train_loss: 0.7029\n",
            "152/168, train_loss: 0.7630\n",
            "153/168, train_loss: 0.8532\n",
            "154/168, train_loss: 0.7762\n",
            "155/168, train_loss: 0.7402\n",
            "156/168, train_loss: 0.4340\n",
            "157/168, train_loss: 0.7368\n",
            "158/168, train_loss: 0.4245\n",
            "159/168, train_loss: 1.0229\n",
            "160/168, train_loss: 0.4603\n",
            "161/168, train_loss: 0.4760\n",
            "162/168, train_loss: 0.4320\n",
            "163/168, train_loss: 0.4859\n",
            "164/168, train_loss: 0.4363\n",
            "165/168, train_loss: 0.8876\n",
            "166/168, train_loss: 0.5111\n",
            "167/168, train_loss: 1.0292\n",
            "168/168, train_loss: 0.8726\n",
            "169/168, train_loss: 0.9899\n",
            "epoch 4 average loss: 0.6429\n",
            "saved new best metric model\n",
            "current epoch: 4 current accuracy: 0.5412 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.4799\n",
            "2/168, train_loss: 0.4474\n",
            "3/168, train_loss: 0.4505\n",
            "4/168, train_loss: 0.5701\n",
            "5/168, train_loss: 0.4391\n",
            "6/168, train_loss: 0.5922\n",
            "7/168, train_loss: 0.4527\n",
            "8/168, train_loss: 0.7503\n",
            "9/168, train_loss: 0.4554\n",
            "10/168, train_loss: 1.0373\n",
            "11/168, train_loss: 1.0298\n",
            "12/168, train_loss: 0.4375\n",
            "13/168, train_loss: 1.0182\n",
            "14/168, train_loss: 0.8234\n",
            "15/168, train_loss: 0.4446\n",
            "16/168, train_loss: 0.9940\n",
            "17/168, train_loss: 0.7754\n",
            "18/168, train_loss: 0.7320\n",
            "19/168, train_loss: 1.0012\n",
            "20/168, train_loss: 0.9742\n",
            "21/168, train_loss: 0.6402\n",
            "22/168, train_loss: 0.5301\n",
            "23/168, train_loss: 0.6393\n",
            "24/168, train_loss: 1.0061\n",
            "25/168, train_loss: 0.4500\n",
            "26/168, train_loss: 0.5964\n",
            "27/168, train_loss: 0.4835\n",
            "28/168, train_loss: 0.4704\n",
            "29/168, train_loss: 0.4792\n",
            "30/168, train_loss: 0.7223\n",
            "31/168, train_loss: 0.4643\n",
            "32/168, train_loss: 0.9978\n",
            "33/168, train_loss: 0.4702\n",
            "34/168, train_loss: 0.7183\n",
            "35/168, train_loss: 1.0099\n",
            "36/168, train_loss: 0.9982\n",
            "37/168, train_loss: 0.5954\n",
            "38/168, train_loss: 0.7347\n",
            "39/168, train_loss: 0.4931\n",
            "40/168, train_loss: 0.6966\n",
            "41/168, train_loss: 0.7112\n",
            "42/168, train_loss: 0.5139\n",
            "43/168, train_loss: 0.4757\n",
            "44/168, train_loss: 0.9898\n",
            "45/168, train_loss: 0.4884\n",
            "46/168, train_loss: 0.7073\n",
            "47/168, train_loss: 0.9451\n",
            "48/168, train_loss: 0.7049\n",
            "49/168, train_loss: 0.5398\n",
            "50/168, train_loss: 0.5165\n",
            "51/168, train_loss: 0.6896\n",
            "52/168, train_loss: 0.7244\n",
            "53/168, train_loss: 0.6278\n",
            "54/168, train_loss: 0.4829\n",
            "55/168, train_loss: 0.5737\n",
            "56/168, train_loss: 0.4996\n",
            "57/168, train_loss: 0.6913\n",
            "58/168, train_loss: 0.6932\n",
            "59/168, train_loss: 0.9734\n",
            "60/168, train_loss: 0.5457\n",
            "61/168, train_loss: 0.5878\n",
            "62/168, train_loss: 0.6759\n",
            "63/168, train_loss: 0.5018\n",
            "64/168, train_loss: 0.7791\n",
            "65/168, train_loss: 0.7176\n",
            "66/168, train_loss: 0.4911\n",
            "67/168, train_loss: 0.7155\n",
            "68/168, train_loss: 0.6352\n",
            "69/168, train_loss: 0.9878\n",
            "70/168, train_loss: 0.5158\n",
            "71/168, train_loss: 0.5031\n",
            "72/168, train_loss: 0.9582\n",
            "73/168, train_loss: 0.9378\n",
            "74/168, train_loss: 0.4819\n",
            "75/168, train_loss: 0.5252\n",
            "76/168, train_loss: 0.4728\n",
            "77/168, train_loss: 0.5108\n",
            "78/168, train_loss: 0.4906\n",
            "79/168, train_loss: 0.6533\n",
            "80/168, train_loss: 0.6839\n",
            "81/168, train_loss: 0.5307\n",
            "82/168, train_loss: 0.8862\n",
            "83/168, train_loss: 0.4775\n",
            "84/168, train_loss: 0.9644\n",
            "85/168, train_loss: 0.4620\n",
            "86/168, train_loss: 0.9374\n",
            "87/168, train_loss: 0.4769\n",
            "88/168, train_loss: 0.6485\n",
            "89/168, train_loss: 0.7610\n",
            "90/168, train_loss: 0.9522\n",
            "91/168, train_loss: 0.5013\n",
            "92/168, train_loss: 0.7281\n",
            "93/168, train_loss: 0.5337\n",
            "94/168, train_loss: 0.4753\n",
            "95/168, train_loss: 0.4147\n",
            "96/168, train_loss: 0.6322\n",
            "97/168, train_loss: 0.4580\n",
            "98/168, train_loss: 0.5643\n",
            "99/168, train_loss: 0.3689\n",
            "100/168, train_loss: 0.4574\n",
            "101/168, train_loss: 0.6000\n",
            "102/168, train_loss: 0.9993\n",
            "103/168, train_loss: 0.4782\n",
            "104/168, train_loss: 0.5354\n",
            "105/168, train_loss: 0.4799\n",
            "106/168, train_loss: 0.5853\n",
            "107/168, train_loss: 0.4821\n",
            "108/168, train_loss: 0.4805\n",
            "109/168, train_loss: 0.5866\n",
            "110/168, train_loss: 0.5488\n",
            "111/168, train_loss: 0.6657\n",
            "112/168, train_loss: 0.9689\n",
            "113/168, train_loss: 0.4712\n",
            "114/168, train_loss: 0.5533\n",
            "115/168, train_loss: 0.5131\n",
            "116/168, train_loss: 0.7334\n",
            "117/168, train_loss: 0.6440\n",
            "118/168, train_loss: 0.5041\n",
            "119/168, train_loss: 0.5983\n",
            "120/168, train_loss: 0.4821\n",
            "121/168, train_loss: 0.5666\n",
            "122/168, train_loss: 0.4571\n",
            "123/168, train_loss: 0.7281\n",
            "124/168, train_loss: 0.5068\n",
            "125/168, train_loss: 0.4521\n",
            "126/168, train_loss: 0.4560\n",
            "127/168, train_loss: 0.4785\n",
            "128/168, train_loss: 0.4413\n",
            "129/168, train_loss: 0.4801\n",
            "130/168, train_loss: 0.9284\n",
            "131/168, train_loss: 0.4706\n",
            "132/168, train_loss: 0.4565\n",
            "133/168, train_loss: 0.7145\n",
            "134/168, train_loss: 0.4539\n",
            "135/168, train_loss: 0.4520\n",
            "136/168, train_loss: 0.7241\n",
            "137/168, train_loss: 0.4712\n",
            "138/168, train_loss: 0.4715\n",
            "139/168, train_loss: 0.5021\n",
            "140/168, train_loss: 0.6023\n",
            "141/168, train_loss: 0.4547\n",
            "142/168, train_loss: 0.7226\n",
            "143/168, train_loss: 0.8482\n",
            "144/168, train_loss: 1.0271\n",
            "145/168, train_loss: 0.5472\n",
            "146/168, train_loss: 0.7221\n",
            "147/168, train_loss: 0.3373\n",
            "148/168, train_loss: 0.4572\n",
            "149/168, train_loss: 0.4281\n",
            "150/168, train_loss: 0.4953\n",
            "151/168, train_loss: 0.8329\n",
            "152/168, train_loss: 0.4533\n",
            "153/168, train_loss: 1.0441\n",
            "154/168, train_loss: 0.7193\n",
            "155/168, train_loss: 0.8817\n",
            "156/168, train_loss: 0.4605\n",
            "157/168, train_loss: 0.4395\n",
            "158/168, train_loss: 0.4937\n",
            "159/168, train_loss: 0.7810\n",
            "160/168, train_loss: 0.5599\n",
            "161/168, train_loss: 0.4946\n",
            "162/168, train_loss: 0.4806\n",
            "163/168, train_loss: 0.5087\n",
            "164/168, train_loss: 0.4636\n",
            "165/168, train_loss: 0.4415\n",
            "166/168, train_loss: 0.7421\n",
            "167/168, train_loss: 0.9766\n",
            "168/168, train_loss: 0.7230\n",
            "169/168, train_loss: 0.4349\n",
            "epoch 5 average loss: 0.6269\n",
            "----------\n",
            "epoch 6/5\n",
            "1/168, train_loss: 0.4230\n",
            "2/168, train_loss: 0.4110\n",
            "3/168, train_loss: 0.4346\n",
            "4/168, train_loss: 0.4575\n",
            "5/168, train_loss: 0.4516\n",
            "6/168, train_loss: 0.4607\n",
            "7/168, train_loss: 0.7599\n",
            "8/168, train_loss: 0.3978\n",
            "9/168, train_loss: 0.7611\n",
            "10/168, train_loss: 0.6817\n",
            "11/168, train_loss: 0.7595\n",
            "12/168, train_loss: 0.4023\n",
            "13/168, train_loss: 0.6051\n",
            "14/168, train_loss: 0.7497\n",
            "15/168, train_loss: 0.4179\n",
            "16/168, train_loss: 0.3879\n",
            "17/168, train_loss: 0.9758\n",
            "18/168, train_loss: 0.7048\n",
            "19/168, train_loss: 0.4219\n",
            "20/168, train_loss: 0.6643\n",
            "21/168, train_loss: 0.4416\n",
            "22/168, train_loss: 0.7524\n",
            "23/168, train_loss: 0.4296\n",
            "24/168, train_loss: 0.4276\n",
            "25/168, train_loss: 1.0721\n",
            "26/168, train_loss: 0.6521\n",
            "27/168, train_loss: 0.4111\n",
            "28/168, train_loss: 0.7528\n",
            "29/168, train_loss: 1.0725\n",
            "30/168, train_loss: 0.4198\n",
            "31/168, train_loss: 0.4913\n",
            "32/168, train_loss: 0.4502\n",
            "33/168, train_loss: 0.8020\n",
            "34/168, train_loss: 0.4241\n",
            "35/168, train_loss: 1.0840\n",
            "36/168, train_loss: 0.4092\n",
            "37/168, train_loss: 0.4252\n",
            "38/168, train_loss: 0.7395\n",
            "39/168, train_loss: 1.0218\n",
            "40/168, train_loss: 0.4215\n",
            "41/168, train_loss: 0.5882\n",
            "42/168, train_loss: 0.4167\n",
            "43/168, train_loss: 0.8333\n",
            "44/168, train_loss: 0.4056\n",
            "45/168, train_loss: 0.6034\n",
            "46/168, train_loss: 0.5159\n",
            "47/168, train_loss: 0.7581\n",
            "48/168, train_loss: 0.4178\n",
            "49/168, train_loss: 0.7627\n",
            "50/168, train_loss: 0.4358\n",
            "51/168, train_loss: 1.0653\n",
            "52/168, train_loss: 0.4151\n",
            "53/168, train_loss: 1.0763\n",
            "54/168, train_loss: 0.4156\n",
            "55/168, train_loss: 0.4133\n",
            "56/168, train_loss: 0.4216\n",
            "57/168, train_loss: 0.7568\n",
            "58/168, train_loss: 0.6184\n",
            "59/168, train_loss: 0.6131\n",
            "60/168, train_loss: 0.6551\n",
            "61/168, train_loss: 0.4174\n",
            "62/168, train_loss: 0.4213\n",
            "63/168, train_loss: 0.9007\n",
            "64/168, train_loss: 1.0812\n",
            "65/168, train_loss: 0.3309\n",
            "66/168, train_loss: 0.5446\n",
            "67/168, train_loss: 0.8214\n",
            "68/168, train_loss: 0.4020\n",
            "69/168, train_loss: 0.4174\n",
            "70/168, train_loss: 0.6504\n",
            "71/168, train_loss: 0.7619\n",
            "72/168, train_loss: 0.5168\n",
            "73/168, train_loss: 0.7514\n",
            "74/168, train_loss: 0.4313\n",
            "75/168, train_loss: 0.4634\n",
            "76/168, train_loss: 1.0072\n",
            "77/168, train_loss: 0.6788\n",
            "78/168, train_loss: 0.4158\n",
            "79/168, train_loss: 0.5802\n",
            "80/168, train_loss: 0.4584\n",
            "81/168, train_loss: 0.7462\n",
            "82/168, train_loss: 0.4420\n",
            "83/168, train_loss: 0.4605\n",
            "84/168, train_loss: 0.4621\n",
            "85/168, train_loss: 0.4468\n",
            "86/168, train_loss: 1.0494\n",
            "87/168, train_loss: 0.4195\n",
            "88/168, train_loss: 0.4386\n",
            "89/168, train_loss: 0.7458\n",
            "90/168, train_loss: 0.4240\n",
            "91/168, train_loss: 0.4399\n",
            "92/168, train_loss: 0.4246\n",
            "93/168, train_loss: 1.1073\n",
            "94/168, train_loss: 0.7476\n",
            "95/168, train_loss: 0.4240\n",
            "96/168, train_loss: 0.4196\n",
            "97/168, train_loss: 0.7445\n",
            "98/168, train_loss: 0.4382\n",
            "99/168, train_loss: 1.1033\n",
            "100/168, train_loss: 1.1589\n",
            "101/168, train_loss: 0.4126\n",
            "102/168, train_loss: 0.7558\n",
            "103/168, train_loss: 0.3985\n",
            "104/168, train_loss: 0.6044\n",
            "105/168, train_loss: 0.4094\n",
            "106/168, train_loss: 0.6673\n",
            "107/168, train_loss: 0.7121\n",
            "108/168, train_loss: 0.4318\n",
            "109/168, train_loss: 0.8429\n",
            "110/168, train_loss: 1.0950\n",
            "111/168, train_loss: 1.0977\n",
            "112/168, train_loss: 0.7271\n",
            "113/168, train_loss: 0.6241\n",
            "114/168, train_loss: 0.4187\n",
            "115/168, train_loss: 0.3809\n",
            "116/168, train_loss: 0.7186\n",
            "117/168, train_loss: 0.7743\n",
            "118/168, train_loss: 0.6362\n",
            "119/168, train_loss: 0.6550\n",
            "120/168, train_loss: 1.0442\n",
            "121/168, train_loss: 0.3874\n",
            "122/168, train_loss: 0.4183\n",
            "123/168, train_loss: 0.4656\n",
            "124/168, train_loss: 0.5620\n",
            "125/168, train_loss: 0.4274\n",
            "126/168, train_loss: 0.6689\n",
            "127/168, train_loss: 0.6315\n",
            "128/168, train_loss: 1.1497\n",
            "129/168, train_loss: 0.6036\n",
            "130/168, train_loss: 0.4134\n",
            "131/168, train_loss: 0.6138\n",
            "132/168, train_loss: 0.4333\n",
            "133/168, train_loss: 1.0159\n",
            "134/168, train_loss: 0.5083\n",
            "135/168, train_loss: 1.0430\n",
            "136/168, train_loss: 1.0930\n",
            "137/168, train_loss: 0.7397\n",
            "138/168, train_loss: 0.4605\n",
            "139/168, train_loss: 0.4741\n",
            "140/168, train_loss: 0.4574\n",
            "141/168, train_loss: 0.4357\n",
            "142/168, train_loss: 0.4160\n",
            "143/168, train_loss: 1.0518\n",
            "144/168, train_loss: 0.4287\n",
            "145/168, train_loss: 0.4396\n",
            "146/168, train_loss: 1.0338\n",
            "147/168, train_loss: 0.6102\n",
            "148/168, train_loss: 0.7369\n",
            "149/168, train_loss: 1.0553\n",
            "150/168, train_loss: 0.7321\n",
            "151/168, train_loss: 1.0487\n",
            "152/168, train_loss: 0.4304\n",
            "153/168, train_loss: 0.6727\n",
            "154/168, train_loss: 0.4903\n",
            "155/168, train_loss: 0.7241\n",
            "156/168, train_loss: 0.5796\n",
            "157/168, train_loss: 0.4357\n",
            "158/168, train_loss: 1.0064\n",
            "159/168, train_loss: 0.7179\n",
            "160/168, train_loss: 0.4623\n",
            "161/168, train_loss: 1.0435\n",
            "162/168, train_loss: 0.9683\n",
            "163/168, train_loss: 0.4492\n",
            "164/168, train_loss: 0.7125\n",
            "165/168, train_loss: 1.0081\n",
            "166/168, train_loss: 0.4333\n",
            "167/168, train_loss: 0.4779\n",
            "168/168, train_loss: 0.4582\n",
            "169/168, train_loss: 1.0111\n",
            "epoch 6 average loss: 0.6329\n",
            "current epoch: 6 current accuracy: 0.4588 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 7/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.4943\n",
            "2/168, train_loss: 0.9805\n",
            "3/168, train_loss: 0.7077\n",
            "4/168, train_loss: 0.8790\n",
            "5/168, train_loss: 0.8776\n",
            "6/168, train_loss: 0.6996\n",
            "7/168, train_loss: 0.5557\n",
            "8/168, train_loss: 0.4156\n",
            "9/168, train_loss: 0.4598\n",
            "10/168, train_loss: 0.6329\n",
            "11/168, train_loss: 0.9686\n",
            "12/168, train_loss: 0.6982\n",
            "13/168, train_loss: 0.6962\n",
            "14/168, train_loss: 0.4698\n",
            "15/168, train_loss: 0.5773\n",
            "16/168, train_loss: 0.9464\n",
            "17/168, train_loss: 0.5290\n",
            "18/168, train_loss: 0.7234\n",
            "19/168, train_loss: 0.9243\n",
            "20/168, train_loss: 0.4870\n",
            "21/168, train_loss: 0.6734\n",
            "22/168, train_loss: 0.5927\n",
            "23/168, train_loss: 0.5555\n",
            "24/168, train_loss: 0.4843\n",
            "25/168, train_loss: 0.9398\n",
            "26/168, train_loss: 0.8562\n",
            "27/168, train_loss: 0.7531\n",
            "28/168, train_loss: 0.4903\n",
            "29/168, train_loss: 0.4977\n",
            "30/168, train_loss: 0.5949\n",
            "31/168, train_loss: 0.4866\n",
            "32/168, train_loss: 0.5807\n",
            "33/168, train_loss: 0.5058\n",
            "34/168, train_loss: 0.4883\n",
            "35/168, train_loss: 0.7083\n",
            "36/168, train_loss: 0.3533\n",
            "37/168, train_loss: 0.9682\n",
            "38/168, train_loss: 0.3455\n",
            "39/168, train_loss: 0.5893\n",
            "40/168, train_loss: 0.4979\n",
            "41/168, train_loss: 0.4870\n",
            "42/168, train_loss: 0.9134\n",
            "43/168, train_loss: 0.5862\n",
            "44/168, train_loss: 0.5410\n",
            "45/168, train_loss: 0.9017\n",
            "46/168, train_loss: 0.4911\n",
            "47/168, train_loss: 0.4913\n",
            "48/168, train_loss: 0.6332\n",
            "49/168, train_loss: 0.4037\n",
            "50/168, train_loss: 0.4944\n",
            "51/168, train_loss: 0.4807\n",
            "52/168, train_loss: 0.4983\n",
            "53/168, train_loss: 0.7111\n",
            "54/168, train_loss: 0.7133\n",
            "55/168, train_loss: 0.9823\n",
            "56/168, train_loss: 0.4865\n",
            "57/168, train_loss: 0.5186\n",
            "58/168, train_loss: 0.6552\n",
            "59/168, train_loss: 0.5749\n",
            "60/168, train_loss: 0.7201\n",
            "61/168, train_loss: 0.4806\n",
            "62/168, train_loss: 1.1419\n",
            "63/168, train_loss: 0.9788\n",
            "64/168, train_loss: 0.5833\n",
            "65/168, train_loss: 1.0082\n",
            "66/168, train_loss: 0.8286\n",
            "67/168, train_loss: 0.4840\n",
            "68/168, train_loss: 0.4934\n",
            "69/168, train_loss: 0.4752\n",
            "70/168, train_loss: 0.5009\n",
            "71/168, train_loss: 0.4831\n",
            "72/168, train_loss: 0.6486\n",
            "73/168, train_loss: 0.9950\n",
            "74/168, train_loss: 0.4765\n",
            "75/168, train_loss: 0.5043\n",
            "76/168, train_loss: 0.9774\n",
            "77/168, train_loss: 0.6182\n",
            "78/168, train_loss: 0.7223\n",
            "79/168, train_loss: 0.5256\n",
            "80/168, train_loss: 0.4042\n",
            "81/168, train_loss: 0.5534\n",
            "82/168, train_loss: 0.4646\n",
            "83/168, train_loss: 0.4672\n",
            "84/168, train_loss: 0.4894\n",
            "85/168, train_loss: 0.7715\n",
            "86/168, train_loss: 0.4278\n",
            "87/168, train_loss: 0.5396\n",
            "88/168, train_loss: 0.8453\n",
            "89/168, train_loss: 0.5589\n",
            "90/168, train_loss: 1.0054\n",
            "91/168, train_loss: 0.7954\n",
            "92/168, train_loss: 0.7257\n",
            "93/168, train_loss: 0.9994\n",
            "94/168, train_loss: 1.1007\n",
            "95/168, train_loss: 0.5394\n",
            "96/168, train_loss: 0.4839\n",
            "97/168, train_loss: 0.4177\n",
            "98/168, train_loss: 0.6446\n",
            "99/168, train_loss: 0.5022\n",
            "100/168, train_loss: 0.5047\n",
            "101/168, train_loss: 0.3531\n",
            "102/168, train_loss: 0.7206\n",
            "103/168, train_loss: 0.4647\n",
            "104/168, train_loss: 0.4573\n",
            "105/168, train_loss: 0.5032\n",
            "106/168, train_loss: 0.7189\n",
            "107/168, train_loss: 0.5098\n",
            "108/168, train_loss: 0.3562\n",
            "109/168, train_loss: 0.5279\n",
            "110/168, train_loss: 0.6041\n",
            "111/168, train_loss: 0.4661\n",
            "112/168, train_loss: 0.7171\n",
            "113/168, train_loss: 0.4617\n",
            "114/168, train_loss: 0.4812\n",
            "115/168, train_loss: 0.4681\n",
            "116/168, train_loss: 0.7903\n",
            "117/168, train_loss: 0.6567\n",
            "118/168, train_loss: 0.4650\n",
            "119/168, train_loss: 0.5929\n",
            "120/168, train_loss: 0.6531\n",
            "121/168, train_loss: 0.4288\n",
            "122/168, train_loss: 0.6259\n",
            "123/168, train_loss: 1.0501\n",
            "124/168, train_loss: 0.6721\n",
            "125/168, train_loss: 0.7130\n",
            "126/168, train_loss: 0.5192\n",
            "127/168, train_loss: 0.4608\n",
            "128/168, train_loss: 0.4568\n",
            "129/168, train_loss: 0.4922\n",
            "130/168, train_loss: 0.4852\n",
            "131/168, train_loss: 0.5100\n",
            "132/168, train_loss: 0.4571\n",
            "133/168, train_loss: 0.4438\n",
            "134/168, train_loss: 0.4970\n",
            "135/168, train_loss: 0.5154\n",
            "136/168, train_loss: 0.4852\n",
            "137/168, train_loss: 0.7619\n",
            "138/168, train_loss: 0.4451\n",
            "139/168, train_loss: 0.4394\n",
            "140/168, train_loss: 0.5007\n",
            "141/168, train_loss: 0.4715\n",
            "142/168, train_loss: 0.4060\n",
            "143/168, train_loss: 0.3721\n",
            "144/168, train_loss: 0.8606\n",
            "145/168, train_loss: 0.5121\n",
            "146/168, train_loss: 0.6102\n",
            "147/168, train_loss: 0.4207\n",
            "148/168, train_loss: 0.4295\n",
            "149/168, train_loss: 0.7540\n",
            "150/168, train_loss: 0.9212\n",
            "151/168, train_loss: 0.3371\n",
            "152/168, train_loss: 0.4382\n",
            "153/168, train_loss: 0.4094\n",
            "154/168, train_loss: 0.3524\n",
            "155/168, train_loss: 0.4140\n",
            "156/168, train_loss: 0.4104\n",
            "157/168, train_loss: 1.0114\n",
            "158/168, train_loss: 0.4390\n",
            "159/168, train_loss: 0.4618\n",
            "160/168, train_loss: 1.0439\n",
            "161/168, train_loss: 0.8668\n",
            "162/168, train_loss: 0.9267\n",
            "163/168, train_loss: 0.4371\n",
            "164/168, train_loss: 0.3954\n",
            "165/168, train_loss: 0.4393\n",
            "166/168, train_loss: 0.4466\n",
            "167/168, train_loss: 1.2004\n",
            "168/168, train_loss: 0.4429\n",
            "169/168, train_loss: 1.0467\n",
            "epoch 7 average loss: 0.6115\n",
            "----------\n",
            "epoch 8/5\n",
            "1/168, train_loss: 0.4157\n",
            "2/168, train_loss: 0.7478\n",
            "3/168, train_loss: 1.0850\n",
            "4/168, train_loss: 0.9470\n",
            "5/168, train_loss: 1.1850\n",
            "6/168, train_loss: 0.4664\n",
            "7/168, train_loss: 0.3894\n",
            "8/168, train_loss: 0.3775\n",
            "9/168, train_loss: 0.4486\n",
            "10/168, train_loss: 1.1989\n",
            "11/168, train_loss: 0.4373\n",
            "12/168, train_loss: 1.0235\n",
            "13/168, train_loss: 0.7617\n",
            "14/168, train_loss: 1.0146\n",
            "15/168, train_loss: 0.5125\n",
            "16/168, train_loss: 0.7947\n",
            "17/168, train_loss: 0.4196\n",
            "18/168, train_loss: 1.0894\n",
            "19/168, train_loss: 0.4672\n",
            "20/168, train_loss: 0.4374\n",
            "21/168, train_loss: 0.7180\n",
            "22/168, train_loss: 1.0495\n",
            "23/168, train_loss: 0.8403\n",
            "24/168, train_loss: 0.6260\n",
            "25/168, train_loss: 0.4455\n",
            "26/168, train_loss: 0.7035\n",
            "27/168, train_loss: 0.3519\n",
            "28/168, train_loss: 0.5701\n",
            "29/168, train_loss: 1.0699\n",
            "30/168, train_loss: 0.5248\n",
            "31/168, train_loss: 0.4387\n",
            "32/168, train_loss: 0.5139\n",
            "33/168, train_loss: 0.4615\n",
            "34/168, train_loss: 0.7272\n",
            "35/168, train_loss: 0.4668\n",
            "36/168, train_loss: 0.7239\n",
            "37/168, train_loss: 0.6075\n",
            "38/168, train_loss: 0.2705\n",
            "39/168, train_loss: 0.9520\n",
            "40/168, train_loss: 0.6838\n",
            "41/168, train_loss: 0.4857\n",
            "42/168, train_loss: 0.4678\n",
            "43/168, train_loss: 0.4691\n",
            "44/168, train_loss: 0.4956\n",
            "45/168, train_loss: 0.4528\n",
            "46/168, train_loss: 0.4698\n",
            "47/168, train_loss: 0.4740\n",
            "48/168, train_loss: 0.4590\n",
            "49/168, train_loss: 0.8136\n",
            "50/168, train_loss: 0.9976\n",
            "51/168, train_loss: 0.3689\n",
            "52/168, train_loss: 0.4552\n",
            "53/168, train_loss: 0.4624\n",
            "54/168, train_loss: 0.7923\n",
            "55/168, train_loss: 0.5560\n",
            "56/168, train_loss: 0.6455\n",
            "57/168, train_loss: 0.8519\n",
            "58/168, train_loss: 0.4548\n",
            "59/168, train_loss: 0.9906\n",
            "60/168, train_loss: 0.6732\n",
            "61/168, train_loss: 0.4484\n",
            "62/168, train_loss: 0.4755\n",
            "63/168, train_loss: 0.7292\n",
            "64/168, train_loss: 0.6757\n",
            "65/168, train_loss: 0.3818\n",
            "66/168, train_loss: 1.0631\n",
            "67/168, train_loss: 0.7665\n",
            "68/168, train_loss: 1.0379\n",
            "69/168, train_loss: 0.6309\n",
            "70/168, train_loss: 0.4519\n",
            "71/168, train_loss: 0.4519\n",
            "72/168, train_loss: 0.4666\n",
            "73/168, train_loss: 0.4292\n",
            "74/168, train_loss: 0.7473\n",
            "75/168, train_loss: 0.7471\n",
            "76/168, train_loss: 0.4513\n",
            "77/168, train_loss: 0.4463\n",
            "78/168, train_loss: 0.5981\n",
            "79/168, train_loss: 0.4452\n",
            "80/168, train_loss: 0.5189\n",
            "81/168, train_loss: 0.7375\n",
            "82/168, train_loss: 0.4412\n",
            "83/168, train_loss: 0.4544\n",
            "84/168, train_loss: 0.7310\n",
            "85/168, train_loss: 0.4829\n",
            "86/168, train_loss: 0.7272\n",
            "87/168, train_loss: 0.5660\n",
            "88/168, train_loss: 0.9798\n",
            "89/168, train_loss: 0.4590\n",
            "90/168, train_loss: 0.4457\n",
            "91/168, train_loss: 0.4407\n",
            "92/168, train_loss: 0.4480\n",
            "93/168, train_loss: 1.0181\n",
            "94/168, train_loss: 1.0246\n",
            "95/168, train_loss: 0.4621\n",
            "96/168, train_loss: 0.4477\n",
            "97/168, train_loss: 1.1539\n",
            "98/168, train_loss: 0.4490\n",
            "99/168, train_loss: 0.7242\n",
            "100/168, train_loss: 0.9906\n",
            "101/168, train_loss: 0.4856\n",
            "102/168, train_loss: 0.4502\n",
            "103/168, train_loss: 0.5037\n",
            "104/168, train_loss: 0.4491\n",
            "105/168, train_loss: 0.4432\n",
            "106/168, train_loss: 0.4502\n",
            "107/168, train_loss: 0.4516\n",
            "108/168, train_loss: 0.7483\n",
            "109/168, train_loss: 0.7443\n",
            "110/168, train_loss: 0.4432\n",
            "111/168, train_loss: 0.8083\n",
            "112/168, train_loss: 1.0480\n",
            "113/168, train_loss: 0.4455\n",
            "114/168, train_loss: 0.4773\n",
            "115/168, train_loss: 0.4304\n",
            "116/168, train_loss: 0.7473\n",
            "117/168, train_loss: 0.7808\n",
            "118/168, train_loss: 0.4318\n",
            "119/168, train_loss: 0.8208\n",
            "120/168, train_loss: 0.4418\n",
            "121/168, train_loss: 0.4247\n",
            "122/168, train_loss: 0.4030\n",
            "123/168, train_loss: 0.4602\n",
            "124/168, train_loss: 0.4403\n",
            "125/168, train_loss: 0.7421\n",
            "126/168, train_loss: 0.4046\n",
            "127/168, train_loss: 0.3837\n",
            "128/168, train_loss: 0.6631\n",
            "129/168, train_loss: 0.4689\n",
            "130/168, train_loss: 1.0618\n",
            "131/168, train_loss: 0.8300\n",
            "132/168, train_loss: 0.4460\n",
            "133/168, train_loss: 0.4436\n",
            "134/168, train_loss: 0.4273\n",
            "135/168, train_loss: 1.0817\n",
            "136/168, train_loss: 1.0954\n",
            "137/168, train_loss: 0.6998\n",
            "138/168, train_loss: 0.4358\n",
            "139/168, train_loss: 0.4203\n",
            "140/168, train_loss: 0.6519\n",
            "141/168, train_loss: 0.5696\n",
            "142/168, train_loss: 0.4303\n",
            "143/168, train_loss: 0.7971\n",
            "144/168, train_loss: 0.3463\n",
            "145/168, train_loss: 0.4240\n",
            "146/168, train_loss: 0.6374\n",
            "147/168, train_loss: 0.4566\n",
            "148/168, train_loss: 0.5564\n",
            "149/168, train_loss: 0.6603\n",
            "150/168, train_loss: 0.4983\n",
            "151/168, train_loss: 0.7460\n",
            "152/168, train_loss: 0.4619\n",
            "153/168, train_loss: 0.5263\n",
            "154/168, train_loss: 0.6181\n",
            "155/168, train_loss: 0.9521\n",
            "156/168, train_loss: 0.4626\n",
            "157/168, train_loss: 0.9892\n",
            "158/168, train_loss: 0.4945\n",
            "159/168, train_loss: 0.9882\n",
            "160/168, train_loss: 0.4153\n",
            "161/168, train_loss: 0.4951\n",
            "162/168, train_loss: 1.2154\n",
            "163/168, train_loss: 1.0094\n",
            "164/168, train_loss: 0.4691\n",
            "165/168, train_loss: 0.4558\n",
            "166/168, train_loss: 0.4949\n",
            "167/168, train_loss: 1.0120\n",
            "168/168, train_loss: 0.7484\n",
            "169/168, train_loss: 0.4242\n",
            "epoch 8 average loss: 0.6266\n",
            "current epoch: 8 current accuracy: 0.4824 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 9/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 1.0224\n",
            "2/168, train_loss: 0.3741\n",
            "3/168, train_loss: 0.4258\n",
            "4/168, train_loss: 1.0035\n",
            "5/168, train_loss: 0.5906\n",
            "6/168, train_loss: 0.4438\n",
            "7/168, train_loss: 0.4529\n",
            "8/168, train_loss: 1.0284\n",
            "9/168, train_loss: 0.4404\n",
            "10/168, train_loss: 0.4486\n",
            "11/168, train_loss: 0.4407\n",
            "12/168, train_loss: 0.6061\n",
            "13/168, train_loss: 0.5574\n",
            "14/168, train_loss: 0.4462\n",
            "15/168, train_loss: 0.6594\n",
            "16/168, train_loss: 0.4161\n",
            "17/168, train_loss: 0.4749\n",
            "18/168, train_loss: 0.4743\n",
            "19/168, train_loss: 0.5895\n",
            "20/168, train_loss: 1.0771\n",
            "21/168, train_loss: 0.7623\n",
            "22/168, train_loss: 0.4777\n",
            "23/168, train_loss: 0.6344\n",
            "24/168, train_loss: 0.8505\n",
            "25/168, train_loss: 1.1107\n",
            "26/168, train_loss: 0.4291\n",
            "27/168, train_loss: 0.6414\n",
            "28/168, train_loss: 1.0057\n",
            "29/168, train_loss: 0.4588\n",
            "30/168, train_loss: 0.4186\n",
            "31/168, train_loss: 0.6084\n",
            "32/168, train_loss: 0.6613\n",
            "33/168, train_loss: 0.9327\n",
            "34/168, train_loss: 0.6973\n",
            "35/168, train_loss: 1.0146\n",
            "36/168, train_loss: 1.1153\n",
            "37/168, train_loss: 0.4609\n",
            "38/168, train_loss: 0.7648\n",
            "39/168, train_loss: 0.4392\n",
            "40/168, train_loss: 0.5898\n",
            "41/168, train_loss: 0.6098\n",
            "42/168, train_loss: 1.0027\n",
            "43/168, train_loss: 0.8898\n",
            "44/168, train_loss: 0.5936\n",
            "45/168, train_loss: 0.9630\n",
            "46/168, train_loss: 0.5046\n",
            "47/168, train_loss: 0.4711\n",
            "48/168, train_loss: 0.4366\n",
            "49/168, train_loss: 0.4545\n",
            "50/168, train_loss: 0.8140\n",
            "51/168, train_loss: 0.4702\n",
            "52/168, train_loss: 0.4003\n",
            "53/168, train_loss: 0.7386\n",
            "54/168, train_loss: 0.4788\n",
            "55/168, train_loss: 0.4384\n",
            "56/168, train_loss: 0.8532\n",
            "57/168, train_loss: 0.4398\n",
            "58/168, train_loss: 0.6529\n",
            "59/168, train_loss: 0.5301\n",
            "60/168, train_loss: 0.7408\n",
            "61/168, train_loss: 0.4552\n",
            "62/168, train_loss: 0.5063\n",
            "63/168, train_loss: 0.9842\n",
            "64/168, train_loss: 0.4604\n",
            "65/168, train_loss: 0.7243\n",
            "66/168, train_loss: 0.8390\n",
            "67/168, train_loss: 0.5705\n",
            "68/168, train_loss: 1.0268\n",
            "69/168, train_loss: 0.7350\n",
            "70/168, train_loss: 0.4502\n",
            "71/168, train_loss: 0.9569\n",
            "72/168, train_loss: 0.4842\n",
            "73/168, train_loss: 0.5557\n",
            "74/168, train_loss: 0.6671\n",
            "75/168, train_loss: 0.4920\n",
            "76/168, train_loss: 0.4470\n",
            "77/168, train_loss: 1.0731\n",
            "78/168, train_loss: 0.7397\n",
            "79/168, train_loss: 0.4799\n",
            "80/168, train_loss: 0.5273\n",
            "81/168, train_loss: 0.4432\n",
            "82/168, train_loss: 0.4486\n",
            "83/168, train_loss: 0.3513\n",
            "84/168, train_loss: 0.4152\n",
            "85/168, train_loss: 0.4733\n",
            "86/168, train_loss: 0.3344\n",
            "87/168, train_loss: 0.4471\n",
            "88/168, train_loss: 0.6087\n",
            "89/168, train_loss: 0.7437\n",
            "90/168, train_loss: 0.3515\n",
            "91/168, train_loss: 0.7396\n",
            "92/168, train_loss: 1.0316\n",
            "93/168, train_loss: 1.0120\n",
            "94/168, train_loss: 0.4661\n",
            "95/168, train_loss: 0.5616\n",
            "96/168, train_loss: 0.5739\n",
            "97/168, train_loss: 0.6084\n",
            "98/168, train_loss: 1.0163\n",
            "99/168, train_loss: 0.4610\n",
            "100/168, train_loss: 0.7316\n",
            "101/168, train_loss: 0.5585\n",
            "102/168, train_loss: 0.5140\n",
            "103/168, train_loss: 0.5925\n",
            "104/168, train_loss: 0.5093\n",
            "105/168, train_loss: 0.7923\n",
            "106/168, train_loss: 0.4917\n",
            "107/168, train_loss: 0.4593\n",
            "108/168, train_loss: 0.4984\n",
            "109/168, train_loss: 0.3949\n",
            "110/168, train_loss: 1.0441\n",
            "111/168, train_loss: 0.7320\n",
            "112/168, train_loss: 0.7983\n",
            "113/168, train_loss: 0.4575\n",
            "114/168, train_loss: 0.4848\n",
            "115/168, train_loss: 0.6631\n",
            "116/168, train_loss: 0.4732\n",
            "117/168, train_loss: 0.8483\n",
            "118/168, train_loss: 0.4746\n",
            "119/168, train_loss: 0.4407\n",
            "120/168, train_loss: 0.4669\n",
            "121/168, train_loss: 0.6430\n",
            "122/168, train_loss: 0.4589\n",
            "123/168, train_loss: 0.7866\n",
            "124/168, train_loss: 0.4542\n",
            "125/168, train_loss: 0.3300\n",
            "126/168, train_loss: 1.0149\n",
            "127/168, train_loss: 0.4326\n",
            "128/168, train_loss: 0.4480\n",
            "129/168, train_loss: 0.4510\n",
            "130/168, train_loss: 0.4929\n",
            "131/168, train_loss: 0.4296\n",
            "132/168, train_loss: 1.0723\n",
            "133/168, train_loss: 0.4480\n",
            "134/168, train_loss: 0.7380\n",
            "135/168, train_loss: 0.7365\n",
            "136/168, train_loss: 0.6438\n",
            "137/168, train_loss: 0.6433\n",
            "138/168, train_loss: 0.4309\n",
            "139/168, train_loss: 0.6608\n",
            "140/168, train_loss: 1.0319\n",
            "141/168, train_loss: 0.4814\n",
            "142/168, train_loss: 0.9581\n",
            "143/168, train_loss: 0.5940\n",
            "144/168, train_loss: 0.4694\n",
            "145/168, train_loss: 0.4337\n",
            "146/168, train_loss: 0.4574\n",
            "147/168, train_loss: 0.4264\n",
            "148/168, train_loss: 0.8476\n",
            "149/168, train_loss: 0.4151\n",
            "150/168, train_loss: 0.7361\n",
            "151/168, train_loss: 0.7868\n",
            "152/168, train_loss: 0.4390\n",
            "153/168, train_loss: 0.4414\n",
            "154/168, train_loss: 0.6790\n",
            "155/168, train_loss: 0.4038\n",
            "156/168, train_loss: 0.4668\n",
            "157/168, train_loss: 0.4643\n",
            "158/168, train_loss: 0.5137\n",
            "159/168, train_loss: 0.4587\n",
            "160/168, train_loss: 0.6691\n",
            "161/168, train_loss: 0.4567\n",
            "162/168, train_loss: 0.4488\n",
            "163/168, train_loss: 0.6223\n",
            "164/168, train_loss: 0.4037\n",
            "165/168, train_loss: 0.4438\n",
            "166/168, train_loss: 0.7529\n",
            "167/168, train_loss: 0.4392\n",
            "168/168, train_loss: 1.0304\n",
            "169/168, train_loss: 0.4179\n",
            "epoch 9 average loss: 0.6137\n",
            "----------\n",
            "epoch 10/5\n",
            "1/168, train_loss: 0.4567\n",
            "2/168, train_loss: 0.7558\n",
            "3/168, train_loss: 0.4691\n",
            "4/168, train_loss: 0.4243\n",
            "5/168, train_loss: 0.5158\n",
            "6/168, train_loss: 1.1182\n",
            "7/168, train_loss: 0.4134\n",
            "8/168, train_loss: 0.4267\n",
            "9/168, train_loss: 0.4334\n",
            "10/168, train_loss: 0.4346\n",
            "11/168, train_loss: 0.4392\n",
            "12/168, train_loss: 0.4163\n",
            "13/168, train_loss: 0.4651\n",
            "14/168, train_loss: 0.4741\n",
            "15/168, train_loss: 0.4156\n",
            "16/168, train_loss: 1.0699\n",
            "17/168, train_loss: 0.4102\n",
            "18/168, train_loss: 1.1151\n",
            "19/168, train_loss: 0.3562\n",
            "20/168, train_loss: 0.4069\n",
            "21/168, train_loss: 0.5027\n",
            "22/168, train_loss: 1.0692\n",
            "23/168, train_loss: 0.9696\n",
            "24/168, train_loss: 0.9015\n",
            "25/168, train_loss: 0.4952\n",
            "26/168, train_loss: 0.4774\n",
            "27/168, train_loss: 0.4223\n",
            "28/168, train_loss: 0.5989\n",
            "29/168, train_loss: 0.4740\n",
            "30/168, train_loss: 0.4446\n",
            "31/168, train_loss: 0.4273\n",
            "32/168, train_loss: 0.7420\n",
            "33/168, train_loss: 0.4053\n",
            "34/168, train_loss: 1.0766\n",
            "35/168, train_loss: 0.4463\n",
            "36/168, train_loss: 0.3049\n",
            "37/168, train_loss: 0.4123\n",
            "38/168, train_loss: 0.4314\n",
            "39/168, train_loss: 1.0571\n",
            "40/168, train_loss: 0.6081\n",
            "41/168, train_loss: 0.4242\n",
            "42/168, train_loss: 0.5801\n",
            "43/168, train_loss: 0.9698\n",
            "44/168, train_loss: 0.4004\n",
            "45/168, train_loss: 0.4933\n",
            "46/168, train_loss: 0.4198\n",
            "47/168, train_loss: 0.7301\n",
            "48/168, train_loss: 0.4763\n",
            "49/168, train_loss: 0.4143\n",
            "50/168, train_loss: 1.0542\n",
            "51/168, train_loss: 0.4351\n",
            "52/168, train_loss: 0.4660\n",
            "53/168, train_loss: 0.4551\n",
            "54/168, train_loss: 0.9949\n",
            "55/168, train_loss: 0.7209\n",
            "56/168, train_loss: 0.4683\n",
            "57/168, train_loss: 0.4442\n",
            "58/168, train_loss: 0.6535\n",
            "59/168, train_loss: 0.4927\n",
            "60/168, train_loss: 0.4031\n",
            "61/168, train_loss: 0.3141\n",
            "62/168, train_loss: 0.4263\n",
            "63/168, train_loss: 0.8923\n",
            "64/168, train_loss: 1.0569\n",
            "65/168, train_loss: 0.7449\n",
            "66/168, train_loss: 1.3411\n",
            "67/168, train_loss: 0.4879\n",
            "68/168, train_loss: 0.4441\n",
            "69/168, train_loss: 0.4458\n",
            "70/168, train_loss: 0.2891\n",
            "71/168, train_loss: 1.0483\n",
            "72/168, train_loss: 0.5093\n",
            "73/168, train_loss: 0.4259\n",
            "74/168, train_loss: 0.4342\n",
            "75/168, train_loss: 0.5188\n",
            "76/168, train_loss: 1.0733\n",
            "77/168, train_loss: 0.4373\n",
            "78/168, train_loss: 0.4240\n",
            "79/168, train_loss: 0.4458\n",
            "80/168, train_loss: 1.0734\n",
            "81/168, train_loss: 1.1423\n",
            "82/168, train_loss: 0.4335\n",
            "83/168, train_loss: 0.4283\n",
            "84/168, train_loss: 0.4675\n",
            "85/168, train_loss: 0.4184\n",
            "86/168, train_loss: 1.0788\n",
            "87/168, train_loss: 1.0530\n",
            "88/168, train_loss: 0.4538\n",
            "89/168, train_loss: 0.5602\n",
            "90/168, train_loss: 0.4115\n",
            "91/168, train_loss: 0.4482\n",
            "92/168, train_loss: 0.4281\n",
            "93/168, train_loss: 0.2603\n",
            "94/168, train_loss: 0.7439\n",
            "95/168, train_loss: 0.4270\n",
            "96/168, train_loss: 1.0577\n",
            "97/168, train_loss: 0.4110\n",
            "98/168, train_loss: 0.4196\n",
            "99/168, train_loss: 0.4352\n",
            "100/168, train_loss: 1.0507\n",
            "101/168, train_loss: 0.7615\n",
            "102/168, train_loss: 0.4397\n",
            "103/168, train_loss: 1.0548\n",
            "104/168, train_loss: 0.5274\n",
            "105/168, train_loss: 0.4051\n",
            "106/168, train_loss: 0.4363\n",
            "107/168, train_loss: 0.5165\n",
            "108/168, train_loss: 0.3423\n",
            "109/168, train_loss: 0.4558\n",
            "110/168, train_loss: 0.6076\n",
            "111/168, train_loss: 0.3646\n",
            "112/168, train_loss: 0.7413\n",
            "113/168, train_loss: 0.3888\n",
            "114/168, train_loss: 0.6959\n",
            "115/168, train_loss: 0.4699\n",
            "116/168, train_loss: 1.0672\n",
            "117/168, train_loss: 0.4949\n",
            "118/168, train_loss: 0.4613\n",
            "119/168, train_loss: 1.0473\n",
            "120/168, train_loss: 0.4274\n",
            "121/168, train_loss: 0.4446\n",
            "122/168, train_loss: 0.4258\n",
            "123/168, train_loss: 0.4282\n",
            "124/168, train_loss: 0.4962\n",
            "125/168, train_loss: 0.8454\n",
            "126/168, train_loss: 0.4739\n",
            "127/168, train_loss: 0.4252\n",
            "128/168, train_loss: 0.5246\n",
            "129/168, train_loss: 0.4242\n",
            "130/168, train_loss: 0.9885\n",
            "131/168, train_loss: 0.7472\n",
            "132/168, train_loss: 0.4489\n",
            "133/168, train_loss: 1.0326\n",
            "134/168, train_loss: 0.4059\n",
            "135/168, train_loss: 0.4218\n",
            "136/168, train_loss: 1.0537\n",
            "137/168, train_loss: 0.8310\n",
            "138/168, train_loss: 0.4291\n",
            "139/168, train_loss: 0.4742\n",
            "140/168, train_loss: 1.1038\n",
            "141/168, train_loss: 0.2471\n",
            "142/168, train_loss: 0.5004\n",
            "143/168, train_loss: 0.4695\n",
            "144/168, train_loss: 1.0142\n",
            "145/168, train_loss: 0.4966\n",
            "146/168, train_loss: 0.7488\n",
            "147/168, train_loss: 0.4199\n",
            "148/168, train_loss: 0.4122\n",
            "149/168, train_loss: 1.1452\n",
            "150/168, train_loss: 0.7586\n",
            "151/168, train_loss: 0.7483\n",
            "152/168, train_loss: 0.7546\n",
            "153/168, train_loss: 0.4145\n",
            "154/168, train_loss: 0.4349\n",
            "155/168, train_loss: 0.4916\n",
            "156/168, train_loss: 0.5290\n",
            "157/168, train_loss: 1.0163\n",
            "158/168, train_loss: 0.4593\n",
            "159/168, train_loss: 0.9587\n",
            "160/168, train_loss: 0.4881\n",
            "161/168, train_loss: 0.3889\n",
            "162/168, train_loss: 0.5279\n",
            "163/168, train_loss: 0.7474\n",
            "164/168, train_loss: 0.7062\n",
            "165/168, train_loss: 0.4523\n",
            "166/168, train_loss: 0.4203\n",
            "167/168, train_loss: 1.0589\n",
            "168/168, train_loss: 0.3783\n",
            "169/168, train_loss: 0.4307\n",
            "epoch 10 average loss: 0.6005\n",
            "current epoch: 10 current accuracy: 0.3882 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 11/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 1.0216\n",
            "2/168, train_loss: 0.4555\n",
            "3/168, train_loss: 0.5822\n",
            "4/168, train_loss: 0.5317\n",
            "5/168, train_loss: 1.0345\n",
            "6/168, train_loss: 0.4818\n",
            "7/168, train_loss: 0.4361\n",
            "8/168, train_loss: 0.6527\n",
            "9/168, train_loss: 1.0871\n",
            "10/168, train_loss: 0.3315\n",
            "11/168, train_loss: 0.5184\n",
            "12/168, train_loss: 0.8718\n",
            "13/168, train_loss: 0.4378\n",
            "14/168, train_loss: 0.4483\n",
            "15/168, train_loss: 0.4502\n",
            "16/168, train_loss: 1.0098\n",
            "17/168, train_loss: 0.4793\n",
            "18/168, train_loss: 1.0443\n",
            "19/168, train_loss: 0.4425\n",
            "20/168, train_loss: 0.4812\n",
            "21/168, train_loss: 0.4360\n",
            "22/168, train_loss: 0.4936\n",
            "23/168, train_loss: 0.4522\n",
            "24/168, train_loss: 1.0265\n",
            "25/168, train_loss: 0.7282\n",
            "26/168, train_loss: 0.4240\n",
            "27/168, train_loss: 0.4314\n",
            "28/168, train_loss: 0.6143\n",
            "29/168, train_loss: 0.4288\n",
            "30/168, train_loss: 0.5136\n",
            "31/168, train_loss: 0.4273\n",
            "32/168, train_loss: 1.0647\n",
            "33/168, train_loss: 1.0382\n",
            "34/168, train_loss: 0.7359\n",
            "35/168, train_loss: 1.1557\n",
            "36/168, train_loss: 0.4291\n",
            "37/168, train_loss: 0.4395\n",
            "38/168, train_loss: 0.3445\n",
            "39/168, train_loss: 1.2004\n",
            "40/168, train_loss: 0.7483\n",
            "41/168, train_loss: 0.5697\n",
            "42/168, train_loss: 0.7419\n",
            "43/168, train_loss: 0.4653\n",
            "44/168, train_loss: 0.9850\n",
            "45/168, train_loss: 0.7328\n",
            "46/168, train_loss: 0.4337\n",
            "47/168, train_loss: 0.7244\n",
            "48/168, train_loss: 0.4538\n",
            "49/168, train_loss: 0.9841\n",
            "50/168, train_loss: 0.9776\n",
            "51/168, train_loss: 0.9536\n",
            "52/168, train_loss: 0.4096\n",
            "53/168, train_loss: 0.7189\n",
            "54/168, train_loss: 0.5142\n",
            "55/168, train_loss: 0.4299\n",
            "56/168, train_loss: 0.4883\n",
            "57/168, train_loss: 0.5717\n",
            "58/168, train_loss: 0.7156\n",
            "59/168, train_loss: 0.5017\n",
            "60/168, train_loss: 0.3992\n",
            "61/168, train_loss: 0.7257\n",
            "62/168, train_loss: 1.0172\n",
            "63/168, train_loss: 0.5041\n",
            "64/168, train_loss: 0.7112\n",
            "65/168, train_loss: 0.4259\n",
            "66/168, train_loss: 0.1887\n",
            "67/168, train_loss: 0.6050\n",
            "68/168, train_loss: 0.4547\n",
            "69/168, train_loss: 0.3573\n",
            "70/168, train_loss: 0.4777\n",
            "71/168, train_loss: 0.4800\n",
            "72/168, train_loss: 1.0182\n",
            "73/168, train_loss: 0.4558\n",
            "74/168, train_loss: 0.4975\n",
            "75/168, train_loss: 0.4773\n",
            "76/168, train_loss: 0.8369\n",
            "77/168, train_loss: 0.4580\n",
            "78/168, train_loss: 0.4231\n",
            "79/168, train_loss: 0.5348\n",
            "80/168, train_loss: 0.5200\n",
            "81/168, train_loss: 0.7158\n",
            "82/168, train_loss: 0.9670\n",
            "83/168, train_loss: 0.4657\n",
            "84/168, train_loss: 0.4663\n",
            "85/168, train_loss: 0.5566\n",
            "86/168, train_loss: 0.9247\n",
            "87/168, train_loss: 0.4467\n",
            "88/168, train_loss: 0.5117\n",
            "89/168, train_loss: 0.4849\n",
            "90/168, train_loss: 0.7250\n",
            "91/168, train_loss: 0.4561\n",
            "92/168, train_loss: 0.4750\n",
            "93/168, train_loss: 0.4429\n",
            "94/168, train_loss: 0.6041\n",
            "95/168, train_loss: 0.9689\n",
            "96/168, train_loss: 0.7901\n",
            "97/168, train_loss: 0.4487\n",
            "98/168, train_loss: 1.0502\n",
            "99/168, train_loss: 1.1086\n",
            "100/168, train_loss: 0.4598\n",
            "101/168, train_loss: 0.2519\n",
            "102/168, train_loss: 0.4405\n",
            "103/168, train_loss: 0.4533\n",
            "104/168, train_loss: 0.2546\n",
            "105/168, train_loss: 0.4945\n",
            "106/168, train_loss: 0.6389\n",
            "107/168, train_loss: 0.6951\n",
            "108/168, train_loss: 0.7210\n",
            "109/168, train_loss: 0.4354\n",
            "110/168, train_loss: 0.4398\n",
            "111/168, train_loss: 0.4057\n",
            "112/168, train_loss: 1.0316\n",
            "113/168, train_loss: 0.5795\n",
            "114/168, train_loss: 0.4891\n",
            "115/168, train_loss: 0.4471\n",
            "116/168, train_loss: 0.4941\n",
            "117/168, train_loss: 0.7160\n",
            "118/168, train_loss: 0.4299\n",
            "119/168, train_loss: 1.0049\n",
            "120/168, train_loss: 0.6241\n",
            "121/168, train_loss: 0.4509\n",
            "122/168, train_loss: 0.3383\n",
            "123/168, train_loss: 0.4610\n",
            "124/168, train_loss: 0.4494\n",
            "125/168, train_loss: 0.4654\n",
            "126/168, train_loss: 0.4360\n",
            "127/168, train_loss: 1.0167\n",
            "128/168, train_loss: 0.7111\n",
            "129/168, train_loss: 0.8674\n",
            "130/168, train_loss: 0.5115\n",
            "131/168, train_loss: 0.5442\n",
            "132/168, train_loss: 1.0856\n",
            "133/168, train_loss: 0.4576\n",
            "134/168, train_loss: 0.7076\n",
            "135/168, train_loss: 0.4643\n",
            "136/168, train_loss: 1.0437\n",
            "137/168, train_loss: 0.5306\n",
            "138/168, train_loss: 0.4688\n",
            "139/168, train_loss: 0.4762\n",
            "140/168, train_loss: 0.4268\n",
            "141/168, train_loss: 0.7049\n",
            "142/168, train_loss: 0.4910\n",
            "143/168, train_loss: 0.4305\n",
            "144/168, train_loss: 0.7274\n",
            "145/168, train_loss: 0.4895\n",
            "146/168, train_loss: 0.4471\n",
            "147/168, train_loss: 0.4795\n",
            "148/168, train_loss: 1.1189\n",
            "149/168, train_loss: 0.4234\n",
            "150/168, train_loss: 0.4415\n",
            "151/168, train_loss: 0.5580\n",
            "152/168, train_loss: 0.4931\n",
            "153/168, train_loss: 0.5106\n",
            "154/168, train_loss: 0.4696\n",
            "155/168, train_loss: 1.0148\n",
            "156/168, train_loss: 0.4165\n",
            "157/168, train_loss: 0.4432\n",
            "158/168, train_loss: 0.4442\n",
            "159/168, train_loss: 0.7417\n",
            "160/168, train_loss: 0.4450\n",
            "161/168, train_loss: 0.8842\n",
            "162/168, train_loss: 1.0322\n",
            "163/168, train_loss: 0.4260\n",
            "164/168, train_loss: 0.4430\n",
            "165/168, train_loss: 0.8322\n",
            "166/168, train_loss: 0.6264\n",
            "167/168, train_loss: 0.5825\n",
            "168/168, train_loss: 1.0884\n",
            "169/168, train_loss: 1.0527\n",
            "epoch 11 average loss: 0.6163\n",
            "----------\n",
            "epoch 12/5\n",
            "1/168, train_loss: 1.0612\n",
            "2/168, train_loss: 0.4890\n",
            "3/168, train_loss: 0.6410\n",
            "4/168, train_loss: 0.6082\n",
            "5/168, train_loss: 1.0693\n",
            "6/168, train_loss: 0.5025\n",
            "7/168, train_loss: 0.4468\n",
            "8/168, train_loss: 0.5402\n",
            "9/168, train_loss: 0.7331\n",
            "10/168, train_loss: 0.4348\n",
            "11/168, train_loss: 0.4974\n",
            "12/168, train_loss: 0.7510\n",
            "13/168, train_loss: 0.9877\n",
            "14/168, train_loss: 0.4323\n",
            "15/168, train_loss: 0.4139\n",
            "16/168, train_loss: 0.4778\n",
            "17/168, train_loss: 0.4837\n",
            "18/168, train_loss: 0.5728\n",
            "19/168, train_loss: 0.4688\n",
            "20/168, train_loss: 0.3736\n",
            "21/168, train_loss: 0.4584\n",
            "22/168, train_loss: 0.4158\n",
            "23/168, train_loss: 0.4934\n",
            "24/168, train_loss: 0.5040\n",
            "25/168, train_loss: 0.4229\n",
            "26/168, train_loss: 0.4494\n",
            "27/168, train_loss: 0.4240\n",
            "28/168, train_loss: 0.5156\n",
            "29/168, train_loss: 0.3840\n",
            "30/168, train_loss: 1.0364\n",
            "31/168, train_loss: 1.0563\n",
            "32/168, train_loss: 0.4080\n",
            "33/168, train_loss: 0.3149\n",
            "34/168, train_loss: 0.4394\n",
            "35/168, train_loss: 0.8074\n",
            "36/168, train_loss: 1.1063\n",
            "37/168, train_loss: 1.0389\n",
            "38/168, train_loss: 0.7448\n",
            "39/168, train_loss: 0.5411\n",
            "40/168, train_loss: 0.4446\n",
            "41/168, train_loss: 0.4705\n",
            "42/168, train_loss: 0.4401\n",
            "43/168, train_loss: 0.7364\n",
            "44/168, train_loss: 0.8524\n",
            "45/168, train_loss: 1.0920\n",
            "46/168, train_loss: 0.4556\n",
            "47/168, train_loss: 1.0240\n",
            "48/168, train_loss: 0.4336\n",
            "49/168, train_loss: 1.0985\n",
            "50/168, train_loss: 1.1298\n",
            "51/168, train_loss: 0.6325\n",
            "52/168, train_loss: 0.9813\n",
            "53/168, train_loss: 0.4561\n",
            "54/168, train_loss: 0.9400\n",
            "55/168, train_loss: 0.4276\n",
            "56/168, train_loss: 0.4421\n",
            "57/168, train_loss: 0.4431\n",
            "58/168, train_loss: 0.7120\n",
            "59/168, train_loss: 0.4274\n",
            "60/168, train_loss: 1.0434\n",
            "61/168, train_loss: 0.5204\n",
            "62/168, train_loss: 0.4215\n",
            "63/168, train_loss: 0.4353\n",
            "64/168, train_loss: 0.4317\n",
            "65/168, train_loss: 0.6728\n",
            "66/168, train_loss: 1.0668\n",
            "67/168, train_loss: 0.5394\n",
            "68/168, train_loss: 0.4334\n",
            "69/168, train_loss: 0.4331\n",
            "70/168, train_loss: 0.4475\n",
            "71/168, train_loss: 1.0785\n",
            "72/168, train_loss: 0.4214\n",
            "73/168, train_loss: 0.4255\n",
            "74/168, train_loss: 0.4484\n",
            "75/168, train_loss: 1.0032\n",
            "76/168, train_loss: 0.4371\n",
            "77/168, train_loss: 0.6422\n",
            "78/168, train_loss: 0.4171\n",
            "79/168, train_loss: 0.4916\n",
            "80/168, train_loss: 0.6930\n",
            "81/168, train_loss: 0.3546\n",
            "82/168, train_loss: 0.9745\n",
            "83/168, train_loss: 1.0577\n",
            "84/168, train_loss: 1.0321\n",
            "85/168, train_loss: 0.4224\n",
            "86/168, train_loss: 0.4380\n",
            "87/168, train_loss: 0.4439\n",
            "88/168, train_loss: 0.5102\n",
            "89/168, train_loss: 0.2915\n",
            "90/168, train_loss: 0.4751\n",
            "91/168, train_loss: 0.4378\n",
            "92/168, train_loss: 0.5976\n",
            "93/168, train_loss: 0.5369\n",
            "94/168, train_loss: 0.4323\n",
            "95/168, train_loss: 0.4235\n",
            "96/168, train_loss: 0.4127\n",
            "97/168, train_loss: 0.6491\n",
            "98/168, train_loss: 0.4511\n",
            "99/168, train_loss: 0.6497\n",
            "100/168, train_loss: 0.4964\n",
            "101/168, train_loss: 0.4211\n",
            "102/168, train_loss: 0.4335\n",
            "103/168, train_loss: 0.4411\n",
            "104/168, train_loss: 0.7293\n",
            "105/168, train_loss: 1.0419\n",
            "106/168, train_loss: 0.7255\n",
            "107/168, train_loss: 1.0298\n",
            "108/168, train_loss: 0.9937\n",
            "109/168, train_loss: 0.7956\n",
            "110/168, train_loss: 0.4391\n",
            "111/168, train_loss: 0.4935\n",
            "112/168, train_loss: 0.3542\n",
            "113/168, train_loss: 0.4676\n",
            "114/168, train_loss: 0.4522\n",
            "115/168, train_loss: 0.8421\n",
            "116/168, train_loss: 1.0526\n",
            "117/168, train_loss: 0.4308\n",
            "118/168, train_loss: 0.4471\n",
            "119/168, train_loss: 0.4552\n",
            "120/168, train_loss: 1.0885\n",
            "121/168, train_loss: 0.4430\n",
            "122/168, train_loss: 1.1325\n",
            "123/168, train_loss: 0.2888\n",
            "124/168, train_loss: 0.8248\n",
            "125/168, train_loss: 0.4263\n",
            "126/168, train_loss: 0.5907\n",
            "127/168, train_loss: 0.4399\n",
            "128/168, train_loss: 0.3672\n",
            "129/168, train_loss: 0.3032\n",
            "130/168, train_loss: 0.7318\n",
            "131/168, train_loss: 0.4573\n",
            "132/168, train_loss: 0.5971\n",
            "133/168, train_loss: 0.6802\n",
            "134/168, train_loss: 0.5487\n",
            "135/168, train_loss: 0.7267\n",
            "136/168, train_loss: 0.4291\n",
            "137/168, train_loss: 0.2722\n",
            "138/168, train_loss: 0.4645\n",
            "139/168, train_loss: 0.4455\n",
            "140/168, train_loss: 0.6613\n",
            "141/168, train_loss: 0.5733\n",
            "142/168, train_loss: 0.6449\n",
            "143/168, train_loss: 0.8446\n",
            "144/168, train_loss: 0.4584\n",
            "145/168, train_loss: 0.4807\n",
            "146/168, train_loss: 0.6908\n",
            "147/168, train_loss: 0.4625\n",
            "148/168, train_loss: 0.6635\n",
            "149/168, train_loss: 0.6431\n",
            "150/168, train_loss: 0.4244\n",
            "151/168, train_loss: 0.6287\n",
            "152/168, train_loss: 0.5463\n",
            "153/168, train_loss: 0.4215\n",
            "154/168, train_loss: 0.4392\n",
            "155/168, train_loss: 0.4819\n",
            "156/168, train_loss: 1.0256\n",
            "157/168, train_loss: 0.3945\n",
            "158/168, train_loss: 1.0130\n",
            "159/168, train_loss: 0.5041\n",
            "160/168, train_loss: 0.4979\n",
            "161/168, train_loss: 0.7303\n",
            "162/168, train_loss: 0.4802\n",
            "163/168, train_loss: 1.0231\n",
            "164/168, train_loss: 0.4836\n",
            "165/168, train_loss: 0.5744\n",
            "166/168, train_loss: 0.4315\n",
            "167/168, train_loss: 0.4641\n",
            "168/168, train_loss: 0.4414\n",
            "169/168, train_loss: 1.0305\n",
            "epoch 12 average loss: 0.6050\n",
            "current epoch: 12 current accuracy: 0.4118 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 13/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.7302\n",
            "2/168, train_loss: 0.4373\n",
            "3/168, train_loss: 0.4439\n",
            "4/168, train_loss: 0.4777\n",
            "5/168, train_loss: 0.8679\n",
            "6/168, train_loss: 0.9190\n",
            "7/168, train_loss: 0.7414\n",
            "8/168, train_loss: 0.4122\n",
            "9/168, train_loss: 1.0339\n",
            "10/168, train_loss: 0.7281\n",
            "11/168, train_loss: 0.4467\n",
            "12/168, train_loss: 0.4466\n",
            "13/168, train_loss: 1.0522\n",
            "14/168, train_loss: 1.0461\n",
            "15/168, train_loss: 0.4786\n",
            "16/168, train_loss: 1.1072\n",
            "17/168, train_loss: 1.0108\n",
            "18/168, train_loss: 0.9139\n",
            "19/168, train_loss: 0.4045\n",
            "20/168, train_loss: 0.5354\n",
            "21/168, train_loss: 0.9606\n",
            "22/168, train_loss: 0.7302\n",
            "23/168, train_loss: 0.4161\n",
            "24/168, train_loss: 0.7126\n",
            "25/168, train_loss: 0.4359\n",
            "26/168, train_loss: 0.6607\n",
            "27/168, train_loss: 1.0851\n",
            "28/168, train_loss: 0.4544\n",
            "29/168, train_loss: 0.4428\n",
            "30/168, train_loss: 0.7098\n",
            "31/168, train_loss: 0.4342\n",
            "32/168, train_loss: 0.4551\n",
            "33/168, train_loss: 0.5733\n",
            "34/168, train_loss: 0.7061\n",
            "35/168, train_loss: 0.5868\n",
            "36/168, train_loss: 0.5248\n",
            "37/168, train_loss: 1.0357\n",
            "38/168, train_loss: 0.5318\n",
            "39/168, train_loss: 0.2095\n",
            "40/168, train_loss: 0.4717\n",
            "41/168, train_loss: 0.4493\n",
            "42/168, train_loss: 0.4552\n",
            "43/168, train_loss: 1.0291\n",
            "44/168, train_loss: 0.3753\n",
            "45/168, train_loss: 0.4359\n",
            "46/168, train_loss: 0.3390\n",
            "47/168, train_loss: 0.9969\n",
            "48/168, train_loss: 0.5509\n",
            "49/168, train_loss: 0.7328\n",
            "50/168, train_loss: 0.5343\n",
            "51/168, train_loss: 0.5140\n",
            "52/168, train_loss: 0.3734\n",
            "53/168, train_loss: 0.5049\n",
            "54/168, train_loss: 1.0422\n",
            "55/168, train_loss: 1.0271\n",
            "56/168, train_loss: 0.4688\n",
            "57/168, train_loss: 0.5555\n",
            "58/168, train_loss: 0.4886\n",
            "59/168, train_loss: 0.5632\n",
            "60/168, train_loss: 0.4444\n",
            "61/168, train_loss: 0.4421\n",
            "62/168, train_loss: 0.3889\n",
            "63/168, train_loss: 0.4361\n",
            "64/168, train_loss: 0.4393\n",
            "65/168, train_loss: 0.4165\n",
            "66/168, train_loss: 0.5825\n",
            "67/168, train_loss: 0.5275\n",
            "68/168, train_loss: 1.0439\n",
            "69/168, train_loss: 0.4553\n",
            "70/168, train_loss: 0.4376\n",
            "71/168, train_loss: 0.4726\n",
            "72/168, train_loss: 0.7624\n",
            "73/168, train_loss: 0.5554\n",
            "74/168, train_loss: 0.9733\n",
            "75/168, train_loss: 0.4843\n",
            "76/168, train_loss: 0.4384\n",
            "77/168, train_loss: 0.6689\n",
            "78/168, train_loss: 0.4272\n",
            "79/168, train_loss: 0.9452\n",
            "80/168, train_loss: 0.4337\n",
            "81/168, train_loss: 1.0710\n",
            "82/168, train_loss: 0.4176\n",
            "83/168, train_loss: 0.4292\n",
            "84/168, train_loss: 0.7316\n",
            "85/168, train_loss: 0.4653\n",
            "86/168, train_loss: 0.4501\n",
            "87/168, train_loss: 0.6681\n",
            "88/168, train_loss: 0.4268\n",
            "89/168, train_loss: 0.3889\n",
            "90/168, train_loss: 1.0267\n",
            "91/168, train_loss: 0.4781\n",
            "92/168, train_loss: 0.4536\n",
            "93/168, train_loss: 0.4735\n",
            "94/168, train_loss: 0.4308\n",
            "95/168, train_loss: 1.0256\n",
            "96/168, train_loss: 0.4336\n",
            "97/168, train_loss: 0.5389\n",
            "98/168, train_loss: 0.4432\n",
            "99/168, train_loss: 0.4300\n",
            "100/168, train_loss: 0.7433\n",
            "101/168, train_loss: 0.3747\n",
            "102/168, train_loss: 0.7486\n",
            "103/168, train_loss: 1.0463\n",
            "104/168, train_loss: 0.3816\n",
            "105/168, train_loss: 0.4275\n",
            "106/168, train_loss: 0.7402\n",
            "107/168, train_loss: 0.7168\n",
            "108/168, train_loss: 0.4192\n",
            "109/168, train_loss: 0.6887\n",
            "110/168, train_loss: 0.4465\n",
            "111/168, train_loss: 0.4178\n",
            "112/168, train_loss: 0.4069\n",
            "113/168, train_loss: 0.4605\n",
            "114/168, train_loss: 0.4305\n",
            "115/168, train_loss: 0.4238\n",
            "116/168, train_loss: 0.4580\n",
            "117/168, train_loss: 0.4694\n",
            "118/168, train_loss: 0.4310\n",
            "119/168, train_loss: 0.6409\n",
            "120/168, train_loss: 0.7496\n",
            "121/168, train_loss: 0.7146\n",
            "122/168, train_loss: 0.4215\n",
            "123/168, train_loss: 0.4226\n",
            "124/168, train_loss: 1.0567\n",
            "125/168, train_loss: 0.4290\n",
            "126/168, train_loss: 0.5412\n",
            "127/168, train_loss: 0.5773\n",
            "128/168, train_loss: 0.4097\n",
            "129/168, train_loss: 0.9507\n",
            "130/168, train_loss: 0.3958\n",
            "131/168, train_loss: 0.5890\n",
            "132/168, train_loss: 1.0955\n",
            "133/168, train_loss: 0.3747\n",
            "134/168, train_loss: 0.4103\n",
            "135/168, train_loss: 1.0763\n",
            "136/168, train_loss: 0.4004\n",
            "137/168, train_loss: 0.3793\n",
            "138/168, train_loss: 0.3354\n",
            "139/168, train_loss: 0.4148\n",
            "140/168, train_loss: 0.7679\n",
            "141/168, train_loss: 0.7753\n",
            "142/168, train_loss: 1.0468\n",
            "143/168, train_loss: 0.4062\n",
            "144/168, train_loss: 0.6523\n",
            "145/168, train_loss: 1.0583\n",
            "146/168, train_loss: 0.3697\n",
            "147/168, train_loss: 0.7558\n",
            "148/168, train_loss: 0.7449\n",
            "149/168, train_loss: 0.6724\n",
            "150/168, train_loss: 0.7635\n",
            "151/168, train_loss: 0.4309\n",
            "152/168, train_loss: 0.5722\n",
            "153/168, train_loss: 1.0424\n",
            "154/168, train_loss: 0.4276\n",
            "155/168, train_loss: 0.2715\n",
            "156/168, train_loss: 0.4759\n",
            "157/168, train_loss: 0.4651\n",
            "158/168, train_loss: 0.4326\n",
            "159/168, train_loss: 0.7371\n",
            "160/168, train_loss: 0.4701\n",
            "161/168, train_loss: 0.4259\n",
            "162/168, train_loss: 0.4490\n",
            "163/168, train_loss: 0.4198\n",
            "164/168, train_loss: 0.5182\n",
            "165/168, train_loss: 0.6037\n",
            "166/168, train_loss: 0.4782\n",
            "167/168, train_loss: 0.7300\n",
            "168/168, train_loss: 1.0252\n",
            "169/168, train_loss: 0.4078\n",
            "epoch 13 average loss: 0.5999\n",
            "----------\n",
            "epoch 14/5\n",
            "1/168, train_loss: 0.4217\n",
            "2/168, train_loss: 0.5785\n",
            "3/168, train_loss: 0.8840\n",
            "4/168, train_loss: 0.7626\n",
            "5/168, train_loss: 0.9740\n",
            "6/168, train_loss: 0.4191\n",
            "7/168, train_loss: 0.2989\n",
            "8/168, train_loss: 0.7382\n",
            "9/168, train_loss: 0.4779\n",
            "10/168, train_loss: 0.7290\n",
            "11/168, train_loss: 0.4458\n",
            "12/168, train_loss: 0.4287\n",
            "13/168, train_loss: 0.2149\n",
            "14/168, train_loss: 0.4236\n",
            "15/168, train_loss: 0.5772\n",
            "16/168, train_loss: 0.4212\n",
            "17/168, train_loss: 0.2072\n",
            "18/168, train_loss: 0.3527\n",
            "19/168, train_loss: 0.2984\n",
            "20/168, train_loss: 1.0349\n",
            "21/168, train_loss: 0.5062\n",
            "22/168, train_loss: 0.4421\n",
            "23/168, train_loss: 1.0451\n",
            "24/168, train_loss: 0.6508\n",
            "25/168, train_loss: 0.2399\n",
            "26/168, train_loss: 0.4416\n",
            "27/168, train_loss: 1.1168\n",
            "28/168, train_loss: 0.4190\n",
            "29/168, train_loss: 0.3865\n",
            "30/168, train_loss: 0.4144\n",
            "31/168, train_loss: 0.5493\n",
            "32/168, train_loss: 0.5437\n",
            "33/168, train_loss: 0.7438\n",
            "34/168, train_loss: 0.4288\n",
            "35/168, train_loss: 0.4364\n",
            "36/168, train_loss: 0.4544\n",
            "37/168, train_loss: 1.1376\n",
            "38/168, train_loss: 0.4049\n",
            "39/168, train_loss: 1.0281\n",
            "40/168, train_loss: 0.3836\n",
            "41/168, train_loss: 0.6639\n",
            "42/168, train_loss: 0.4844\n",
            "43/168, train_loss: 0.4511\n",
            "44/168, train_loss: 0.4581\n",
            "45/168, train_loss: 0.7282\n",
            "46/168, train_loss: 0.4276\n",
            "47/168, train_loss: 0.4381\n",
            "48/168, train_loss: 0.2988\n",
            "49/168, train_loss: 0.9995\n",
            "50/168, train_loss: 1.1295\n",
            "51/168, train_loss: 0.6629\n",
            "52/168, train_loss: 0.3217\n",
            "53/168, train_loss: 0.4458\n",
            "54/168, train_loss: 0.3443\n",
            "55/168, train_loss: 0.7225\n",
            "56/168, train_loss: 0.2721\n",
            "57/168, train_loss: 0.4303\n",
            "58/168, train_loss: 0.4581\n",
            "59/168, train_loss: 0.6655\n",
            "60/168, train_loss: 0.4936\n",
            "61/168, train_loss: 1.0312\n",
            "62/168, train_loss: 0.6256\n",
            "63/168, train_loss: 0.8897\n",
            "64/168, train_loss: 0.2723\n",
            "65/168, train_loss: 0.3982\n",
            "66/168, train_loss: 0.8702\n",
            "67/168, train_loss: 0.3264\n",
            "68/168, train_loss: 0.9724\n",
            "69/168, train_loss: 0.4227\n",
            "70/168, train_loss: 0.4561\n",
            "71/168, train_loss: 0.7146\n",
            "72/168, train_loss: 0.7132\n",
            "73/168, train_loss: 0.5311\n",
            "74/168, train_loss: 0.4564\n",
            "75/168, train_loss: 0.4721\n",
            "76/168, train_loss: 1.0553\n",
            "77/168, train_loss: 0.5288\n",
            "78/168, train_loss: 0.1757\n",
            "79/168, train_loss: 1.0181\n",
            "80/168, train_loss: 0.4534\n",
            "81/168, train_loss: 0.8227\n",
            "82/168, train_loss: 0.4855\n",
            "83/168, train_loss: 0.4294\n",
            "84/168, train_loss: 0.4471\n",
            "85/168, train_loss: 1.0569\n",
            "86/168, train_loss: 0.4769\n",
            "87/168, train_loss: 0.5015\n",
            "88/168, train_loss: 0.7368\n",
            "89/168, train_loss: 1.0266\n",
            "90/168, train_loss: 0.4171\n",
            "91/168, train_loss: 0.4990\n",
            "92/168, train_loss: 0.7331\n",
            "93/168, train_loss: 0.9731\n",
            "94/168, train_loss: 0.9146\n",
            "95/168, train_loss: 0.5716\n",
            "96/168, train_loss: 0.8252\n",
            "97/168, train_loss: 0.7130\n",
            "98/168, train_loss: 0.6527\n",
            "99/168, train_loss: 0.7106\n",
            "100/168, train_loss: 0.4324\n",
            "101/168, train_loss: 0.4695\n",
            "102/168, train_loss: 0.5876\n",
            "103/168, train_loss: 0.4871\n",
            "104/168, train_loss: 0.6339\n",
            "105/168, train_loss: 0.5379\n",
            "106/168, train_loss: 0.8166\n",
            "107/168, train_loss: 0.5653\n",
            "108/168, train_loss: 0.4778\n",
            "109/168, train_loss: 0.4953\n",
            "110/168, train_loss: 0.4415\n",
            "111/168, train_loss: 0.6786\n",
            "112/168, train_loss: 0.7300\n",
            "113/168, train_loss: 1.0053\n",
            "114/168, train_loss: 1.1469\n",
            "115/168, train_loss: 0.4534\n",
            "116/168, train_loss: 0.7227\n",
            "117/168, train_loss: 0.6356\n",
            "118/168, train_loss: 0.8694\n",
            "119/168, train_loss: 0.4293\n",
            "120/168, train_loss: 0.4626\n",
            "121/168, train_loss: 0.6805\n",
            "122/168, train_loss: 0.4253\n",
            "123/168, train_loss: 0.7214\n",
            "124/168, train_loss: 0.3398\n",
            "125/168, train_loss: 0.5459\n",
            "126/168, train_loss: 1.0325\n",
            "127/168, train_loss: 0.4151\n",
            "128/168, train_loss: 0.9689\n",
            "129/168, train_loss: 0.4411\n",
            "130/168, train_loss: 0.4587\n",
            "131/168, train_loss: 0.8475\n",
            "132/168, train_loss: 0.7160\n",
            "133/168, train_loss: 0.9217\n",
            "134/168, train_loss: 0.4730\n",
            "135/168, train_loss: 0.5326\n",
            "136/168, train_loss: 0.4610\n",
            "137/168, train_loss: 0.4740\n",
            "138/168, train_loss: 0.2793\n",
            "139/168, train_loss: 0.4630\n",
            "140/168, train_loss: 1.0153\n",
            "141/168, train_loss: 0.4192\n",
            "142/168, train_loss: 0.4732\n",
            "143/168, train_loss: 0.8973\n",
            "144/168, train_loss: 1.0950\n",
            "145/168, train_loss: 0.2685\n",
            "146/168, train_loss: 0.5125\n",
            "147/168, train_loss: 0.7184\n",
            "148/168, train_loss: 0.4354\n",
            "149/168, train_loss: 0.4992\n",
            "150/168, train_loss: 0.4427\n",
            "151/168, train_loss: 0.7403\n",
            "152/168, train_loss: 0.4313\n",
            "153/168, train_loss: 0.3628\n",
            "154/168, train_loss: 0.4714\n",
            "155/168, train_loss: 1.0159\n",
            "156/168, train_loss: 0.4388\n",
            "157/168, train_loss: 0.4470\n",
            "158/168, train_loss: 0.6146\n",
            "159/168, train_loss: 0.9609\n",
            "160/168, train_loss: 0.4364\n",
            "161/168, train_loss: 0.4372\n",
            "162/168, train_loss: 0.4316\n",
            "163/168, train_loss: 0.5563\n",
            "164/168, train_loss: 0.4231\n",
            "165/168, train_loss: 0.4254\n",
            "166/168, train_loss: 0.9819\n",
            "167/168, train_loss: 0.4579\n",
            "168/168, train_loss: 0.6492\n",
            "169/168, train_loss: 0.4194\n",
            "epoch 14 average loss: 0.5946\n",
            "current epoch: 14 current accuracy: 0.4706 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 15/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.7219\n",
            "2/168, train_loss: 0.4370\n",
            "3/168, train_loss: 0.9953\n",
            "4/168, train_loss: 0.4072\n",
            "5/168, train_loss: 0.4709\n",
            "6/168, train_loss: 1.0678\n",
            "7/168, train_loss: 0.4341\n",
            "8/168, train_loss: 0.4622\n",
            "9/168, train_loss: 0.5446\n",
            "10/168, train_loss: 0.6870\n",
            "11/168, train_loss: 0.5641\n",
            "12/168, train_loss: 0.3720\n",
            "13/168, train_loss: 0.4232\n",
            "14/168, train_loss: 0.3819\n",
            "15/168, train_loss: 0.5205\n",
            "16/168, train_loss: 0.3060\n",
            "17/168, train_loss: 0.2853\n",
            "18/168, train_loss: 0.7477\n",
            "19/168, train_loss: 0.4670\n",
            "20/168, train_loss: 0.4435\n",
            "21/168, train_loss: 0.4111\n",
            "22/168, train_loss: 0.2441\n",
            "23/168, train_loss: 0.4754\n",
            "24/168, train_loss: 0.2406\n",
            "25/168, train_loss: 0.2213\n",
            "26/168, train_loss: 0.7359\n",
            "27/168, train_loss: 0.3942\n",
            "28/168, train_loss: 0.4203\n",
            "29/168, train_loss: 1.0053\n",
            "30/168, train_loss: 0.4343\n",
            "31/168, train_loss: 0.4535\n",
            "32/168, train_loss: 0.8541\n",
            "33/168, train_loss: 1.0056\n",
            "34/168, train_loss: 0.4081\n",
            "35/168, train_loss: 0.3610\n",
            "36/168, train_loss: 0.3000\n",
            "37/168, train_loss: 0.4523\n",
            "38/168, train_loss: 0.4600\n",
            "39/168, train_loss: 0.3939\n",
            "40/168, train_loss: 0.5569\n",
            "41/168, train_loss: 0.4723\n",
            "42/168, train_loss: 0.6811\n",
            "43/168, train_loss: 1.1029\n",
            "44/168, train_loss: 0.4674\n",
            "45/168, train_loss: 0.3656\n",
            "46/168, train_loss: 0.4818\n",
            "47/168, train_loss: 0.7347\n",
            "48/168, train_loss: 0.4818\n",
            "49/168, train_loss: 0.7318\n",
            "50/168, train_loss: 0.4905\n",
            "51/168, train_loss: 0.8407\n",
            "52/168, train_loss: 0.3548\n",
            "53/168, train_loss: 0.2665\n",
            "54/168, train_loss: 0.5755\n",
            "55/168, train_loss: 1.0505\n",
            "56/168, train_loss: 0.5562\n",
            "57/168, train_loss: 0.7270\n",
            "58/168, train_loss: 0.4237\n",
            "59/168, train_loss: 0.6352\n",
            "60/168, train_loss: 0.4775\n",
            "61/168, train_loss: 0.4921\n",
            "62/168, train_loss: 0.5361\n",
            "63/168, train_loss: 0.9971\n",
            "64/168, train_loss: 0.2942\n",
            "65/168, train_loss: 0.5363\n",
            "66/168, train_loss: 0.7204\n",
            "67/168, train_loss: 0.4441\n",
            "68/168, train_loss: 0.4899\n",
            "69/168, train_loss: 0.4801\n",
            "70/168, train_loss: 0.4641\n",
            "71/168, train_loss: 0.4909\n",
            "72/168, train_loss: 0.4930\n",
            "73/168, train_loss: 0.3667\n",
            "74/168, train_loss: 0.7479\n",
            "75/168, train_loss: 0.3391\n",
            "76/168, train_loss: 0.4815\n",
            "77/168, train_loss: 1.0968\n",
            "78/168, train_loss: 0.5326\n",
            "79/168, train_loss: 0.4381\n",
            "80/168, train_loss: 0.4546\n",
            "81/168, train_loss: 0.9721\n",
            "82/168, train_loss: 1.1189\n",
            "83/168, train_loss: 0.3894\n",
            "84/168, train_loss: 0.4593\n",
            "85/168, train_loss: 0.7209\n",
            "86/168, train_loss: 0.4430\n",
            "87/168, train_loss: 0.4280\n",
            "88/168, train_loss: 0.4950\n",
            "89/168, train_loss: 0.4377\n",
            "90/168, train_loss: 0.4808\n",
            "91/168, train_loss: 0.5864\n",
            "92/168, train_loss: 0.4521\n",
            "93/168, train_loss: 0.1884\n",
            "94/168, train_loss: 0.4124\n",
            "95/168, train_loss: 0.4222\n",
            "96/168, train_loss: 0.8193\n",
            "97/168, train_loss: 0.4077\n",
            "98/168, train_loss: 0.4566\n",
            "99/168, train_loss: 0.4278\n",
            "100/168, train_loss: 0.4731\n",
            "101/168, train_loss: 0.7189\n",
            "102/168, train_loss: 0.9390\n",
            "103/168, train_loss: 0.5550\n",
            "104/168, train_loss: 0.7154\n",
            "105/168, train_loss: 0.3500\n",
            "106/168, train_loss: 0.3191\n",
            "107/168, train_loss: 0.4883\n",
            "108/168, train_loss: 1.1107\n",
            "109/168, train_loss: 0.7360\n",
            "110/168, train_loss: 0.4385\n",
            "111/168, train_loss: 0.7841\n",
            "112/168, train_loss: 1.0182\n",
            "113/168, train_loss: 0.6082\n",
            "114/168, train_loss: 0.7749\n",
            "115/168, train_loss: 0.4234\n",
            "116/168, train_loss: 0.4664\n",
            "117/168, train_loss: 0.4172\n",
            "118/168, train_loss: 0.4084\n",
            "119/168, train_loss: 1.1013\n",
            "120/168, train_loss: 0.6034\n",
            "121/168, train_loss: 0.4798\n",
            "122/168, train_loss: 1.0324\n",
            "123/168, train_loss: 0.3829\n",
            "124/168, train_loss: 0.4158\n",
            "125/168, train_loss: 0.3922\n",
            "126/168, train_loss: 0.5383\n",
            "127/168, train_loss: 0.5412\n",
            "128/168, train_loss: 0.4142\n",
            "129/168, train_loss: 0.5037\n",
            "130/168, train_loss: 1.0464\n",
            "131/168, train_loss: 0.8787\n",
            "132/168, train_loss: 0.4304\n",
            "133/168, train_loss: 0.6745\n",
            "134/168, train_loss: 0.6779\n",
            "135/168, train_loss: 0.3705\n",
            "136/168, train_loss: 0.7795\n",
            "137/168, train_loss: 0.9627\n",
            "138/168, train_loss: 0.4349\n",
            "139/168, train_loss: 1.1995\n",
            "140/168, train_loss: 0.6377\n",
            "141/168, train_loss: 0.5351\n",
            "142/168, train_loss: 0.6683\n",
            "143/168, train_loss: 0.3650\n",
            "144/168, train_loss: 0.7675\n",
            "145/168, train_loss: 0.8153\n",
            "146/168, train_loss: 0.2052\n",
            "147/168, train_loss: 0.4124\n",
            "148/168, train_loss: 0.4270\n",
            "149/168, train_loss: 1.1188\n",
            "150/168, train_loss: 0.4883\n",
            "151/168, train_loss: 0.4269\n",
            "152/168, train_loss: 0.1718\n",
            "153/168, train_loss: 0.4603\n",
            "154/168, train_loss: 1.0509\n",
            "155/168, train_loss: 0.4472\n",
            "156/168, train_loss: 0.7814\n",
            "157/168, train_loss: 0.5641\n",
            "158/168, train_loss: 0.4282\n",
            "159/168, train_loss: 0.4108\n",
            "160/168, train_loss: 0.4379\n",
            "161/168, train_loss: 0.7633\n",
            "162/168, train_loss: 1.1224\n",
            "163/168, train_loss: 0.5676\n",
            "164/168, train_loss: 0.4217\n",
            "165/168, train_loss: 0.7175\n",
            "166/168, train_loss: 0.7930\n",
            "167/168, train_loss: 1.0100\n",
            "168/168, train_loss: 0.4392\n",
            "169/168, train_loss: 0.4273\n",
            "epoch 15 average loss: 0.5739\n",
            "----------\n",
            "epoch 16/5\n",
            "1/168, train_loss: 0.5169\n",
            "2/168, train_loss: 0.5526\n",
            "3/168, train_loss: 1.2551\n",
            "4/168, train_loss: 0.5011\n",
            "5/168, train_loss: 0.7517\n",
            "6/168, train_loss: 0.3951\n",
            "7/168, train_loss: 0.6609\n",
            "8/168, train_loss: 0.4985\n",
            "9/168, train_loss: 0.4298\n",
            "10/168, train_loss: 0.4235\n",
            "11/168, train_loss: 0.6868\n",
            "12/168, train_loss: 0.7431\n",
            "13/168, train_loss: 1.0503\n",
            "14/168, train_loss: 0.4249\n",
            "15/168, train_loss: 1.0270\n",
            "16/168, train_loss: 0.4381\n",
            "17/168, train_loss: 0.4482\n",
            "18/168, train_loss: 0.3208\n",
            "19/168, train_loss: 0.9508\n",
            "20/168, train_loss: 0.5164\n",
            "21/168, train_loss: 0.4259\n",
            "22/168, train_loss: 0.5763\n",
            "23/168, train_loss: 0.5070\n",
            "24/168, train_loss: 0.5646\n",
            "25/168, train_loss: 0.3767\n",
            "26/168, train_loss: 0.4654\n",
            "27/168, train_loss: 0.7583\n",
            "28/168, train_loss: 0.4346\n",
            "29/168, train_loss: 0.9913\n",
            "30/168, train_loss: 0.4260\n",
            "31/168, train_loss: 0.4599\n",
            "32/168, train_loss: 0.6520\n",
            "33/168, train_loss: 0.4347\n",
            "34/168, train_loss: 1.0489\n",
            "35/168, train_loss: 0.5031\n",
            "36/168, train_loss: 0.4836\n",
            "37/168, train_loss: 1.0161\n",
            "38/168, train_loss: 1.0967\n",
            "39/168, train_loss: 0.4244\n",
            "40/168, train_loss: 0.4271\n",
            "41/168, train_loss: 1.0130\n",
            "42/168, train_loss: 0.4824\n",
            "43/168, train_loss: 0.4151\n",
            "44/168, train_loss: 0.3582\n",
            "45/168, train_loss: 1.0031\n",
            "46/168, train_loss: 0.4658\n",
            "47/168, train_loss: 1.0076\n",
            "48/168, train_loss: 0.7928\n",
            "49/168, train_loss: 0.4081\n",
            "50/168, train_loss: 0.4683\n",
            "51/168, train_loss: 0.4684\n",
            "52/168, train_loss: 0.7306\n",
            "53/168, train_loss: 0.4427\n",
            "54/168, train_loss: 0.4632\n",
            "55/168, train_loss: 0.4244\n",
            "56/168, train_loss: 0.6694\n",
            "57/168, train_loss: 0.2410\n",
            "58/168, train_loss: 0.4578\n",
            "59/168, train_loss: 1.1590\n",
            "60/168, train_loss: 0.4406\n",
            "61/168, train_loss: 0.7392\n",
            "62/168, train_loss: 0.4646\n",
            "63/168, train_loss: 0.5315\n",
            "64/168, train_loss: 0.6656\n",
            "65/168, train_loss: 1.0064\n",
            "66/168, train_loss: 0.4682\n",
            "67/168, train_loss: 0.6368\n",
            "68/168, train_loss: 1.0207\n",
            "69/168, train_loss: 0.7272\n",
            "70/168, train_loss: 0.7561\n",
            "71/168, train_loss: 0.3320\n",
            "72/168, train_loss: 0.4442\n",
            "73/168, train_loss: 0.4309\n",
            "74/168, train_loss: 0.4370\n",
            "75/168, train_loss: 0.2840\n",
            "76/168, train_loss: 0.4263\n",
            "77/168, train_loss: 0.4778\n",
            "78/168, train_loss: 0.4380\n",
            "79/168, train_loss: 0.4571\n",
            "80/168, train_loss: 0.4413\n",
            "81/168, train_loss: 0.4670\n",
            "82/168, train_loss: 0.7202\n",
            "83/168, train_loss: 0.9063\n",
            "84/168, train_loss: 0.4693\n",
            "85/168, train_loss: 0.2958\n",
            "86/168, train_loss: 0.5268\n",
            "87/168, train_loss: 0.4367\n",
            "88/168, train_loss: 0.4260\n",
            "89/168, train_loss: 0.7165\n",
            "90/168, train_loss: 0.9275\n",
            "91/168, train_loss: 0.4764\n",
            "92/168, train_loss: 0.3551\n",
            "93/168, train_loss: 0.4457\n",
            "94/168, train_loss: 0.4493\n",
            "95/168, train_loss: 0.1775\n",
            "96/168, train_loss: 0.1601\n",
            "97/168, train_loss: 0.7896\n",
            "98/168, train_loss: 0.5111\n",
            "99/168, train_loss: 0.3898\n",
            "100/168, train_loss: 0.7256\n",
            "101/168, train_loss: 0.4890\n",
            "102/168, train_loss: 1.0218\n",
            "103/168, train_loss: 0.4554\n",
            "104/168, train_loss: 0.9517\n",
            "105/168, train_loss: 0.6880\n",
            "106/168, train_loss: 0.4232\n",
            "107/168, train_loss: 0.4787\n",
            "108/168, train_loss: 0.4233\n",
            "109/168, train_loss: 1.0180\n",
            "110/168, train_loss: 0.6116\n",
            "111/168, train_loss: 0.4184\n",
            "112/168, train_loss: 0.5760\n",
            "113/168, train_loss: 0.4335\n",
            "114/168, train_loss: 0.4357\n",
            "115/168, train_loss: 1.1169\n",
            "116/168, train_loss: 0.3792\n",
            "117/168, train_loss: 0.6390\n",
            "118/168, train_loss: 1.0539\n",
            "119/168, train_loss: 0.4137\n",
            "120/168, train_loss: 0.4588\n",
            "121/168, train_loss: 0.2437\n",
            "122/168, train_loss: 0.4119\n",
            "123/168, train_loss: 0.4453\n",
            "124/168, train_loss: 0.4957\n",
            "125/168, train_loss: 0.5133\n",
            "126/168, train_loss: 1.0344\n",
            "127/168, train_loss: 0.4105\n",
            "128/168, train_loss: 0.7471\n",
            "129/168, train_loss: 1.0631\n",
            "130/168, train_loss: 0.6776\n",
            "131/168, train_loss: 0.3213\n",
            "132/168, train_loss: 0.4462\n",
            "133/168, train_loss: 0.4517\n",
            "134/168, train_loss: 0.5272\n",
            "135/168, train_loss: 0.2676\n",
            "136/168, train_loss: 0.1590\n",
            "137/168, train_loss: 0.4303\n",
            "138/168, train_loss: 1.0718\n",
            "139/168, train_loss: 0.5021\n",
            "140/168, train_loss: 0.7907\n",
            "141/168, train_loss: 1.0015\n",
            "142/168, train_loss: 0.3521\n",
            "143/168, train_loss: 0.4396\n",
            "144/168, train_loss: 0.4499\n",
            "145/168, train_loss: 0.4313\n",
            "146/168, train_loss: 0.7380\n",
            "147/168, train_loss: 0.4170\n",
            "148/168, train_loss: 0.4291\n",
            "149/168, train_loss: 1.0400\n",
            "150/168, train_loss: 0.3239\n",
            "151/168, train_loss: 0.4269\n",
            "152/168, train_loss: 0.3977\n",
            "153/168, train_loss: 0.8623\n",
            "154/168, train_loss: 1.0372\n",
            "155/168, train_loss: 0.4136\n",
            "156/168, train_loss: 0.2113\n",
            "157/168, train_loss: 0.8109\n",
            "158/168, train_loss: 0.4684\n",
            "159/168, train_loss: 1.1189\n",
            "160/168, train_loss: 0.8306\n",
            "161/168, train_loss: 0.4037\n",
            "162/168, train_loss: 0.5964\n",
            "163/168, train_loss: 0.4418\n",
            "164/168, train_loss: 0.9793\n",
            "165/168, train_loss: 0.4045\n",
            "166/168, train_loss: 1.0693\n",
            "167/168, train_loss: 0.4627\n",
            "168/168, train_loss: 0.4386\n",
            "169/168, train_loss: 0.4435\n",
            "epoch 16 average loss: 0.5862\n",
            "current epoch: 16 current accuracy: 0.4000 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 17/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.5023\n",
            "2/168, train_loss: 0.4177\n",
            "3/168, train_loss: 0.4462\n",
            "4/168, train_loss: 0.5682\n",
            "5/168, train_loss: 1.0912\n",
            "6/168, train_loss: 0.4977\n",
            "7/168, train_loss: 0.4677\n",
            "8/168, train_loss: 0.5718\n",
            "9/168, train_loss: 0.4243\n",
            "10/168, train_loss: 0.4293\n",
            "11/168, train_loss: 1.0333\n",
            "12/168, train_loss: 0.3914\n",
            "13/168, train_loss: 0.5781\n",
            "14/168, train_loss: 0.4777\n",
            "15/168, train_loss: 0.5080\n",
            "16/168, train_loss: 0.3253\n",
            "17/168, train_loss: 0.4217\n",
            "18/168, train_loss: 0.7502\n",
            "19/168, train_loss: 0.3976\n",
            "20/168, train_loss: 0.3608\n",
            "21/168, train_loss: 0.9833\n",
            "22/168, train_loss: 0.1656\n",
            "23/168, train_loss: 0.3818\n",
            "24/168, train_loss: 0.4141\n",
            "25/168, train_loss: 0.2081\n",
            "26/168, train_loss: 0.9185\n",
            "27/168, train_loss: 0.4090\n",
            "28/168, train_loss: 0.4219\n",
            "29/168, train_loss: 0.8725\n",
            "30/168, train_loss: 0.4175\n",
            "31/168, train_loss: 1.0083\n",
            "32/168, train_loss: 0.6358\n",
            "33/168, train_loss: 0.7390\n",
            "34/168, train_loss: 1.1001\n",
            "35/168, train_loss: 0.4644\n",
            "36/168, train_loss: 0.9881\n",
            "37/168, train_loss: 1.0289\n",
            "38/168, train_loss: 0.4622\n",
            "39/168, train_loss: 0.5252\n",
            "40/168, train_loss: 0.4637\n",
            "41/168, train_loss: 1.0127\n",
            "42/168, train_loss: 0.4285\n",
            "43/168, train_loss: 0.4298\n",
            "44/168, train_loss: 0.9935\n",
            "45/168, train_loss: 0.4472\n",
            "46/168, train_loss: 0.5473\n",
            "47/168, train_loss: 0.4374\n",
            "48/168, train_loss: 1.0083\n",
            "49/168, train_loss: 0.4692\n",
            "50/168, train_loss: 0.3879\n",
            "51/168, train_loss: 0.4305\n",
            "52/168, train_loss: 0.4932\n",
            "53/168, train_loss: 0.3105\n",
            "54/168, train_loss: 1.0558\n",
            "55/168, train_loss: 0.7196\n",
            "56/168, train_loss: 0.3636\n",
            "57/168, train_loss: 0.4446\n",
            "58/168, train_loss: 0.6359\n",
            "59/168, train_loss: 0.4909\n",
            "60/168, train_loss: 0.8440\n",
            "61/168, train_loss: 0.4548\n",
            "62/168, train_loss: 0.4762\n",
            "63/168, train_loss: 0.5411\n",
            "64/168, train_loss: 0.2855\n",
            "65/168, train_loss: 0.4382\n",
            "66/168, train_loss: 0.5313\n",
            "67/168, train_loss: 0.4732\n",
            "68/168, train_loss: 0.5019\n",
            "69/168, train_loss: 0.6630\n",
            "70/168, train_loss: 0.5498\n",
            "71/168, train_loss: 0.5570\n",
            "72/168, train_loss: 0.4303\n",
            "73/168, train_loss: 0.6120\n",
            "74/168, train_loss: 0.7840\n",
            "75/168, train_loss: 0.4476\n",
            "76/168, train_loss: 0.7883\n",
            "77/168, train_loss: 0.7893\n",
            "78/168, train_loss: 0.4446\n",
            "79/168, train_loss: 0.4493\n",
            "80/168, train_loss: 0.2580\n",
            "81/168, train_loss: 0.4463\n",
            "82/168, train_loss: 0.2795\n",
            "83/168, train_loss: 0.4482\n",
            "84/168, train_loss: 0.5273\n",
            "85/168, train_loss: 0.1150\n",
            "86/168, train_loss: 0.4525\n",
            "87/168, train_loss: 0.6646\n",
            "88/168, train_loss: 0.4432\n",
            "89/168, train_loss: 0.7741\n",
            "90/168, train_loss: 0.6034\n",
            "91/168, train_loss: 0.4446\n",
            "92/168, train_loss: 0.4318\n",
            "93/168, train_loss: 0.4832\n",
            "94/168, train_loss: 0.4943\n",
            "95/168, train_loss: 0.7499\n",
            "96/168, train_loss: 0.5351\n",
            "97/168, train_loss: 0.7458\n",
            "98/168, train_loss: 0.9621\n",
            "99/168, train_loss: 0.4279\n",
            "100/168, train_loss: 1.0685\n",
            "101/168, train_loss: 0.4047\n",
            "102/168, train_loss: 0.7757\n",
            "103/168, train_loss: 0.5368\n",
            "104/168, train_loss: 0.5603\n",
            "105/168, train_loss: 0.4133\n",
            "106/168, train_loss: 0.6921\n",
            "107/168, train_loss: 0.6551\n",
            "108/168, train_loss: 0.9498\n",
            "109/168, train_loss: 0.4264\n",
            "110/168, train_loss: 0.6868\n",
            "111/168, train_loss: 0.4146\n",
            "112/168, train_loss: 1.0748\n",
            "113/168, train_loss: 0.4163\n",
            "114/168, train_loss: 0.4950\n",
            "115/168, train_loss: 0.6147\n",
            "116/168, train_loss: 0.8163\n",
            "117/168, train_loss: 0.6201\n",
            "118/168, train_loss: 0.5039\n",
            "119/168, train_loss: 0.8032\n",
            "120/168, train_loss: 0.4457\n",
            "121/168, train_loss: 0.4039\n",
            "122/168, train_loss: 0.4266\n",
            "123/168, train_loss: 0.7419\n",
            "124/168, train_loss: 0.4583\n",
            "125/168, train_loss: 0.4284\n",
            "126/168, train_loss: 0.5303\n",
            "127/168, train_loss: 0.4630\n",
            "128/168, train_loss: 0.4048\n",
            "129/168, train_loss: 1.0535\n",
            "130/168, train_loss: 1.0207\n",
            "131/168, train_loss: 0.4168\n",
            "132/168, train_loss: 0.7299\n",
            "133/168, train_loss: 0.4189\n",
            "134/168, train_loss: 0.3882\n",
            "135/168, train_loss: 0.6086\n",
            "136/168, train_loss: 0.5139\n",
            "137/168, train_loss: 1.0540\n",
            "138/168, train_loss: 0.4367\n",
            "139/168, train_loss: 0.5034\n",
            "140/168, train_loss: 0.2758\n",
            "141/168, train_loss: 0.4100\n",
            "142/168, train_loss: 0.4245\n",
            "143/168, train_loss: 0.9904\n",
            "144/168, train_loss: 0.4713\n",
            "145/168, train_loss: 0.6091\n",
            "146/168, train_loss: 0.4779\n",
            "147/168, train_loss: 0.9987\n",
            "148/168, train_loss: 0.4464\n",
            "149/168, train_loss: 0.3390\n",
            "150/168, train_loss: 0.4809\n",
            "151/168, train_loss: 0.5163\n",
            "152/168, train_loss: 0.6031\n",
            "153/168, train_loss: 0.5807\n",
            "154/168, train_loss: 0.3912\n",
            "155/168, train_loss: 0.4908\n",
            "156/168, train_loss: 0.4839\n",
            "157/168, train_loss: 1.0269\n",
            "158/168, train_loss: 0.5363\n",
            "159/168, train_loss: 0.9938\n",
            "160/168, train_loss: 0.4559\n",
            "161/168, train_loss: 1.1098\n",
            "162/168, train_loss: 0.9550\n",
            "163/168, train_loss: 0.4826\n",
            "164/168, train_loss: 0.2436\n",
            "165/168, train_loss: 1.2190\n",
            "166/168, train_loss: 0.4354\n",
            "167/168, train_loss: 0.4668\n",
            "168/168, train_loss: 0.5918\n",
            "169/168, train_loss: 0.3972\n",
            "epoch 17 average loss: 0.5799\n",
            "----------\n",
            "epoch 18/5\n",
            "1/168, train_loss: 0.4311\n",
            "2/168, train_loss: 0.4859\n",
            "3/168, train_loss: 0.7380\n",
            "4/168, train_loss: 0.7362\n",
            "5/168, train_loss: 0.5415\n",
            "6/168, train_loss: 0.4089\n",
            "7/168, train_loss: 0.4034\n",
            "8/168, train_loss: 0.7314\n",
            "9/168, train_loss: 0.7294\n",
            "10/168, train_loss: 0.4467\n",
            "11/168, train_loss: 0.4256\n",
            "12/168, train_loss: 0.3388\n",
            "13/168, train_loss: 0.7227\n",
            "14/168, train_loss: 1.0940\n",
            "15/168, train_loss: 0.6545\n",
            "16/168, train_loss: 0.4551\n",
            "17/168, train_loss: 0.4878\n",
            "18/168, train_loss: 0.4163\n",
            "19/168, train_loss: 0.6888\n",
            "20/168, train_loss: 0.4920\n",
            "21/168, train_loss: 0.9271\n",
            "22/168, train_loss: 0.7114\n",
            "23/168, train_loss: 0.4466\n",
            "24/168, train_loss: 0.4878\n",
            "25/168, train_loss: 0.4263\n",
            "26/168, train_loss: 0.4314\n",
            "27/168, train_loss: 0.5373\n",
            "28/168, train_loss: 0.8657\n",
            "29/168, train_loss: 0.4681\n",
            "30/168, train_loss: 0.4991\n",
            "31/168, train_loss: 0.5175\n",
            "32/168, train_loss: 0.5018\n",
            "33/168, train_loss: 0.4361\n",
            "34/168, train_loss: 0.3715\n",
            "35/168, train_loss: 0.5018\n",
            "36/168, train_loss: 0.4502\n",
            "37/168, train_loss: 0.3353\n",
            "38/168, train_loss: 0.4194\n",
            "39/168, train_loss: 1.0418\n",
            "40/168, train_loss: 0.4385\n",
            "41/168, train_loss: 0.4677\n",
            "42/168, train_loss: 0.5370\n",
            "43/168, train_loss: 0.5134\n",
            "44/168, train_loss: 1.0952\n",
            "45/168, train_loss: 0.7620\n",
            "46/168, train_loss: 0.7374\n",
            "47/168, train_loss: 0.4189\n",
            "48/168, train_loss: 0.4429\n",
            "49/168, train_loss: 0.4056\n",
            "50/168, train_loss: 0.5920\n",
            "51/168, train_loss: 0.3081\n",
            "52/168, train_loss: 1.1146\n",
            "53/168, train_loss: 0.3923\n",
            "54/168, train_loss: 0.5964\n",
            "55/168, train_loss: 0.5516\n",
            "56/168, train_loss: 0.4139\n",
            "57/168, train_loss: 1.0772\n",
            "58/168, train_loss: 0.1202\n",
            "59/168, train_loss: 0.7680\n",
            "60/168, train_loss: 0.4376\n",
            "61/168, train_loss: 0.5211\n",
            "62/168, train_loss: 0.3956\n",
            "63/168, train_loss: 0.6252\n",
            "64/168, train_loss: 0.9496\n",
            "65/168, train_loss: 0.4040\n",
            "66/168, train_loss: 0.2425\n",
            "67/168, train_loss: 0.4192\n",
            "68/168, train_loss: 0.6021\n",
            "69/168, train_loss: 0.4928\n",
            "70/168, train_loss: 0.4392\n",
            "71/168, train_loss: 0.4308\n",
            "72/168, train_loss: 0.4132\n",
            "73/168, train_loss: 0.4233\n",
            "74/168, train_loss: 0.7458\n",
            "75/168, train_loss: 1.0817\n",
            "76/168, train_loss: 0.4285\n",
            "77/168, train_loss: 0.6611\n",
            "78/168, train_loss: 0.6036\n",
            "79/168, train_loss: 1.1354\n",
            "80/168, train_loss: 1.1582\n",
            "81/168, train_loss: 1.0639\n",
            "82/168, train_loss: 0.6297\n",
            "83/168, train_loss: 0.3601\n",
            "84/168, train_loss: 1.0454\n",
            "85/168, train_loss: 1.0229\n",
            "86/168, train_loss: 0.4195\n",
            "87/168, train_loss: 0.2951\n",
            "88/168, train_loss: 0.7907\n",
            "89/168, train_loss: 0.6323\n",
            "90/168, train_loss: 0.7543\n",
            "91/168, train_loss: 0.4434\n",
            "92/168, train_loss: 0.4127\n",
            "93/168, train_loss: 0.8397\n",
            "94/168, train_loss: 0.5014\n",
            "95/168, train_loss: 0.5136\n",
            "96/168, train_loss: 1.0025\n",
            "97/168, train_loss: 0.4716\n",
            "98/168, train_loss: 0.4877\n",
            "99/168, train_loss: 0.4891\n",
            "100/168, train_loss: 0.8416\n",
            "101/168, train_loss: 0.7615\n",
            "102/168, train_loss: 0.7583\n",
            "103/168, train_loss: 0.3323\n",
            "104/168, train_loss: 0.6362\n",
            "105/168, train_loss: 0.4150\n",
            "106/168, train_loss: 0.4081\n",
            "107/168, train_loss: 0.4432\n",
            "108/168, train_loss: 0.6104\n",
            "109/168, train_loss: 0.7585\n",
            "110/168, train_loss: 0.4843\n",
            "111/168, train_loss: 0.7800\n",
            "112/168, train_loss: 0.3411\n",
            "113/168, train_loss: 0.4801\n",
            "114/168, train_loss: 0.4166\n",
            "115/168, train_loss: 1.1199\n",
            "116/168, train_loss: 1.0157\n",
            "117/168, train_loss: 0.3989\n",
            "118/168, train_loss: 0.6887\n",
            "119/168, train_loss: 0.7014\n",
            "120/168, train_loss: 0.3785\n",
            "121/168, train_loss: 0.4560\n",
            "122/168, train_loss: 0.4243\n",
            "123/168, train_loss: 1.0444\n",
            "124/168, train_loss: 0.7812\n",
            "125/168, train_loss: 0.4891\n",
            "126/168, train_loss: 1.0286\n",
            "127/168, train_loss: 0.5872\n",
            "128/168, train_loss: 0.2607\n",
            "129/168, train_loss: 0.6798\n",
            "130/168, train_loss: 0.4413\n",
            "131/168, train_loss: 0.3452\n",
            "132/168, train_loss: 0.4343\n",
            "133/168, train_loss: 0.7548\n",
            "134/168, train_loss: 0.7810\n",
            "135/168, train_loss: 1.0837\n",
            "136/168, train_loss: 0.4266\n",
            "137/168, train_loss: 0.4814\n",
            "138/168, train_loss: 0.7359\n",
            "139/168, train_loss: 0.4435\n",
            "140/168, train_loss: 0.5108\n",
            "141/168, train_loss: 0.6731\n",
            "142/168, train_loss: 0.4554\n",
            "143/168, train_loss: 0.4083\n",
            "144/168, train_loss: 0.4305\n",
            "145/168, train_loss: 0.5199\n",
            "146/168, train_loss: 0.2519\n",
            "147/168, train_loss: 0.2994\n",
            "148/168, train_loss: 0.7510\n",
            "149/168, train_loss: 0.5405\n",
            "150/168, train_loss: 0.3110\n",
            "151/168, train_loss: 0.4455\n",
            "152/168, train_loss: 0.4451\n",
            "153/168, train_loss: 0.4219\n",
            "154/168, train_loss: 0.4985\n",
            "155/168, train_loss: 0.4050\n",
            "156/168, train_loss: 1.0628\n",
            "157/168, train_loss: 0.4195\n",
            "158/168, train_loss: 0.2738\n",
            "159/168, train_loss: 0.6647\n",
            "160/168, train_loss: 0.4315\n",
            "161/168, train_loss: 1.0309\n",
            "162/168, train_loss: 0.4549\n",
            "163/168, train_loss: 0.2858\n",
            "164/168, train_loss: 0.7841\n",
            "165/168, train_loss: 0.2373\n",
            "166/168, train_loss: 0.4273\n",
            "167/168, train_loss: 0.3594\n",
            "168/168, train_loss: 1.1364\n",
            "169/168, train_loss: 0.4073\n",
            "epoch 18 average loss: 0.5807\n",
            "current epoch: 18 current accuracy: 0.5176 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 19/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.4307\n",
            "2/168, train_loss: 0.6994\n",
            "3/168, train_loss: 0.6346\n",
            "4/168, train_loss: 0.4347\n",
            "5/168, train_loss: 0.4416\n",
            "6/168, train_loss: 0.2250\n",
            "7/168, train_loss: 0.4007\n",
            "8/168, train_loss: 0.4339\n",
            "9/168, train_loss: 0.4710\n",
            "10/168, train_loss: 0.4650\n",
            "11/168, train_loss: 0.5304\n",
            "12/168, train_loss: 0.6530\n",
            "13/168, train_loss: 1.0205\n",
            "14/168, train_loss: 0.5273\n",
            "15/168, train_loss: 0.4480\n",
            "16/168, train_loss: 0.4044\n",
            "17/168, train_loss: 0.3973\n",
            "18/168, train_loss: 0.5511\n",
            "19/168, train_loss: 0.5361\n",
            "20/168, train_loss: 0.5176\n",
            "21/168, train_loss: 0.7836\n",
            "22/168, train_loss: 0.4265\n",
            "23/168, train_loss: 0.4497\n",
            "24/168, train_loss: 0.3796\n",
            "25/168, train_loss: 0.4399\n",
            "26/168, train_loss: 0.4739\n",
            "27/168, train_loss: 0.2587\n",
            "28/168, train_loss: 0.4009\n",
            "29/168, train_loss: 0.4447\n",
            "30/168, train_loss: 0.4271\n",
            "31/168, train_loss: 0.1995\n",
            "32/168, train_loss: 0.7742\n",
            "33/168, train_loss: 0.3773\n",
            "34/168, train_loss: 0.4237\n",
            "35/168, train_loss: 0.5128\n",
            "36/168, train_loss: 1.0909\n",
            "37/168, train_loss: 0.3826\n",
            "38/168, train_loss: 0.4145\n",
            "39/168, train_loss: 0.3728\n",
            "40/168, train_loss: 0.4771\n",
            "41/168, train_loss: 0.3746\n",
            "42/168, train_loss: 0.7783\n",
            "43/168, train_loss: 1.0734\n",
            "44/168, train_loss: 0.2320\n",
            "45/168, train_loss: 0.4721\n",
            "46/168, train_loss: 0.2016\n",
            "47/168, train_loss: 0.8831\n",
            "48/168, train_loss: 0.4053\n",
            "49/168, train_loss: 0.3865\n",
            "50/168, train_loss: 0.3864\n",
            "51/168, train_loss: 0.4681\n",
            "52/168, train_loss: 1.0581\n",
            "53/168, train_loss: 0.4851\n",
            "54/168, train_loss: 0.8400\n",
            "55/168, train_loss: 0.5997\n",
            "56/168, train_loss: 0.3984\n",
            "57/168, train_loss: 0.4980\n",
            "58/168, train_loss: 0.4542\n",
            "59/168, train_loss: 0.5013\n",
            "60/168, train_loss: 0.2106\n",
            "61/168, train_loss: 0.3163\n",
            "62/168, train_loss: 1.0855\n",
            "63/168, train_loss: 0.4165\n",
            "64/168, train_loss: 0.4282\n",
            "65/168, train_loss: 0.4156\n",
            "66/168, train_loss: 0.3981\n",
            "67/168, train_loss: 0.6302\n",
            "68/168, train_loss: 1.0261\n",
            "69/168, train_loss: 0.3665\n",
            "70/168, train_loss: 1.2502\n",
            "71/168, train_loss: 0.4214\n",
            "72/168, train_loss: 1.0711\n",
            "73/168, train_loss: 0.4562\n",
            "74/168, train_loss: 0.4305\n",
            "75/168, train_loss: 0.7861\n",
            "76/168, train_loss: 0.7788\n",
            "77/168, train_loss: 0.2465\n",
            "78/168, train_loss: 1.0297\n",
            "79/168, train_loss: 1.0017\n",
            "80/168, train_loss: 0.7650\n",
            "81/168, train_loss: 0.3898\n",
            "82/168, train_loss: 0.3936\n",
            "83/168, train_loss: 0.3977\n",
            "84/168, train_loss: 0.7608\n",
            "85/168, train_loss: 1.0683\n",
            "86/168, train_loss: 0.3524\n",
            "87/168, train_loss: 0.3263\n",
            "88/168, train_loss: 1.0987\n",
            "89/168, train_loss: 0.7115\n",
            "90/168, train_loss: 1.3291\n",
            "91/168, train_loss: 0.7486\n",
            "92/168, train_loss: 0.7444\n",
            "93/168, train_loss: 0.4589\n",
            "94/168, train_loss: 0.4113\n",
            "95/168, train_loss: 0.4799\n",
            "96/168, train_loss: 1.0227\n",
            "97/168, train_loss: 0.4234\n",
            "98/168, train_loss: 0.8867\n",
            "99/168, train_loss: 0.2932\n",
            "100/168, train_loss: 0.5378\n",
            "101/168, train_loss: 0.9753\n",
            "102/168, train_loss: 0.7243\n",
            "103/168, train_loss: 0.4442\n",
            "104/168, train_loss: 0.4852\n",
            "105/168, train_loss: 0.5389\n",
            "106/168, train_loss: 0.7288\n",
            "107/168, train_loss: 0.5143\n",
            "108/168, train_loss: 0.2827\n",
            "109/168, train_loss: 0.7298\n",
            "110/168, train_loss: 0.4388\n",
            "111/168, train_loss: 0.4355\n",
            "112/168, train_loss: 0.4305\n",
            "113/168, train_loss: 0.4980\n",
            "114/168, train_loss: 0.4426\n",
            "115/168, train_loss: 0.8100\n",
            "116/168, train_loss: 0.2135\n",
            "117/168, train_loss: 0.4312\n",
            "118/168, train_loss: 0.4669\n",
            "119/168, train_loss: 0.1479\n",
            "120/168, train_loss: 0.4708\n",
            "121/168, train_loss: 0.4343\n",
            "122/168, train_loss: 1.4099\n",
            "123/168, train_loss: 0.7837\n",
            "124/168, train_loss: 0.4425\n",
            "125/168, train_loss: 0.4339\n",
            "126/168, train_loss: 0.4171\n",
            "127/168, train_loss: 0.4429\n",
            "128/168, train_loss: 0.6759\n",
            "129/168, train_loss: 1.3279\n",
            "130/168, train_loss: 0.4438\n",
            "131/168, train_loss: 0.4445\n",
            "132/168, train_loss: 0.4009\n",
            "133/168, train_loss: 0.1882\n",
            "134/168, train_loss: 0.4479\n",
            "135/168, train_loss: 1.0837\n",
            "136/168, train_loss: 0.4505\n",
            "137/168, train_loss: 0.4152\n",
            "138/168, train_loss: 0.7354\n",
            "139/168, train_loss: 0.7026\n",
            "140/168, train_loss: 1.1446\n",
            "141/168, train_loss: 0.7306\n",
            "142/168, train_loss: 0.3011\n",
            "143/168, train_loss: 0.5764\n",
            "144/168, train_loss: 0.4102\n",
            "145/168, train_loss: 0.3548\n",
            "146/168, train_loss: 0.4306\n",
            "147/168, train_loss: 0.7981\n",
            "148/168, train_loss: 0.9579\n",
            "149/168, train_loss: 0.5043\n",
            "150/168, train_loss: 0.4862\n",
            "151/168, train_loss: 0.5167\n",
            "152/168, train_loss: 0.6313\n",
            "153/168, train_loss: 0.9607\n",
            "154/168, train_loss: 0.3976\n",
            "155/168, train_loss: 0.4093\n",
            "156/168, train_loss: 1.1735\n",
            "157/168, train_loss: 0.5221\n",
            "158/168, train_loss: 0.5218\n",
            "159/168, train_loss: 0.9971\n",
            "160/168, train_loss: 1.0350\n",
            "161/168, train_loss: 0.4094\n",
            "162/168, train_loss: 0.5022\n",
            "163/168, train_loss: 0.4306\n",
            "164/168, train_loss: 0.3930\n",
            "165/168, train_loss: 0.2349\n",
            "166/168, train_loss: 0.4381\n",
            "167/168, train_loss: 0.4846\n",
            "168/168, train_loss: 0.5771\n",
            "169/168, train_loss: 0.9621\n",
            "epoch 19 average loss: 0.5726\n",
            "----------\n",
            "epoch 20/5\n",
            "1/168, train_loss: 0.2064\n",
            "2/168, train_loss: 0.4383\n",
            "3/168, train_loss: 0.3075\n",
            "4/168, train_loss: 0.4281\n",
            "5/168, train_loss: 0.5176\n",
            "6/168, train_loss: 0.7536\n",
            "7/168, train_loss: 0.7146\n",
            "8/168, train_loss: 0.7137\n",
            "9/168, train_loss: 0.4230\n",
            "10/168, train_loss: 0.5090\n",
            "11/168, train_loss: 0.4146\n",
            "12/168, train_loss: 0.5327\n",
            "13/168, train_loss: 0.4134\n",
            "14/168, train_loss: 1.0321\n",
            "15/168, train_loss: 0.4258\n",
            "16/168, train_loss: 0.4042\n",
            "17/168, train_loss: 0.4320\n",
            "18/168, train_loss: 1.1600\n",
            "19/168, train_loss: 0.4943\n",
            "20/168, train_loss: 0.7225\n",
            "21/168, train_loss: 0.3986\n",
            "22/168, train_loss: 0.3428\n",
            "23/168, train_loss: 0.9444\n",
            "24/168, train_loss: 0.4170\n",
            "25/168, train_loss: 0.4177\n",
            "26/168, train_loss: 0.4884\n",
            "27/168, train_loss: 1.0475\n",
            "28/168, train_loss: 0.4310\n",
            "29/168, train_loss: 0.4079\n",
            "30/168, train_loss: 0.4553\n",
            "31/168, train_loss: 0.4295\n",
            "32/168, train_loss: 0.4239\n",
            "33/168, train_loss: 1.0575\n",
            "34/168, train_loss: 0.4166\n",
            "35/168, train_loss: 0.5258\n",
            "36/168, train_loss: 0.2254\n",
            "37/168, train_loss: 0.3133\n",
            "38/168, train_loss: 0.4209\n",
            "39/168, train_loss: 1.1471\n",
            "40/168, train_loss: 1.0641\n",
            "41/168, train_loss: 0.7295\n",
            "42/168, train_loss: 0.5919\n",
            "43/168, train_loss: 0.9178\n",
            "44/168, train_loss: 0.7256\n",
            "45/168, train_loss: 0.4762\n",
            "46/168, train_loss: 0.4559\n",
            "47/168, train_loss: 1.1634\n",
            "48/168, train_loss: 0.5904\n",
            "49/168, train_loss: 0.4054\n",
            "50/168, train_loss: 0.4012\n",
            "51/168, train_loss: 0.5717\n",
            "52/168, train_loss: 1.1903\n",
            "53/168, train_loss: 0.7638\n",
            "54/168, train_loss: 0.4148\n",
            "55/168, train_loss: 1.0329\n",
            "56/168, train_loss: 0.3887\n",
            "57/168, train_loss: 1.0134\n",
            "58/168, train_loss: 0.7255\n",
            "59/168, train_loss: 0.4740\n",
            "60/168, train_loss: 0.4141\n",
            "61/168, train_loss: 0.3192\n",
            "62/168, train_loss: 1.2405\n",
            "63/168, train_loss: 0.4087\n",
            "64/168, train_loss: 0.5184\n",
            "65/168, train_loss: 0.8564\n",
            "66/168, train_loss: 0.4605\n",
            "67/168, train_loss: 0.4447\n",
            "68/168, train_loss: 0.4737\n",
            "69/168, train_loss: 0.5811\n",
            "70/168, train_loss: 0.4197\n",
            "71/168, train_loss: 0.5472\n",
            "72/168, train_loss: 0.4563\n",
            "73/168, train_loss: 0.4219\n",
            "74/168, train_loss: 0.5622\n",
            "75/168, train_loss: 1.0301\n",
            "76/168, train_loss: 0.4392\n",
            "77/168, train_loss: 0.4848\n",
            "78/168, train_loss: 0.6055\n",
            "79/168, train_loss: 0.4946\n",
            "80/168, train_loss: 0.7497\n",
            "81/168, train_loss: 1.1976\n",
            "82/168, train_loss: 0.7447\n",
            "83/168, train_loss: 0.4161\n",
            "84/168, train_loss: 0.5145\n",
            "85/168, train_loss: 0.3719\n",
            "86/168, train_loss: 0.7360\n",
            "87/168, train_loss: 0.4632\n",
            "88/168, train_loss: 0.7313\n",
            "89/168, train_loss: 1.0084\n",
            "90/168, train_loss: 0.4205\n",
            "91/168, train_loss: 1.0733\n",
            "92/168, train_loss: 0.7241\n",
            "93/168, train_loss: 0.5239\n",
            "94/168, train_loss: 0.5475\n",
            "95/168, train_loss: 0.2243\n",
            "96/168, train_loss: 0.2512\n",
            "97/168, train_loss: 1.0938\n",
            "98/168, train_loss: 0.7607\n",
            "99/168, train_loss: 0.7159\n",
            "100/168, train_loss: 0.4652\n",
            "101/168, train_loss: 1.0479\n",
            "102/168, train_loss: 0.4631\n",
            "103/168, train_loss: 0.5203\n",
            "104/168, train_loss: 0.3521\n",
            "105/168, train_loss: 0.5379\n",
            "106/168, train_loss: 1.0547\n",
            "107/168, train_loss: 0.4305\n",
            "108/168, train_loss: 0.6232\n",
            "109/168, train_loss: 0.3421\n",
            "110/168, train_loss: 0.7122\n",
            "111/168, train_loss: 0.6524\n",
            "112/168, train_loss: 0.4345\n",
            "113/168, train_loss: 0.4330\n",
            "114/168, train_loss: 0.4200\n",
            "115/168, train_loss: 0.5800\n",
            "116/168, train_loss: 0.5729\n",
            "117/168, train_loss: 0.4735\n",
            "118/168, train_loss: 0.4966\n",
            "119/168, train_loss: 0.4278\n",
            "120/168, train_loss: 0.7679\n",
            "121/168, train_loss: 0.4411\n",
            "122/168, train_loss: 0.4881\n",
            "123/168, train_loss: 0.4848\n",
            "124/168, train_loss: 0.2154\n",
            "125/168, train_loss: 0.4771\n",
            "126/168, train_loss: 0.6623\n",
            "127/168, train_loss: 0.3311\n",
            "128/168, train_loss: 0.5369\n",
            "129/168, train_loss: 0.3053\n",
            "130/168, train_loss: 0.4034\n",
            "131/168, train_loss: 0.4382\n",
            "132/168, train_loss: 1.0243\n",
            "133/168, train_loss: 0.5195\n",
            "134/168, train_loss: 0.4790\n",
            "135/168, train_loss: 0.5047\n",
            "136/168, train_loss: 0.4192\n",
            "137/168, train_loss: 0.5450\n",
            "138/168, train_loss: 0.5056\n",
            "139/168, train_loss: 0.4732\n",
            "140/168, train_loss: 0.4934\n",
            "141/168, train_loss: 0.3836\n",
            "142/168, train_loss: 0.4836\n",
            "143/168, train_loss: 0.4786\n",
            "144/168, train_loss: 0.4574\n",
            "145/168, train_loss: 0.4570\n",
            "146/168, train_loss: 0.3707\n",
            "147/168, train_loss: 0.4141\n",
            "148/168, train_loss: 0.2598\n",
            "149/168, train_loss: 0.4814\n",
            "150/168, train_loss: 0.5585\n",
            "151/168, train_loss: 0.3895\n",
            "152/168, train_loss: 1.1149\n",
            "153/168, train_loss: 0.3959\n",
            "154/168, train_loss: 0.6010\n",
            "155/168, train_loss: 0.1737\n",
            "156/168, train_loss: 0.6196\n",
            "157/168, train_loss: 1.0598\n",
            "158/168, train_loss: 0.4563\n",
            "159/168, train_loss: 1.0256\n",
            "160/168, train_loss: 0.4206\n",
            "161/168, train_loss: 0.5051\n",
            "162/168, train_loss: 0.4153\n",
            "163/168, train_loss: 0.1901\n",
            "164/168, train_loss: 0.4641\n",
            "165/168, train_loss: 0.1646\n",
            "166/168, train_loss: 0.4651\n",
            "167/168, train_loss: 0.2864\n",
            "168/168, train_loss: 0.2292\n",
            "169/168, train_loss: 0.4214\n",
            "epoch 20 average loss: 0.5633\n",
            "current epoch: 20 current accuracy: 0.4706 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 21/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 1.0282\n",
            "2/168, train_loss: 0.9862\n",
            "3/168, train_loss: 1.0002\n",
            "4/168, train_loss: 0.4984\n",
            "5/168, train_loss: 0.2362\n",
            "6/168, train_loss: 0.4262\n",
            "7/168, train_loss: 0.3206\n",
            "8/168, train_loss: 0.2456\n",
            "9/168, train_loss: 0.5166\n",
            "10/168, train_loss: 0.2826\n",
            "11/168, train_loss: 0.1928\n",
            "12/168, train_loss: 0.4145\n",
            "13/168, train_loss: 0.4464\n",
            "14/168, train_loss: 0.4353\n",
            "15/168, train_loss: 0.7306\n",
            "16/168, train_loss: 0.7289\n",
            "17/168, train_loss: 0.4680\n",
            "18/168, train_loss: 0.2796\n",
            "19/168, train_loss: 0.3469\n",
            "20/168, train_loss: 0.5589\n",
            "21/168, train_loss: 0.6348\n",
            "22/168, train_loss: 0.7329\n",
            "23/168, train_loss: 0.4519\n",
            "24/168, train_loss: 0.4682\n",
            "25/168, train_loss: 0.4615\n",
            "26/168, train_loss: 0.4311\n",
            "27/168, train_loss: 0.4521\n",
            "28/168, train_loss: 0.4182\n",
            "29/168, train_loss: 0.5348\n",
            "30/168, train_loss: 0.1601\n",
            "31/168, train_loss: 0.5492\n",
            "32/168, train_loss: 1.3408\n",
            "33/168, train_loss: 0.9655\n",
            "34/168, train_loss: 0.5261\n",
            "35/168, train_loss: 0.5090\n",
            "36/168, train_loss: 0.4551\n",
            "37/168, train_loss: 0.2108\n",
            "38/168, train_loss: 0.1211\n",
            "39/168, train_loss: 0.2128\n",
            "40/168, train_loss: 0.4879\n",
            "41/168, train_loss: 0.3892\n",
            "42/168, train_loss: 0.4822\n",
            "43/168, train_loss: 0.4393\n",
            "44/168, train_loss: 0.4318\n",
            "45/168, train_loss: 0.5348\n",
            "46/168, train_loss: 0.7840\n",
            "47/168, train_loss: 0.4834\n",
            "48/168, train_loss: 0.7790\n",
            "49/168, train_loss: 0.2882\n",
            "50/168, train_loss: 0.5005\n",
            "51/168, train_loss: 0.5854\n",
            "52/168, train_loss: 0.7411\n",
            "53/168, train_loss: 0.4190\n",
            "54/168, train_loss: 0.3934\n",
            "55/168, train_loss: 1.0493\n",
            "56/168, train_loss: 0.1733\n",
            "57/168, train_loss: 1.3082\n",
            "58/168, train_loss: 1.0228\n",
            "59/168, train_loss: 0.3300\n",
            "60/168, train_loss: 0.7618\n",
            "61/168, train_loss: 0.5919\n",
            "62/168, train_loss: 0.4112\n",
            "63/168, train_loss: 0.7749\n",
            "64/168, train_loss: 0.2834\n",
            "65/168, train_loss: 0.4878\n",
            "66/168, train_loss: 0.4056\n",
            "67/168, train_loss: 0.7480\n",
            "68/168, train_loss: 0.2792\n",
            "69/168, train_loss: 0.4690\n",
            "70/168, train_loss: 0.8150\n",
            "71/168, train_loss: 1.0473\n",
            "72/168, train_loss: 0.5278\n",
            "73/168, train_loss: 0.5065\n",
            "74/168, train_loss: 0.4350\n",
            "75/168, train_loss: 0.4392\n",
            "76/168, train_loss: 0.3751\n",
            "77/168, train_loss: 0.7026\n",
            "78/168, train_loss: 1.0378\n",
            "79/168, train_loss: 0.8000\n",
            "80/168, train_loss: 1.2218\n",
            "81/168, train_loss: 0.4284\n",
            "82/168, train_loss: 0.6645\n",
            "83/168, train_loss: 0.7497\n",
            "84/168, train_loss: 1.0831\n",
            "85/168, train_loss: 0.3951\n",
            "86/168, train_loss: 0.5121\n",
            "87/168, train_loss: 0.4117\n",
            "88/168, train_loss: 0.7198\n",
            "89/168, train_loss: 0.4076\n",
            "90/168, train_loss: 0.4104\n",
            "91/168, train_loss: 0.6534\n",
            "92/168, train_loss: 1.0578\n",
            "93/168, train_loss: 0.4244\n",
            "94/168, train_loss: 1.0581\n",
            "95/168, train_loss: 0.5351\n",
            "96/168, train_loss: 0.7307\n",
            "97/168, train_loss: 0.4650\n",
            "98/168, train_loss: 0.2349\n",
            "99/168, train_loss: 1.0994\n",
            "100/168, train_loss: 0.4531\n",
            "101/168, train_loss: 1.0896\n",
            "102/168, train_loss: 0.5051\n",
            "103/168, train_loss: 0.4114\n",
            "104/168, train_loss: 0.4332\n",
            "105/168, train_loss: 0.4276\n",
            "106/168, train_loss: 0.4252\n",
            "107/168, train_loss: 1.0203\n",
            "108/168, train_loss: 1.1639\n",
            "109/168, train_loss: 0.4527\n",
            "110/168, train_loss: 0.5573\n",
            "111/168, train_loss: 0.5715\n",
            "112/168, train_loss: 0.4254\n",
            "113/168, train_loss: 0.4450\n",
            "114/168, train_loss: 0.3453\n",
            "115/168, train_loss: 0.3731\n",
            "116/168, train_loss: 1.0494\n",
            "117/168, train_loss: 0.4463\n",
            "118/168, train_loss: 0.3642\n",
            "119/168, train_loss: 0.4981\n",
            "120/168, train_loss: 0.6942\n",
            "121/168, train_loss: 0.6557\n",
            "122/168, train_loss: 0.4106\n",
            "123/168, train_loss: 0.6440\n",
            "124/168, train_loss: 0.7508\n",
            "125/168, train_loss: 0.5488\n",
            "126/168, train_loss: 0.3474\n",
            "127/168, train_loss: 0.5551\n",
            "128/168, train_loss: 0.4469\n",
            "129/168, train_loss: 0.4482\n",
            "130/168, train_loss: 1.0070\n",
            "131/168, train_loss: 1.0799\n",
            "132/168, train_loss: 0.3085\n",
            "133/168, train_loss: 0.3616\n",
            "134/168, train_loss: 0.4659\n",
            "135/168, train_loss: 0.4418\n",
            "136/168, train_loss: 0.5465\n",
            "137/168, train_loss: 1.0949\n",
            "138/168, train_loss: 0.4581\n",
            "139/168, train_loss: 0.4631\n",
            "140/168, train_loss: 0.4446\n",
            "141/168, train_loss: 0.7324\n",
            "142/168, train_loss: 0.4551\n",
            "143/168, train_loss: 1.0200\n",
            "144/168, train_loss: 0.7292\n",
            "145/168, train_loss: 0.4575\n",
            "146/168, train_loss: 0.4604\n",
            "147/168, train_loss: 0.4493\n",
            "148/168, train_loss: 0.5998\n",
            "149/168, train_loss: 0.6498\n",
            "150/168, train_loss: 0.4171\n",
            "151/168, train_loss: 0.4533\n",
            "152/168, train_loss: 0.9577\n",
            "153/168, train_loss: 0.8773\n",
            "154/168, train_loss: 0.2452\n",
            "155/168, train_loss: 0.3986\n",
            "156/168, train_loss: 0.4125\n",
            "157/168, train_loss: 0.5127\n",
            "158/168, train_loss: 0.7201\n",
            "159/168, train_loss: 0.3519\n",
            "160/168, train_loss: 0.5290\n",
            "161/168, train_loss: 1.0080\n",
            "162/168, train_loss: 0.4190\n",
            "163/168, train_loss: 0.4837\n",
            "164/168, train_loss: 0.5450\n",
            "165/168, train_loss: 0.4708\n",
            "166/168, train_loss: 0.4555\n",
            "167/168, train_loss: 0.4262\n",
            "168/168, train_loss: 0.7198\n",
            "169/168, train_loss: 0.4247\n",
            "epoch 21 average loss: 0.5715\n",
            "----------\n",
            "epoch 22/5\n",
            "1/168, train_loss: 0.4248\n",
            "2/168, train_loss: 0.4913\n",
            "3/168, train_loss: 0.4789\n",
            "4/168, train_loss: 0.4540\n",
            "5/168, train_loss: 0.4432\n",
            "6/168, train_loss: 0.8098\n",
            "7/168, train_loss: 1.0017\n",
            "8/168, train_loss: 0.4458\n",
            "9/168, train_loss: 0.5300\n",
            "10/168, train_loss: 0.3594\n",
            "11/168, train_loss: 0.4730\n",
            "12/168, train_loss: 0.4812\n",
            "13/168, train_loss: 0.5137\n",
            "14/168, train_loss: 0.4463\n",
            "15/168, train_loss: 0.2403\n",
            "16/168, train_loss: 0.4993\n",
            "17/168, train_loss: 0.4239\n",
            "18/168, train_loss: 0.6326\n",
            "19/168, train_loss: 0.6128\n",
            "20/168, train_loss: 0.7306\n",
            "21/168, train_loss: 0.4885\n",
            "22/168, train_loss: 1.0387\n",
            "23/168, train_loss: 0.3392\n",
            "24/168, train_loss: 0.5770\n",
            "25/168, train_loss: 0.2352\n",
            "26/168, train_loss: 0.4786\n",
            "27/168, train_loss: 0.4607\n",
            "28/168, train_loss: 1.0161\n",
            "29/168, train_loss: 0.7370\n",
            "30/168, train_loss: 0.4737\n",
            "31/168, train_loss: 0.1581\n",
            "32/168, train_loss: 0.3679\n",
            "33/168, train_loss: 0.4248\n",
            "34/168, train_loss: 0.3235\n",
            "35/168, train_loss: 0.4149\n",
            "36/168, train_loss: 0.4467\n",
            "37/168, train_loss: 0.5939\n",
            "38/168, train_loss: 0.2298\n",
            "39/168, train_loss: 0.2016\n",
            "40/168, train_loss: 0.2566\n",
            "41/168, train_loss: 0.7440\n",
            "42/168, train_loss: 0.7427\n",
            "43/168, train_loss: 0.9878\n",
            "44/168, train_loss: 0.4765\n",
            "45/168, train_loss: 0.4032\n",
            "46/168, train_loss: 0.5379\n",
            "47/168, train_loss: 0.7362\n",
            "48/168, train_loss: 1.0340\n",
            "49/168, train_loss: 0.5108\n",
            "50/168, train_loss: 1.0091\n",
            "51/168, train_loss: 0.7792\n",
            "52/168, train_loss: 1.0660\n",
            "53/168, train_loss: 0.4807\n",
            "54/168, train_loss: 0.9658\n",
            "55/168, train_loss: 0.3755\n",
            "56/168, train_loss: 0.3638\n",
            "57/168, train_loss: 0.2849\n",
            "58/168, train_loss: 0.4585\n",
            "59/168, train_loss: 0.4990\n",
            "60/168, train_loss: 1.0075\n",
            "61/168, train_loss: 0.3818\n",
            "62/168, train_loss: 1.0417\n",
            "63/168, train_loss: 0.4186\n",
            "64/168, train_loss: 0.6232\n",
            "65/168, train_loss: 0.6062\n",
            "66/168, train_loss: 0.7357\n",
            "67/168, train_loss: 0.4456\n",
            "68/168, train_loss: 0.9597\n",
            "69/168, train_loss: 0.4378\n",
            "70/168, train_loss: 0.3078\n",
            "71/168, train_loss: 0.6580\n",
            "72/168, train_loss: 0.7422\n",
            "73/168, train_loss: 0.4434\n",
            "74/168, train_loss: 0.3452\n",
            "75/168, train_loss: 0.4154\n",
            "76/168, train_loss: 0.4152\n",
            "77/168, train_loss: 0.7376\n",
            "78/168, train_loss: 0.7371\n",
            "79/168, train_loss: 0.7936\n",
            "80/168, train_loss: 0.9561\n",
            "81/168, train_loss: 0.2109\n",
            "82/168, train_loss: 0.1630\n",
            "83/168, train_loss: 0.4284\n",
            "84/168, train_loss: 0.5511\n",
            "85/168, train_loss: 0.4725\n",
            "86/168, train_loss: 0.3600\n",
            "87/168, train_loss: 0.4521\n",
            "88/168, train_loss: 0.6050\n",
            "89/168, train_loss: 0.5978\n",
            "90/168, train_loss: 1.0146\n",
            "91/168, train_loss: 0.9921\n",
            "92/168, train_loss: 0.4784\n",
            "93/168, train_loss: 0.5107\n",
            "94/168, train_loss: 0.2019\n",
            "95/168, train_loss: 0.7236\n",
            "96/168, train_loss: 0.4286\n",
            "97/168, train_loss: 0.7223\n",
            "98/168, train_loss: 0.4430\n",
            "99/168, train_loss: 0.7196\n",
            "100/168, train_loss: 1.2405\n",
            "101/168, train_loss: 0.6404\n",
            "102/168, train_loss: 0.5489\n",
            "103/168, train_loss: 0.5077\n",
            "104/168, train_loss: 0.4984\n",
            "105/168, train_loss: 0.4985\n",
            "106/168, train_loss: 0.6185\n",
            "107/168, train_loss: 0.7285\n",
            "108/168, train_loss: 0.5550\n",
            "109/168, train_loss: 0.5611\n",
            "110/168, train_loss: 0.5069\n",
            "111/168, train_loss: 0.3492\n",
            "112/168, train_loss: 0.9786\n",
            "113/168, train_loss: 0.4471\n",
            "114/168, train_loss: 0.4569\n",
            "115/168, train_loss: 0.4182\n",
            "116/168, train_loss: 0.4452\n",
            "117/168, train_loss: 0.4457\n",
            "118/168, train_loss: 0.4374\n",
            "119/168, train_loss: 0.4791\n",
            "120/168, train_loss: 0.8656\n",
            "121/168, train_loss: 0.7483\n",
            "122/168, train_loss: 0.5797\n",
            "123/168, train_loss: 0.4525\n",
            "124/168, train_loss: 1.0089\n",
            "125/168, train_loss: 0.4431\n",
            "126/168, train_loss: 0.4165\n",
            "127/168, train_loss: 0.2163\n",
            "128/168, train_loss: 0.5047\n",
            "129/168, train_loss: 1.0123\n",
            "130/168, train_loss: 0.5385\n",
            "131/168, train_loss: 0.4035\n",
            "132/168, train_loss: 0.2702\n",
            "133/168, train_loss: 0.2517\n",
            "134/168, train_loss: 0.4209\n",
            "135/168, train_loss: 1.0115\n",
            "136/168, train_loss: 0.1204\n",
            "137/168, train_loss: 0.4323\n",
            "138/168, train_loss: 0.4909\n",
            "139/168, train_loss: 0.7607\n",
            "140/168, train_loss: 0.4226\n",
            "141/168, train_loss: 0.7561\n",
            "142/168, train_loss: 0.4057\n",
            "143/168, train_loss: 0.6014\n",
            "144/168, train_loss: 0.2365\n",
            "145/168, train_loss: 0.4158\n",
            "146/168, train_loss: 0.8426\n",
            "147/168, train_loss: 0.1689\n",
            "148/168, train_loss: 0.2256\n",
            "149/168, train_loss: 0.8158\n",
            "150/168, train_loss: 0.4279\n",
            "151/168, train_loss: 1.1122\n",
            "152/168, train_loss: 0.1713\n",
            "153/168, train_loss: 0.4140\n",
            "154/168, train_loss: 0.7422\n",
            "155/168, train_loss: 0.4238\n",
            "156/168, train_loss: 0.4400\n",
            "157/168, train_loss: 0.4437\n",
            "158/168, train_loss: 0.1977\n",
            "159/168, train_loss: 0.4349\n",
            "160/168, train_loss: 0.9101\n",
            "161/168, train_loss: 0.7424\n",
            "162/168, train_loss: 0.2528\n",
            "163/168, train_loss: 0.4232\n",
            "164/168, train_loss: 0.5231\n",
            "165/168, train_loss: 0.5132\n",
            "166/168, train_loss: 0.2711\n",
            "167/168, train_loss: 0.5647\n",
            "168/168, train_loss: 0.4096\n",
            "169/168, train_loss: 0.3964\n",
            "epoch 22 average loss: 0.5474\n",
            "current epoch: 22 current accuracy: 0.5059 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 23/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.4415\n",
            "2/168, train_loss: 0.4421\n",
            "3/168, train_loss: 0.4152\n",
            "4/168, train_loss: 0.3449\n",
            "5/168, train_loss: 0.7397\n",
            "6/168, train_loss: 0.4473\n",
            "7/168, train_loss: 0.7369\n",
            "8/168, train_loss: 0.4797\n",
            "9/168, train_loss: 0.5740\n",
            "10/168, train_loss: 0.4576\n",
            "11/168, train_loss: 1.0277\n",
            "12/168, train_loss: 1.0024\n",
            "13/168, train_loss: 0.4671\n",
            "14/168, train_loss: 0.4793\n",
            "15/168, train_loss: 0.4143\n",
            "16/168, train_loss: 0.4311\n",
            "17/168, train_loss: 0.5065\n",
            "18/168, train_loss: 0.6895\n",
            "19/168, train_loss: 1.0434\n",
            "20/168, train_loss: 0.4976\n",
            "21/168, train_loss: 0.5349\n",
            "22/168, train_loss: 0.4168\n",
            "23/168, train_loss: 0.3509\n",
            "24/168, train_loss: 0.4301\n",
            "25/168, train_loss: 0.8151\n",
            "26/168, train_loss: 0.4162\n",
            "27/168, train_loss: 0.3955\n",
            "28/168, train_loss: 0.3918\n",
            "29/168, train_loss: 1.2758\n",
            "30/168, train_loss: 0.1333\n",
            "31/168, train_loss: 0.4297\n",
            "32/168, train_loss: 0.3974\n",
            "33/168, train_loss: 0.2304\n",
            "34/168, train_loss: 0.1643\n",
            "35/168, train_loss: 0.3973\n",
            "36/168, train_loss: 0.4308\n",
            "37/168, train_loss: 0.4661\n",
            "38/168, train_loss: 1.0726\n",
            "39/168, train_loss: 0.4170\n",
            "40/168, train_loss: 1.0200\n",
            "41/168, train_loss: 0.4024\n",
            "42/168, train_loss: 0.4312\n",
            "43/168, train_loss: 0.6985\n",
            "44/168, train_loss: 0.4039\n",
            "45/168, train_loss: 0.7554\n",
            "46/168, train_loss: 0.4159\n",
            "47/168, train_loss: 0.6038\n",
            "48/168, train_loss: 0.4897\n",
            "49/168, train_loss: 0.2867\n",
            "50/168, train_loss: 1.0876\n",
            "51/168, train_loss: 0.5536\n",
            "52/168, train_loss: 1.0581\n",
            "53/168, train_loss: 1.1174\n",
            "54/168, train_loss: 0.4097\n",
            "55/168, train_loss: 0.3943\n",
            "56/168, train_loss: 0.5122\n",
            "57/168, train_loss: 0.4376\n",
            "58/168, train_loss: 0.5430\n",
            "59/168, train_loss: 0.3953\n",
            "60/168, train_loss: 0.5481\n",
            "61/168, train_loss: 0.3827\n",
            "62/168, train_loss: 1.0859\n",
            "63/168, train_loss: 0.3087\n",
            "64/168, train_loss: 0.4561\n",
            "65/168, train_loss: 0.4403\n",
            "66/168, train_loss: 1.0388\n",
            "67/168, train_loss: 0.5016\n",
            "68/168, train_loss: 0.4173\n",
            "69/168, train_loss: 0.7290\n",
            "70/168, train_loss: 0.4774\n",
            "71/168, train_loss: 0.4135\n",
            "72/168, train_loss: 0.2566\n",
            "73/168, train_loss: 0.4279\n",
            "74/168, train_loss: 0.4953\n",
            "75/168, train_loss: 0.4814\n",
            "76/168, train_loss: 0.3292\n",
            "77/168, train_loss: 0.4143\n",
            "78/168, train_loss: 0.4783\n",
            "79/168, train_loss: 0.4039\n",
            "80/168, train_loss: 0.3964\n",
            "81/168, train_loss: 1.2692\n",
            "82/168, train_loss: 0.7347\n",
            "83/168, train_loss: 1.0884\n",
            "84/168, train_loss: 0.7341\n",
            "85/168, train_loss: 0.2746\n",
            "86/168, train_loss: 0.4552\n",
            "87/168, train_loss: 0.4266\n",
            "88/168, train_loss: 0.7192\n",
            "89/168, train_loss: 0.7304\n",
            "90/168, train_loss: 0.3867\n",
            "91/168, train_loss: 0.3301\n",
            "92/168, train_loss: 0.3293\n",
            "93/168, train_loss: 0.7921\n",
            "94/168, train_loss: 0.3953\n",
            "95/168, train_loss: 0.1789\n",
            "96/168, train_loss: 0.4795\n",
            "97/168, train_loss: 0.9767\n",
            "98/168, train_loss: 0.3443\n",
            "99/168, train_loss: 0.2441\n",
            "100/168, train_loss: 1.0806\n",
            "101/168, train_loss: 0.7163\n",
            "102/168, train_loss: 0.2156\n",
            "103/168, train_loss: 0.4195\n",
            "104/168, train_loss: 0.4246\n",
            "105/168, train_loss: 0.7344\n",
            "106/168, train_loss: 0.4689\n",
            "107/168, train_loss: 0.7128\n",
            "108/168, train_loss: 0.4547\n",
            "109/168, train_loss: 0.4550\n",
            "110/168, train_loss: 0.3880\n",
            "111/168, train_loss: 1.0705\n",
            "112/168, train_loss: 0.4208\n",
            "113/168, train_loss: 0.1553\n",
            "114/168, train_loss: 0.4465\n",
            "115/168, train_loss: 0.2154\n",
            "116/168, train_loss: 0.3986\n",
            "117/168, train_loss: 0.5533\n",
            "118/168, train_loss: 0.5117\n",
            "119/168, train_loss: 1.0393\n",
            "120/168, train_loss: 0.3780\n",
            "121/168, train_loss: 0.7294\n",
            "122/168, train_loss: 0.6219\n",
            "123/168, train_loss: 0.7090\n",
            "124/168, train_loss: 0.8973\n",
            "125/168, train_loss: 0.4890\n",
            "126/168, train_loss: 1.1274\n",
            "127/168, train_loss: 0.7416\n",
            "128/168, train_loss: 0.5465\n",
            "129/168, train_loss: 0.4296\n",
            "130/168, train_loss: 0.4150\n",
            "131/168, train_loss: 0.6534\n",
            "132/168, train_loss: 0.4345\n",
            "133/168, train_loss: 0.4642\n",
            "134/168, train_loss: 1.0903\n",
            "135/168, train_loss: 0.4313\n",
            "136/168, train_loss: 0.1377\n",
            "137/168, train_loss: 0.9502\n",
            "138/168, train_loss: 0.4675\n",
            "139/168, train_loss: 0.4917\n",
            "140/168, train_loss: 0.4338\n",
            "141/168, train_loss: 0.3538\n",
            "142/168, train_loss: 0.5119\n",
            "143/168, train_loss: 0.1909\n",
            "144/168, train_loss: 0.5033\n",
            "145/168, train_loss: 0.1663\n",
            "146/168, train_loss: 0.7128\n",
            "147/168, train_loss: 0.4555\n",
            "148/168, train_loss: 0.5196\n",
            "149/168, train_loss: 0.4028\n",
            "150/168, train_loss: 0.1543\n",
            "151/168, train_loss: 0.4315\n",
            "152/168, train_loss: 0.4176\n",
            "153/168, train_loss: 0.8374\n",
            "154/168, train_loss: 0.2101\n",
            "155/168, train_loss: 0.4270\n",
            "156/168, train_loss: 0.6098\n",
            "157/168, train_loss: 0.4400\n",
            "158/168, train_loss: 1.1625\n",
            "159/168, train_loss: 0.4947\n",
            "160/168, train_loss: 1.0963\n",
            "161/168, train_loss: 0.3009\n",
            "162/168, train_loss: 0.4932\n",
            "163/168, train_loss: 0.6001\n",
            "164/168, train_loss: 0.6195\n",
            "165/168, train_loss: 0.4382\n",
            "166/168, train_loss: 0.5159\n",
            "167/168, train_loss: 0.1801\n",
            "168/168, train_loss: 0.4673\n",
            "169/168, train_loss: 0.3851\n",
            "epoch 23 average loss: 0.5433\n",
            "----------\n",
            "epoch 24/5\n",
            "1/168, train_loss: 0.4040\n",
            "2/168, train_loss: 0.4212\n",
            "3/168, train_loss: 0.5979\n",
            "4/168, train_loss: 0.4352\n",
            "5/168, train_loss: 0.5277\n",
            "6/168, train_loss: 0.7286\n",
            "7/168, train_loss: 0.4132\n",
            "8/168, train_loss: 0.7260\n",
            "9/168, train_loss: 0.4959\n",
            "10/168, train_loss: 0.4250\n",
            "11/168, train_loss: 1.0648\n",
            "12/168, train_loss: 0.4491\n",
            "13/168, train_loss: 0.3477\n",
            "14/168, train_loss: 1.1275\n",
            "15/168, train_loss: 0.4817\n",
            "16/168, train_loss: 0.4950\n",
            "17/168, train_loss: 0.4191\n",
            "18/168, train_loss: 0.4131\n",
            "19/168, train_loss: 0.2441\n",
            "20/168, train_loss: 0.4472\n",
            "21/168, train_loss: 0.4225\n",
            "22/168, train_loss: 0.4650\n",
            "23/168, train_loss: 0.5055\n",
            "24/168, train_loss: 0.4607\n",
            "25/168, train_loss: 1.3123\n",
            "26/168, train_loss: 0.3853\n",
            "27/168, train_loss: 1.0408\n",
            "28/168, train_loss: 1.0328\n",
            "29/168, train_loss: 0.2240\n",
            "30/168, train_loss: 0.2823\n",
            "31/168, train_loss: 0.9898\n",
            "32/168, train_loss: 0.6158\n",
            "33/168, train_loss: 0.2574\n",
            "34/168, train_loss: 0.2102\n",
            "35/168, train_loss: 0.2031\n",
            "36/168, train_loss: 0.3673\n",
            "37/168, train_loss: 0.3472\n",
            "38/168, train_loss: 0.4128\n",
            "39/168, train_loss: 0.9139\n",
            "40/168, train_loss: 0.3547\n",
            "41/168, train_loss: 0.6904\n",
            "42/168, train_loss: 0.5022\n",
            "43/168, train_loss: 0.1742\n",
            "44/168, train_loss: 0.2611\n",
            "45/168, train_loss: 0.5495\n",
            "46/168, train_loss: 0.2635\n",
            "47/168, train_loss: 0.7334\n",
            "48/168, train_loss: 0.5111\n",
            "49/168, train_loss: 0.4332\n",
            "50/168, train_loss: 0.2091\n",
            "51/168, train_loss: 0.4825\n",
            "52/168, train_loss: 0.2648\n",
            "53/168, train_loss: 0.4811\n",
            "54/168, train_loss: 0.4529\n",
            "55/168, train_loss: 1.0498\n",
            "56/168, train_loss: 0.2999\n",
            "57/168, train_loss: 0.3227\n",
            "58/168, train_loss: 0.5082\n",
            "59/168, train_loss: 0.3881\n",
            "60/168, train_loss: 0.5571\n",
            "61/168, train_loss: 1.1205\n",
            "62/168, train_loss: 0.7417\n",
            "63/168, train_loss: 1.0443\n",
            "64/168, train_loss: 0.7320\n",
            "65/168, train_loss: 0.5363\n",
            "66/168, train_loss: 0.4007\n",
            "67/168, train_loss: 0.3203\n",
            "68/168, train_loss: 0.7244\n",
            "69/168, train_loss: 0.7228\n",
            "70/168, train_loss: 0.4890\n",
            "71/168, train_loss: 0.2808\n",
            "72/168, train_loss: 0.6652\n",
            "73/168, train_loss: 0.6333\n",
            "74/168, train_loss: 0.7174\n",
            "75/168, train_loss: 0.7160\n",
            "76/168, train_loss: 0.2549\n",
            "77/168, train_loss: 0.4883\n",
            "78/168, train_loss: 0.5655\n",
            "79/168, train_loss: 0.0827\n",
            "80/168, train_loss: 0.5467\n",
            "81/168, train_loss: 0.1200\n",
            "82/168, train_loss: 0.2492\n",
            "83/168, train_loss: 0.4716\n",
            "84/168, train_loss: 0.4207\n",
            "85/168, train_loss: 0.4502\n",
            "86/168, train_loss: 0.2046\n",
            "87/168, train_loss: 0.5264\n",
            "88/168, train_loss: 0.5708\n",
            "89/168, train_loss: 0.4063\n",
            "90/168, train_loss: 1.1030\n",
            "91/168, train_loss: 0.1639\n",
            "92/168, train_loss: 1.3561\n",
            "93/168, train_loss: 0.4449\n",
            "94/168, train_loss: 0.4282\n",
            "95/168, train_loss: 1.0996\n",
            "96/168, train_loss: 1.1229\n",
            "97/168, train_loss: 0.7247\n",
            "98/168, train_loss: 1.3871\n",
            "99/168, train_loss: 0.7224\n",
            "100/168, train_loss: 0.1885\n",
            "101/168, train_loss: 0.5081\n",
            "102/168, train_loss: 0.4645\n",
            "103/168, train_loss: 0.4698\n",
            "104/168, train_loss: 0.5061\n",
            "105/168, train_loss: 0.1051\n",
            "106/168, train_loss: 0.4795\n",
            "107/168, train_loss: 0.5036\n",
            "108/168, train_loss: 0.4675\n",
            "109/168, train_loss: 0.4600\n",
            "110/168, train_loss: 0.2917\n",
            "111/168, train_loss: 0.4513\n",
            "112/168, train_loss: 0.4571\n",
            "113/168, train_loss: 0.6263\n",
            "114/168, train_loss: 0.3407\n",
            "115/168, train_loss: 0.4030\n",
            "116/168, train_loss: 1.1072\n",
            "117/168, train_loss: 0.5180\n",
            "118/168, train_loss: 0.4519\n",
            "119/168, train_loss: 0.3903\n",
            "120/168, train_loss: 0.4863\n",
            "121/168, train_loss: 0.7455\n",
            "122/168, train_loss: 0.4312\n",
            "123/168, train_loss: 0.5924\n",
            "124/168, train_loss: 1.3921\n",
            "125/168, train_loss: 0.4317\n",
            "126/168, train_loss: 1.2123\n",
            "127/168, train_loss: 1.0262\n",
            "128/168, train_loss: 0.5263\n",
            "129/168, train_loss: 0.3991\n",
            "130/168, train_loss: 0.4236\n",
            "131/168, train_loss: 0.4252\n",
            "132/168, train_loss: 0.4577\n",
            "133/168, train_loss: 0.4858\n",
            "134/168, train_loss: 0.4786\n",
            "135/168, train_loss: 0.4331\n",
            "136/168, train_loss: 0.5017\n",
            "137/168, train_loss: 0.5065\n",
            "138/168, train_loss: 0.3042\n",
            "139/168, train_loss: 0.4481\n",
            "140/168, train_loss: 0.4458\n",
            "141/168, train_loss: 0.7646\n",
            "142/168, train_loss: 0.7415\n",
            "143/168, train_loss: 0.4274\n",
            "144/168, train_loss: 1.0491\n",
            "145/168, train_loss: 0.3943\n",
            "146/168, train_loss: 0.4299\n",
            "147/168, train_loss: 0.4020\n",
            "148/168, train_loss: 0.9684\n",
            "149/168, train_loss: 1.0763\n",
            "150/168, train_loss: 0.7650\n",
            "151/168, train_loss: 1.0668\n",
            "152/168, train_loss: 1.0922\n",
            "153/168, train_loss: 0.1731\n",
            "154/168, train_loss: 0.3748\n",
            "155/168, train_loss: 1.1248\n",
            "156/168, train_loss: 0.4389\n",
            "157/168, train_loss: 0.6384\n",
            "158/168, train_loss: 0.4045\n",
            "159/168, train_loss: 0.5504\n",
            "160/168, train_loss: 0.4638\n",
            "161/168, train_loss: 0.4179\n",
            "162/168, train_loss: 0.5608\n",
            "163/168, train_loss: 0.4113\n",
            "164/168, train_loss: 0.4095\n",
            "165/168, train_loss: 0.4195\n",
            "166/168, train_loss: 0.2523\n",
            "167/168, train_loss: 0.4058\n",
            "168/168, train_loss: 0.3205\n",
            "169/168, train_loss: 1.0999\n",
            "epoch 24 average loss: 0.5523\n",
            "current epoch: 24 current accuracy: 0.4471 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 25/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.4005\n",
            "2/168, train_loss: 1.0530\n",
            "3/168, train_loss: 0.4027\n",
            "4/168, train_loss: 0.3617\n",
            "5/168, train_loss: 0.5927\n",
            "6/168, train_loss: 0.2966\n",
            "7/168, train_loss: 0.3239\n",
            "8/168, train_loss: 1.0368\n",
            "9/168, train_loss: 1.0622\n",
            "10/168, train_loss: 0.3732\n",
            "11/168, train_loss: 0.4145\n",
            "12/168, train_loss: 0.4679\n",
            "13/168, train_loss: 0.4313\n",
            "14/168, train_loss: 0.4524\n",
            "15/168, train_loss: 0.1708\n",
            "16/168, train_loss: 0.8791\n",
            "17/168, train_loss: 0.5208\n",
            "18/168, train_loss: 0.4064\n",
            "19/168, train_loss: 0.6978\n",
            "20/168, train_loss: 0.1695\n",
            "21/168, train_loss: 0.4099\n",
            "22/168, train_loss: 0.4162\n",
            "23/168, train_loss: 0.6557\n",
            "24/168, train_loss: 0.3295\n",
            "25/168, train_loss: 1.0172\n",
            "26/168, train_loss: 0.3702\n",
            "27/168, train_loss: 0.4906\n",
            "28/168, train_loss: 0.4303\n",
            "29/168, train_loss: 0.4703\n",
            "30/168, train_loss: 0.4251\n",
            "31/168, train_loss: 0.4368\n",
            "32/168, train_loss: 0.4395\n",
            "33/168, train_loss: 1.0119\n",
            "34/168, train_loss: 1.1031\n",
            "35/168, train_loss: 0.4739\n",
            "36/168, train_loss: 0.4440\n",
            "37/168, train_loss: 0.4490\n",
            "38/168, train_loss: 0.3832\n",
            "39/168, train_loss: 0.4338\n",
            "40/168, train_loss: 0.4249\n",
            "41/168, train_loss: 0.3365\n",
            "42/168, train_loss: 0.5850\n",
            "43/168, train_loss: 0.4626\n",
            "44/168, train_loss: 0.7466\n",
            "45/168, train_loss: 0.5498\n",
            "46/168, train_loss: 0.1439\n",
            "47/168, train_loss: 1.1815\n",
            "48/168, train_loss: 0.4095\n",
            "49/168, train_loss: 1.1000\n",
            "50/168, train_loss: 0.3221\n",
            "51/168, train_loss: 1.0293\n",
            "52/168, train_loss: 0.1697\n",
            "53/168, train_loss: 0.5361\n",
            "54/168, train_loss: 0.4330\n",
            "55/168, train_loss: 0.4758\n",
            "56/168, train_loss: 0.4163\n",
            "57/168, train_loss: 0.3815\n",
            "58/168, train_loss: 1.2808\n",
            "59/168, train_loss: 0.4020\n",
            "60/168, train_loss: 0.4918\n",
            "61/168, train_loss: 0.6369\n",
            "62/168, train_loss: 0.6280\n",
            "63/168, train_loss: 0.4516\n",
            "64/168, train_loss: 0.2805\n",
            "65/168, train_loss: 0.8259\n",
            "66/168, train_loss: 0.4308\n",
            "67/168, train_loss: 0.4684\n",
            "68/168, train_loss: 0.4297\n",
            "69/168, train_loss: 0.6301\n",
            "70/168, train_loss: 0.2262\n",
            "71/168, train_loss: 0.4540\n",
            "72/168, train_loss: 0.5842\n",
            "73/168, train_loss: 0.4444\n",
            "74/168, train_loss: 0.1515\n",
            "75/168, train_loss: 0.4709\n",
            "76/168, train_loss: 0.1780\n",
            "77/168, train_loss: 0.4296\n",
            "78/168, train_loss: 1.0488\n",
            "79/168, train_loss: 0.4777\n",
            "80/168, train_loss: 0.4286\n",
            "81/168, train_loss: 0.7763\n",
            "82/168, train_loss: 0.4166\n",
            "83/168, train_loss: 0.4071\n",
            "84/168, train_loss: 0.3095\n",
            "85/168, train_loss: 0.1502\n",
            "86/168, train_loss: 0.4730\n",
            "87/168, train_loss: 0.4142\n",
            "88/168, train_loss: 0.4921\n",
            "89/168, train_loss: 0.7296\n",
            "90/168, train_loss: 0.4182\n",
            "91/168, train_loss: 0.3856\n",
            "92/168, train_loss: 0.4042\n",
            "93/168, train_loss: 0.2353\n",
            "94/168, train_loss: 0.4672\n",
            "95/168, train_loss: 0.7309\n",
            "96/168, train_loss: 0.4750\n",
            "97/168, train_loss: 0.3820\n",
            "98/168, train_loss: 0.4368\n",
            "99/168, train_loss: 0.3980\n",
            "100/168, train_loss: 1.1109\n",
            "101/168, train_loss: 0.6480\n",
            "102/168, train_loss: 0.6643\n",
            "103/168, train_loss: 0.3445\n",
            "104/168, train_loss: 0.7551\n",
            "105/168, train_loss: 1.0270\n",
            "106/168, train_loss: 0.7507\n",
            "107/168, train_loss: 1.0796\n",
            "108/168, train_loss: 0.2879\n",
            "109/168, train_loss: 1.0331\n",
            "110/168, train_loss: 0.7355\n",
            "111/168, train_loss: 1.1106\n",
            "112/168, train_loss: 0.4541\n",
            "113/168, train_loss: 0.5842\n",
            "114/168, train_loss: 0.7108\n",
            "115/168, train_loss: 0.1883\n",
            "116/168, train_loss: 0.5980\n",
            "117/168, train_loss: 0.5162\n",
            "118/168, train_loss: 0.7947\n",
            "119/168, train_loss: 0.4377\n",
            "120/168, train_loss: 1.0432\n",
            "121/168, train_loss: 1.0112\n",
            "122/168, train_loss: 0.1376\n",
            "123/168, train_loss: 1.0784\n",
            "124/168, train_loss: 0.5123\n",
            "125/168, train_loss: 0.3278\n",
            "126/168, train_loss: 0.4386\n",
            "127/168, train_loss: 0.4039\n",
            "128/168, train_loss: 0.3454\n",
            "129/168, train_loss: 0.4534\n",
            "130/168, train_loss: 1.2509\n",
            "131/168, train_loss: 0.7474\n",
            "132/168, train_loss: 0.4260\n",
            "133/168, train_loss: 0.4536\n",
            "134/168, train_loss: 0.4307\n",
            "135/168, train_loss: 1.0439\n",
            "136/168, train_loss: 0.1673\n",
            "137/168, train_loss: 0.4777\n",
            "138/168, train_loss: 0.4110\n",
            "139/168, train_loss: 0.5862\n",
            "140/168, train_loss: 0.4297\n",
            "141/168, train_loss: 0.4263\n",
            "142/168, train_loss: 0.4275\n",
            "143/168, train_loss: 0.3904\n",
            "144/168, train_loss: 0.4335\n",
            "145/168, train_loss: 0.6167\n",
            "146/168, train_loss: 0.4041\n",
            "147/168, train_loss: 0.6247\n",
            "148/168, train_loss: 0.7503\n",
            "149/168, train_loss: 0.4164\n",
            "150/168, train_loss: 0.7486\n",
            "151/168, train_loss: 0.5855\n",
            "152/168, train_loss: 0.3200\n",
            "153/168, train_loss: 0.5137\n",
            "154/168, train_loss: 0.4436\n",
            "155/168, train_loss: 0.3683\n",
            "156/168, train_loss: 0.5476\n",
            "157/168, train_loss: 0.4391\n",
            "158/168, train_loss: 1.1053\n",
            "159/168, train_loss: 0.5069\n",
            "160/168, train_loss: 0.4605\n",
            "161/168, train_loss: 0.4045\n",
            "162/168, train_loss: 0.4291\n",
            "163/168, train_loss: 0.4036\n",
            "164/168, train_loss: 1.1612\n",
            "165/168, train_loss: 0.5092\n",
            "166/168, train_loss: 0.2626\n",
            "167/168, train_loss: 0.4002\n",
            "168/168, train_loss: 0.1727\n",
            "169/168, train_loss: 1.0860\n",
            "epoch 25 average loss: 0.5451\n",
            "----------\n",
            "epoch 26/5\n",
            "1/168, train_loss: 0.4151\n",
            "2/168, train_loss: 0.5070\n",
            "3/168, train_loss: 0.3127\n",
            "4/168, train_loss: 0.4913\n",
            "5/168, train_loss: 0.1496\n",
            "6/168, train_loss: 0.5618\n",
            "7/168, train_loss: 0.6156\n",
            "8/168, train_loss: 0.5311\n",
            "9/168, train_loss: 0.1493\n",
            "10/168, train_loss: 0.4149\n",
            "11/168, train_loss: 0.5437\n",
            "12/168, train_loss: 0.7414\n",
            "13/168, train_loss: 0.4521\n",
            "14/168, train_loss: 0.4740\n",
            "15/168, train_loss: 1.0856\n",
            "16/168, train_loss: 0.4859\n",
            "17/168, train_loss: 0.4661\n",
            "18/168, train_loss: 0.4316\n",
            "19/168, train_loss: 0.7347\n",
            "20/168, train_loss: 0.2331\n",
            "21/168, train_loss: 0.5312\n",
            "22/168, train_loss: 1.6971\n",
            "23/168, train_loss: 0.2909\n",
            "24/168, train_loss: 0.3894\n",
            "25/168, train_loss: 0.7276\n",
            "26/168, train_loss: 0.5400\n",
            "27/168, train_loss: 0.1897\n",
            "28/168, train_loss: 0.5229\n",
            "29/168, train_loss: 0.5720\n",
            "30/168, train_loss: 1.4009\n",
            "31/168, train_loss: 0.7022\n",
            "32/168, train_loss: 0.4722\n",
            "33/168, train_loss: 0.3827\n",
            "34/168, train_loss: 0.3822\n",
            "35/168, train_loss: 0.7228\n",
            "36/168, train_loss: 0.4751\n",
            "37/168, train_loss: 1.1160\n",
            "38/168, train_loss: 0.2132\n",
            "39/168, train_loss: 1.1479\n",
            "40/168, train_loss: 0.4905\n",
            "41/168, train_loss: 0.4793\n",
            "42/168, train_loss: 0.7292\n",
            "43/168, train_loss: 0.7324\n",
            "44/168, train_loss: 0.5979\n",
            "45/168, train_loss: 0.3027\n",
            "46/168, train_loss: 0.3932\n",
            "47/168, train_loss: 0.5703\n",
            "48/168, train_loss: 0.1224\n",
            "49/168, train_loss: 1.0674\n",
            "50/168, train_loss: 1.1508\n",
            "51/168, train_loss: 0.4388\n",
            "52/168, train_loss: 1.0313\n",
            "53/168, train_loss: 0.2989\n",
            "54/168, train_loss: 0.7273\n",
            "55/168, train_loss: 0.4333\n",
            "56/168, train_loss: 0.4829\n",
            "57/168, train_loss: 0.3949\n",
            "58/168, train_loss: 0.4058\n",
            "59/168, train_loss: 0.2794\n",
            "60/168, train_loss: 0.4985\n",
            "61/168, train_loss: 0.5689\n",
            "62/168, train_loss: 0.4606\n",
            "63/168, train_loss: 1.4043\n",
            "64/168, train_loss: 0.6488\n",
            "65/168, train_loss: 0.6231\n",
            "66/168, train_loss: 0.2603\n",
            "67/168, train_loss: 0.2359\n",
            "68/168, train_loss: 0.7150\n",
            "69/168, train_loss: 0.2754\n",
            "70/168, train_loss: 0.4773\n",
            "71/168, train_loss: 0.2736\n",
            "72/168, train_loss: 0.4050\n",
            "73/168, train_loss: 0.4873\n",
            "74/168, train_loss: 0.4712\n",
            "75/168, train_loss: 1.0860\n",
            "76/168, train_loss: 1.0590\n",
            "77/168, train_loss: 0.6043\n",
            "78/168, train_loss: 0.4300\n",
            "79/168, train_loss: 0.3952\n",
            "80/168, train_loss: 0.4012\n",
            "81/168, train_loss: 0.4272\n",
            "82/168, train_loss: 0.3950\n",
            "83/168, train_loss: 0.5264\n",
            "84/168, train_loss: 0.4056\n",
            "85/168, train_loss: 0.2319\n",
            "86/168, train_loss: 0.7276\n",
            "87/168, train_loss: 1.0410\n",
            "88/168, train_loss: 0.7432\n",
            "89/168, train_loss: 0.9691\n",
            "90/168, train_loss: 0.4245\n",
            "91/168, train_loss: 0.3960\n",
            "92/168, train_loss: 1.0776\n",
            "93/168, train_loss: 0.3858\n",
            "94/168, train_loss: 0.1374\n",
            "95/168, train_loss: 0.3925\n",
            "96/168, train_loss: 0.4300\n",
            "97/168, train_loss: 0.4493\n",
            "98/168, train_loss: 1.1369\n",
            "99/168, train_loss: 0.4146\n",
            "100/168, train_loss: 0.4614\n",
            "101/168, train_loss: 0.4046\n",
            "102/168, train_loss: 0.7176\n",
            "103/168, train_loss: 0.4799\n",
            "104/168, train_loss: 0.5308\n",
            "105/168, train_loss: 1.1142\n",
            "106/168, train_loss: 0.1488\n",
            "107/168, train_loss: 0.3926\n",
            "108/168, train_loss: 0.7138\n",
            "109/168, train_loss: 0.3973\n",
            "110/168, train_loss: 0.4375\n",
            "111/168, train_loss: 1.1984\n",
            "112/168, train_loss: 1.1647\n",
            "113/168, train_loss: 0.3601\n",
            "114/168, train_loss: 1.1606\n",
            "115/168, train_loss: 0.5326\n",
            "116/168, train_loss: 0.3891\n",
            "117/168, train_loss: 0.4227\n",
            "118/168, train_loss: 0.3886\n",
            "119/168, train_loss: 1.0389\n",
            "120/168, train_loss: 0.4716\n",
            "121/168, train_loss: 0.3940\n",
            "122/168, train_loss: 0.5870\n",
            "123/168, train_loss: 0.7218\n",
            "124/168, train_loss: 0.3767\n",
            "125/168, train_loss: 0.5544\n",
            "126/168, train_loss: 1.0889\n",
            "127/168, train_loss: 0.2549\n",
            "128/168, train_loss: 0.5036\n",
            "129/168, train_loss: 0.3516\n",
            "130/168, train_loss: 0.2498\n",
            "131/168, train_loss: 0.3023\n",
            "132/168, train_loss: 0.7195\n",
            "133/168, train_loss: 0.4835\n",
            "134/168, train_loss: 0.2245\n",
            "135/168, train_loss: 0.5454\n",
            "136/168, train_loss: 0.4865\n",
            "137/168, train_loss: 0.7850\n",
            "138/168, train_loss: 0.5048\n",
            "139/168, train_loss: 0.7182\n",
            "140/168, train_loss: 0.4881\n",
            "141/168, train_loss: 0.9760\n",
            "142/168, train_loss: 0.6244\n",
            "143/168, train_loss: 0.1779\n",
            "144/168, train_loss: 0.4510\n",
            "145/168, train_loss: 0.2283\n",
            "146/168, train_loss: 0.4810\n",
            "147/168, train_loss: 0.5938\n",
            "148/168, train_loss: 0.4191\n",
            "149/168, train_loss: 0.5850\n",
            "150/168, train_loss: 1.0834\n",
            "151/168, train_loss: 0.6649\n",
            "152/168, train_loss: 0.4392\n",
            "153/168, train_loss: 0.4417\n",
            "154/168, train_loss: 0.6739\n",
            "155/168, train_loss: 0.4601\n",
            "156/168, train_loss: 0.4763\n",
            "157/168, train_loss: 0.4808\n",
            "158/168, train_loss: 0.5837\n",
            "159/168, train_loss: 0.3449\n",
            "160/168, train_loss: 1.0596\n",
            "161/168, train_loss: 1.1438\n",
            "162/168, train_loss: 0.3990\n",
            "163/168, train_loss: 0.3047\n",
            "164/168, train_loss: 0.5194\n",
            "165/168, train_loss: 0.2460\n",
            "166/168, train_loss: 0.5446\n",
            "167/168, train_loss: 0.2920\n",
            "168/168, train_loss: 0.4331\n",
            "169/168, train_loss: 0.4033\n",
            "epoch 26 average loss: 0.5603\n",
            "current epoch: 26 current accuracy: 0.4235 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 27/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.3931\n",
            "2/168, train_loss: 0.6388\n",
            "3/168, train_loss: 0.7225\n",
            "4/168, train_loss: 0.7825\n",
            "5/168, train_loss: 0.7729\n",
            "6/168, train_loss: 0.3960\n",
            "7/168, train_loss: 0.7668\n",
            "8/168, train_loss: 0.4193\n",
            "9/168, train_loss: 0.5604\n",
            "10/168, train_loss: 0.4164\n",
            "11/168, train_loss: 0.3408\n",
            "12/168, train_loss: 0.5569\n",
            "13/168, train_loss: 0.2615\n",
            "14/168, train_loss: 0.7499\n",
            "15/168, train_loss: 0.3977\n",
            "16/168, train_loss: 0.4361\n",
            "17/168, train_loss: 1.1376\n",
            "18/168, train_loss: 0.4124\n",
            "19/168, train_loss: 1.1657\n",
            "20/168, train_loss: 1.0112\n",
            "21/168, train_loss: 0.4219\n",
            "22/168, train_loss: 0.4417\n",
            "23/168, train_loss: 0.2607\n",
            "24/168, train_loss: 0.5361\n",
            "25/168, train_loss: 0.3026\n",
            "26/168, train_loss: 0.3928\n",
            "27/168, train_loss: 0.3918\n",
            "28/168, train_loss: 0.2415\n",
            "29/168, train_loss: 0.4015\n",
            "30/168, train_loss: 0.3490\n",
            "31/168, train_loss: 1.0939\n",
            "32/168, train_loss: 0.4511\n",
            "33/168, train_loss: 0.4168\n",
            "34/168, train_loss: 0.1079\n",
            "35/168, train_loss: 0.4128\n",
            "36/168, train_loss: 0.4203\n",
            "37/168, train_loss: 0.3137\n",
            "38/168, train_loss: 1.0890\n",
            "39/168, train_loss: 1.1326\n",
            "40/168, train_loss: 0.6238\n",
            "41/168, train_loss: 0.1187\n",
            "42/168, train_loss: 0.4027\n",
            "43/168, train_loss: 0.3969\n",
            "44/168, train_loss: 0.7384\n",
            "45/168, train_loss: 1.1020\n",
            "46/168, train_loss: 0.3947\n",
            "47/168, train_loss: 0.2575\n",
            "48/168, train_loss: 0.7326\n",
            "49/168, train_loss: 0.2291\n",
            "50/168, train_loss: 0.2118\n",
            "51/168, train_loss: 0.2725\n",
            "52/168, train_loss: 0.5290\n",
            "53/168, train_loss: 0.3757\n",
            "54/168, train_loss: 0.2017\n",
            "55/168, train_loss: 0.6073\n",
            "56/168, train_loss: 0.3904\n",
            "57/168, train_loss: 0.2764\n",
            "58/168, train_loss: 0.3993\n",
            "59/168, train_loss: 0.3955\n",
            "60/168, train_loss: 0.4729\n",
            "61/168, train_loss: 0.8155\n",
            "62/168, train_loss: 0.3800\n",
            "63/168, train_loss: 0.2741\n",
            "64/168, train_loss: 0.4701\n",
            "65/168, train_loss: 0.4053\n",
            "66/168, train_loss: 0.5300\n",
            "67/168, train_loss: 1.1619\n",
            "68/168, train_loss: 1.0867\n",
            "69/168, train_loss: 0.4272\n",
            "70/168, train_loss: 1.0759\n",
            "71/168, train_loss: 0.4047\n",
            "72/168, train_loss: 0.3811\n",
            "73/168, train_loss: 0.4188\n",
            "74/168, train_loss: 0.4247\n",
            "75/168, train_loss: 0.3913\n",
            "76/168, train_loss: 0.3955\n",
            "77/168, train_loss: 0.5639\n",
            "78/168, train_loss: 0.5722\n",
            "79/168, train_loss: 1.3999\n",
            "80/168, train_loss: 0.6161\n",
            "81/168, train_loss: 0.4791\n",
            "82/168, train_loss: 0.7410\n",
            "83/168, train_loss: 0.5169\n",
            "84/168, train_loss: 0.3964\n",
            "85/168, train_loss: 0.5045\n",
            "86/168, train_loss: 0.4302\n",
            "87/168, train_loss: 0.4612\n",
            "88/168, train_loss: 0.2978\n",
            "89/168, train_loss: 0.3950\n",
            "90/168, train_loss: 1.0449\n",
            "91/168, train_loss: 0.4577\n",
            "92/168, train_loss: 0.4590\n",
            "93/168, train_loss: 0.5623\n",
            "94/168, train_loss: 0.3985\n",
            "95/168, train_loss: 0.4793\n",
            "96/168, train_loss: 0.4181\n",
            "97/168, train_loss: 0.3460\n",
            "98/168, train_loss: 0.3691\n",
            "99/168, train_loss: 0.2170\n",
            "100/168, train_loss: 0.2720\n",
            "101/168, train_loss: 0.4956\n",
            "102/168, train_loss: 0.4333\n",
            "103/168, train_loss: 1.1172\n",
            "104/168, train_loss: 0.3849\n",
            "105/168, train_loss: 1.0644\n",
            "106/168, train_loss: 0.5025\n",
            "107/168, train_loss: 1.0867\n",
            "108/168, train_loss: 0.5441\n",
            "109/168, train_loss: 0.3803\n",
            "110/168, train_loss: 0.3989\n",
            "111/168, train_loss: 1.0834\n",
            "112/168, train_loss: 0.4261\n",
            "113/168, train_loss: 0.3968\n",
            "114/168, train_loss: 0.4449\n",
            "115/168, train_loss: 0.2860\n",
            "116/168, train_loss: 0.3826\n",
            "117/168, train_loss: 0.3847\n",
            "118/168, train_loss: 1.1594\n",
            "119/168, train_loss: 0.4125\n",
            "120/168, train_loss: 0.4239\n",
            "121/168, train_loss: 0.6741\n",
            "122/168, train_loss: 0.2564\n",
            "123/168, train_loss: 0.3765\n",
            "124/168, train_loss: 0.4022\n",
            "125/168, train_loss: 0.4573\n",
            "126/168, train_loss: 0.5574\n",
            "127/168, train_loss: 0.6991\n",
            "128/168, train_loss: 0.3959\n",
            "129/168, train_loss: 0.4658\n",
            "130/168, train_loss: 0.6331\n",
            "131/168, train_loss: 0.5488\n",
            "132/168, train_loss: 1.1669\n",
            "133/168, train_loss: 0.7440\n",
            "134/168, train_loss: 0.5276\n",
            "135/168, train_loss: 1.0974\n",
            "136/168, train_loss: 0.4916\n",
            "137/168, train_loss: 0.4734\n",
            "138/168, train_loss: 0.3760\n",
            "139/168, train_loss: 0.3818\n",
            "140/168, train_loss: 0.4042\n",
            "141/168, train_loss: 1.2937\n",
            "142/168, train_loss: 0.5049\n",
            "143/168, train_loss: 0.4235\n",
            "144/168, train_loss: 0.4319\n",
            "145/168, train_loss: 0.5278\n",
            "146/168, train_loss: 1.1275\n",
            "147/168, train_loss: 0.7507\n",
            "148/168, train_loss: 0.4254\n",
            "149/168, train_loss: 0.4943\n",
            "150/168, train_loss: 1.0289\n",
            "151/168, train_loss: 1.0997\n",
            "152/168, train_loss: 1.2323\n",
            "153/168, train_loss: 0.9227\n",
            "154/168, train_loss: 0.4227\n",
            "155/168, train_loss: 1.0694\n",
            "156/168, train_loss: 0.3536\n",
            "157/168, train_loss: 0.3172\n",
            "158/168, train_loss: 1.0294\n",
            "159/168, train_loss: 0.4063\n",
            "160/168, train_loss: 1.0080\n",
            "161/168, train_loss: 0.4976\n",
            "162/168, train_loss: 0.5111\n",
            "163/168, train_loss: 0.3987\n",
            "164/168, train_loss: 0.2221\n",
            "165/168, train_loss: 0.3811\n",
            "166/168, train_loss: 0.3331\n",
            "167/168, train_loss: 0.4071\n",
            "168/168, train_loss: 0.5252\n",
            "169/168, train_loss: 0.4603\n",
            "epoch 27 average loss: 0.5540\n",
            "----------\n",
            "epoch 28/5\n",
            "1/168, train_loss: 0.3710\n",
            "2/168, train_loss: 0.4415\n",
            "3/168, train_loss: 0.4288\n",
            "4/168, train_loss: 0.4325\n",
            "5/168, train_loss: 0.4026\n",
            "6/168, train_loss: 0.5651\n",
            "7/168, train_loss: 1.1002\n",
            "8/168, train_loss: 0.3605\n",
            "9/168, train_loss: 0.2082\n",
            "10/168, train_loss: 0.6125\n",
            "11/168, train_loss: 0.2966\n",
            "12/168, train_loss: 0.4209\n",
            "13/168, train_loss: 0.6023\n",
            "14/168, train_loss: 0.7391\n",
            "15/168, train_loss: 0.4459\n",
            "16/168, train_loss: 0.2177\n",
            "17/168, train_loss: 0.2678\n",
            "18/168, train_loss: 0.4361\n",
            "19/168, train_loss: 1.0652\n",
            "20/168, train_loss: 0.4608\n",
            "21/168, train_loss: 0.3080\n",
            "22/168, train_loss: 0.2949\n",
            "23/168, train_loss: 0.4446\n",
            "24/168, train_loss: 0.2800\n",
            "25/168, train_loss: 1.2942\n",
            "26/168, train_loss: 1.0866\n",
            "27/168, train_loss: 0.5332\n",
            "28/168, train_loss: 0.4132\n",
            "29/168, train_loss: 0.4165\n",
            "30/168, train_loss: 0.4123\n",
            "31/168, train_loss: 0.3997\n",
            "32/168, train_loss: 0.4349\n",
            "33/168, train_loss: 0.3916\n",
            "34/168, train_loss: 0.4453\n",
            "35/168, train_loss: 0.3870\n",
            "36/168, train_loss: 0.3908\n",
            "37/168, train_loss: 0.1453\n",
            "38/168, train_loss: 0.2923\n",
            "39/168, train_loss: 0.4524\n",
            "40/168, train_loss: 0.7706\n",
            "41/168, train_loss: 1.0931\n",
            "42/168, train_loss: 0.9543\n",
            "43/168, train_loss: 0.4769\n",
            "44/168, train_loss: 0.1578\n",
            "45/168, train_loss: 0.4881\n",
            "46/168, train_loss: 0.3810\n",
            "47/168, train_loss: 1.0238\n",
            "48/168, train_loss: 0.7595\n",
            "49/168, train_loss: 1.1339\n",
            "50/168, train_loss: 0.1437\n",
            "51/168, train_loss: 0.3815\n",
            "52/168, train_loss: 0.6347\n",
            "53/168, train_loss: 0.3890\n",
            "54/168, train_loss: 1.1806\n",
            "55/168, train_loss: 0.6226\n",
            "56/168, train_loss: 1.0827\n",
            "57/168, train_loss: 0.5915\n",
            "58/168, train_loss: 0.7384\n",
            "59/168, train_loss: 0.5493\n",
            "60/168, train_loss: 0.4765\n",
            "61/168, train_loss: 0.4168\n",
            "62/168, train_loss: 0.3964\n",
            "63/168, train_loss: 0.4032\n",
            "64/168, train_loss: 0.3747\n",
            "65/168, train_loss: 0.4431\n",
            "66/168, train_loss: 1.0098\n",
            "67/168, train_loss: 1.0213\n",
            "68/168, train_loss: 0.4338\n",
            "69/168, train_loss: 0.5196\n",
            "70/168, train_loss: 1.0572\n",
            "71/168, train_loss: 0.3414\n",
            "72/168, train_loss: 0.3373\n",
            "73/168, train_loss: 0.5465\n",
            "74/168, train_loss: 0.3648\n",
            "75/168, train_loss: 0.5420\n",
            "76/168, train_loss: 0.4187\n",
            "77/168, train_loss: 0.5251\n",
            "78/168, train_loss: 0.1463\n",
            "79/168, train_loss: 0.9995\n",
            "80/168, train_loss: 0.4739\n",
            "81/168, train_loss: 0.3952\n",
            "82/168, train_loss: 0.3900\n",
            "83/168, train_loss: 1.1184\n",
            "84/168, train_loss: 0.3883\n",
            "85/168, train_loss: 0.4307\n",
            "86/168, train_loss: 0.1714\n",
            "87/168, train_loss: 0.3905\n",
            "88/168, train_loss: 0.5380\n",
            "89/168, train_loss: 0.4337\n",
            "90/168, train_loss: 0.4353\n",
            "91/168, train_loss: 0.7406\n",
            "92/168, train_loss: 0.4107\n",
            "93/168, train_loss: 0.4841\n",
            "94/168, train_loss: 1.2451\n",
            "95/168, train_loss: 0.1847\n",
            "96/168, train_loss: 0.6227\n",
            "97/168, train_loss: 0.3624\n",
            "98/168, train_loss: 1.0841\n",
            "99/168, train_loss: 0.4311\n",
            "100/168, train_loss: 1.3358\n",
            "101/168, train_loss: 0.3657\n",
            "102/168, train_loss: 0.7268\n",
            "103/168, train_loss: 0.5721\n",
            "104/168, train_loss: 0.4655\n",
            "105/168, train_loss: 0.4446\n",
            "106/168, train_loss: 0.2396\n",
            "107/168, train_loss: 0.7155\n",
            "108/168, train_loss: 0.1904\n",
            "109/168, train_loss: 0.4247\n",
            "110/168, train_loss: 0.7487\n",
            "111/168, train_loss: 0.1853\n",
            "112/168, train_loss: 0.3995\n",
            "113/168, train_loss: 0.4797\n",
            "114/168, train_loss: 0.2769\n",
            "115/168, train_loss: 1.0185\n",
            "116/168, train_loss: 0.3994\n",
            "117/168, train_loss: 1.0839\n",
            "118/168, train_loss: 0.4311\n",
            "119/168, train_loss: 0.4174\n",
            "120/168, train_loss: 0.2490\n",
            "121/168, train_loss: 0.2780\n",
            "122/168, train_loss: 0.5935\n",
            "123/168, train_loss: 0.2836\n",
            "124/168, train_loss: 0.3981\n",
            "125/168, train_loss: 0.2082\n",
            "126/168, train_loss: 0.5682\n",
            "127/168, train_loss: 0.4608\n",
            "128/168, train_loss: 1.0507\n",
            "129/168, train_loss: 0.3082\n",
            "130/168, train_loss: 0.3885\n",
            "131/168, train_loss: 0.3171\n",
            "132/168, train_loss: 0.3985\n",
            "133/168, train_loss: 0.8750\n",
            "134/168, train_loss: 0.5175\n",
            "135/168, train_loss: 0.4312\n",
            "136/168, train_loss: 0.3552\n",
            "137/168, train_loss: 0.4913\n",
            "138/168, train_loss: 0.4443\n",
            "139/168, train_loss: 0.4967\n",
            "140/168, train_loss: 0.5140\n",
            "141/168, train_loss: 0.4203\n",
            "142/168, train_loss: 0.4834\n",
            "143/168, train_loss: 0.3174\n",
            "144/168, train_loss: 1.2743\n",
            "145/168, train_loss: 1.0739\n",
            "146/168, train_loss: 0.7545\n",
            "147/168, train_loss: 0.4084\n",
            "148/168, train_loss: 1.1180\n",
            "149/168, train_loss: 0.6142\n",
            "150/168, train_loss: 0.3309\n",
            "151/168, train_loss: 0.6405\n",
            "152/168, train_loss: 0.2217\n",
            "153/168, train_loss: 0.5228\n",
            "154/168, train_loss: 0.4426\n",
            "155/168, train_loss: 0.7519\n",
            "156/168, train_loss: 1.0780\n",
            "157/168, train_loss: 0.7457\n",
            "158/168, train_loss: 0.4162\n",
            "159/168, train_loss: 0.4328\n",
            "160/168, train_loss: 0.4839\n",
            "161/168, train_loss: 0.4649\n",
            "162/168, train_loss: 0.4101\n",
            "163/168, train_loss: 0.4672\n",
            "164/168, train_loss: 0.9870\n",
            "165/168, train_loss: 0.4391\n",
            "166/168, train_loss: 0.3900\n",
            "167/168, train_loss: 0.7504\n",
            "168/168, train_loss: 1.0585\n",
            "169/168, train_loss: 0.4211\n",
            "epoch 28 average loss: 0.5457\n",
            "current epoch: 28 current accuracy: 0.4118 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 29/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.5252\n",
            "2/168, train_loss: 0.3290\n",
            "3/168, train_loss: 1.0669\n",
            "4/168, train_loss: 0.4891\n",
            "5/168, train_loss: 0.7494\n",
            "6/168, train_loss: 0.5070\n",
            "7/168, train_loss: 0.2419\n",
            "8/168, train_loss: 0.5106\n",
            "9/168, train_loss: 0.1932\n",
            "10/168, train_loss: 0.2589\n",
            "11/168, train_loss: 0.7666\n",
            "12/168, train_loss: 1.0489\n",
            "13/168, train_loss: 0.4321\n",
            "14/168, train_loss: 0.3229\n",
            "15/168, train_loss: 0.9354\n",
            "16/168, train_loss: 0.4090\n",
            "17/168, train_loss: 0.4087\n",
            "18/168, train_loss: 1.0831\n",
            "19/168, train_loss: 0.4573\n",
            "20/168, train_loss: 1.0836\n",
            "21/168, train_loss: 0.4048\n",
            "22/168, train_loss: 0.3501\n",
            "23/168, train_loss: 0.5403\n",
            "24/168, train_loss: 0.4444\n",
            "25/168, train_loss: 0.6052\n",
            "26/168, train_loss: 0.5657\n",
            "27/168, train_loss: 0.4270\n",
            "28/168, train_loss: 0.4865\n",
            "29/168, train_loss: 0.5124\n",
            "30/168, train_loss: 0.4651\n",
            "31/168, train_loss: 0.4915\n",
            "32/168, train_loss: 0.1623\n",
            "33/168, train_loss: 0.2859\n",
            "34/168, train_loss: 0.4066\n",
            "35/168, train_loss: 0.4591\n",
            "36/168, train_loss: 1.0307\n",
            "37/168, train_loss: 0.4575\n",
            "38/168, train_loss: 1.0370\n",
            "39/168, train_loss: 0.4255\n",
            "40/168, train_loss: 0.4124\n",
            "41/168, train_loss: 0.4194\n",
            "42/168, train_loss: 0.7533\n",
            "43/168, train_loss: 0.6370\n",
            "44/168, train_loss: 1.0143\n",
            "45/168, train_loss: 0.7761\n",
            "46/168, train_loss: 0.1735\n",
            "47/168, train_loss: 0.4193\n",
            "48/168, train_loss: 0.4156\n",
            "49/168, train_loss: 0.2207\n",
            "50/168, train_loss: 0.7436\n",
            "51/168, train_loss: 0.9973\n",
            "52/168, train_loss: 0.4683\n",
            "53/168, train_loss: 1.0674\n",
            "54/168, train_loss: 0.4120\n",
            "55/168, train_loss: 0.4238\n",
            "56/168, train_loss: 0.5264\n",
            "57/168, train_loss: 0.2512\n",
            "58/168, train_loss: 0.1846\n",
            "59/168, train_loss: 0.4836\n",
            "60/168, train_loss: 0.5580\n",
            "61/168, train_loss: 0.7357\n",
            "62/168, train_loss: 0.4488\n",
            "63/168, train_loss: 0.4589\n",
            "64/168, train_loss: 0.4681\n",
            "65/168, train_loss: 0.5007\n",
            "66/168, train_loss: 1.0636\n",
            "67/168, train_loss: 0.1827\n",
            "68/168, train_loss: 0.7353\n",
            "69/168, train_loss: 0.5734\n",
            "70/168, train_loss: 1.4891\n",
            "71/168, train_loss: 0.3411\n",
            "72/168, train_loss: 0.5032\n",
            "73/168, train_loss: 0.2509\n",
            "74/168, train_loss: 0.8297\n",
            "75/168, train_loss: 0.4864\n",
            "76/168, train_loss: 0.2861\n",
            "77/168, train_loss: 0.2953\n",
            "78/168, train_loss: 1.0215\n",
            "79/168, train_loss: 0.9570\n",
            "80/168, train_loss: 0.7329\n",
            "81/168, train_loss: 0.7362\n",
            "82/168, train_loss: 0.4262\n",
            "83/168, train_loss: 1.0422\n",
            "84/168, train_loss: 0.4556\n",
            "85/168, train_loss: 0.4642\n",
            "86/168, train_loss: 0.2837\n",
            "87/168, train_loss: 0.3685\n",
            "88/168, train_loss: 0.4627\n",
            "89/168, train_loss: 0.4350\n",
            "90/168, train_loss: 0.5591\n",
            "91/168, train_loss: 0.4193\n",
            "92/168, train_loss: 0.4766\n",
            "93/168, train_loss: 0.2372\n",
            "94/168, train_loss: 0.4136\n",
            "95/168, train_loss: 0.6205\n",
            "96/168, train_loss: 0.4093\n",
            "97/168, train_loss: 0.7396\n",
            "98/168, train_loss: 0.1664\n",
            "99/168, train_loss: 0.2612\n",
            "100/168, train_loss: 0.4859\n",
            "101/168, train_loss: 0.4451\n",
            "102/168, train_loss: 0.4061\n",
            "103/168, train_loss: 0.3437\n",
            "104/168, train_loss: 0.2717\n",
            "105/168, train_loss: 0.4415\n",
            "106/168, train_loss: 0.4243\n",
            "107/168, train_loss: 0.4872\n",
            "108/168, train_loss: 1.0938\n",
            "109/168, train_loss: 0.4456\n",
            "110/168, train_loss: 0.5296\n",
            "111/168, train_loss: 0.5905\n",
            "112/168, train_loss: 0.5953\n",
            "113/168, train_loss: 0.4911\n",
            "114/168, train_loss: 0.2548\n",
            "115/168, train_loss: 0.5264\n",
            "116/168, train_loss: 0.4267\n",
            "117/168, train_loss: 0.5115\n",
            "118/168, train_loss: 0.7554\n",
            "119/168, train_loss: 0.4106\n",
            "120/168, train_loss: 0.5170\n",
            "121/168, train_loss: 0.4454\n",
            "122/168, train_loss: 0.1437\n",
            "123/168, train_loss: 0.7462\n",
            "124/168, train_loss: 0.2787\n",
            "125/168, train_loss: 0.4774\n",
            "126/168, train_loss: 0.7404\n",
            "127/168, train_loss: 0.3219\n",
            "128/168, train_loss: 0.3847\n",
            "129/168, train_loss: 0.7348\n",
            "130/168, train_loss: 0.7320\n",
            "131/168, train_loss: 0.4104\n",
            "132/168, train_loss: 0.2078\n",
            "133/168, train_loss: 0.3922\n",
            "134/168, train_loss: 1.2082\n",
            "135/168, train_loss: 0.5396\n",
            "136/168, train_loss: 0.7281\n",
            "137/168, train_loss: 0.4433\n",
            "138/168, train_loss: 0.2828\n",
            "139/168, train_loss: 0.3840\n",
            "140/168, train_loss: 0.6532\n",
            "141/168, train_loss: 0.7278\n",
            "142/168, train_loss: 0.3947\n",
            "143/168, train_loss: 1.0824\n",
            "144/168, train_loss: 0.5337\n",
            "145/168, train_loss: 0.5977\n",
            "146/168, train_loss: 0.3090\n",
            "147/168, train_loss: 0.4066\n",
            "148/168, train_loss: 0.3990\n",
            "149/168, train_loss: 0.4876\n",
            "150/168, train_loss: 0.4987\n",
            "151/168, train_loss: 0.5321\n",
            "152/168, train_loss: 0.7237\n",
            "153/168, train_loss: 0.2143\n",
            "154/168, train_loss: 1.0952\n",
            "155/168, train_loss: 0.7169\n",
            "156/168, train_loss: 0.3997\n",
            "157/168, train_loss: 0.3941\n",
            "158/168, train_loss: 1.1671\n",
            "159/168, train_loss: 0.4028\n",
            "160/168, train_loss: 0.4764\n",
            "161/168, train_loss: 0.4026\n",
            "162/168, train_loss: 0.3046\n",
            "163/168, train_loss: 0.7205\n",
            "164/168, train_loss: 0.4050\n",
            "165/168, train_loss: 0.3977\n",
            "166/168, train_loss: 1.0519\n",
            "167/168, train_loss: 0.9681\n",
            "168/168, train_loss: 0.3936\n",
            "169/168, train_loss: 0.3938\n",
            "epoch 29 average loss: 0.5407\n",
            "----------\n",
            "epoch 30/5\n",
            "1/168, train_loss: 0.3274\n",
            "2/168, train_loss: 0.3992\n",
            "3/168, train_loss: 0.7125\n",
            "4/168, train_loss: 0.4563\n",
            "5/168, train_loss: 0.7103\n",
            "6/168, train_loss: 0.3962\n",
            "7/168, train_loss: 0.4874\n",
            "8/168, train_loss: 0.7078\n",
            "9/168, train_loss: 0.4683\n",
            "10/168, train_loss: 0.3998\n",
            "11/168, train_loss: 0.4448\n",
            "12/168, train_loss: 1.0821\n",
            "13/168, train_loss: 0.3900\n",
            "14/168, train_loss: 1.2817\n",
            "15/168, train_loss: 0.5380\n",
            "16/168, train_loss: 0.4120\n",
            "17/168, train_loss: 0.4815\n",
            "18/168, train_loss: 0.1856\n",
            "19/168, train_loss: 0.7350\n",
            "20/168, train_loss: 0.9839\n",
            "21/168, train_loss: 1.1610\n",
            "22/168, train_loss: 0.5852\n",
            "23/168, train_loss: 0.3793\n",
            "24/168, train_loss: 0.4384\n",
            "25/168, train_loss: 0.4838\n",
            "26/168, train_loss: 0.5336\n",
            "27/168, train_loss: 0.2638\n",
            "28/168, train_loss: 1.4510\n",
            "29/168, train_loss: 0.5824\n",
            "30/168, train_loss: 0.3149\n",
            "31/168, train_loss: 0.4495\n",
            "32/168, train_loss: 0.3964\n",
            "33/168, train_loss: 1.1536\n",
            "34/168, train_loss: 0.7527\n",
            "35/168, train_loss: 0.5070\n",
            "36/168, train_loss: 0.4227\n",
            "37/168, train_loss: 1.0682\n",
            "38/168, train_loss: 0.7458\n",
            "39/168, train_loss: 0.3225\n",
            "40/168, train_loss: 0.2068\n",
            "41/168, train_loss: 0.1971\n",
            "42/168, train_loss: 0.1670\n",
            "43/168, train_loss: 0.4360\n",
            "44/168, train_loss: 0.3902\n",
            "45/168, train_loss: 0.4685\n",
            "46/168, train_loss: 0.4255\n",
            "47/168, train_loss: 0.4465\n",
            "48/168, train_loss: 0.7349\n",
            "49/168, train_loss: 0.4902\n",
            "50/168, train_loss: 0.3593\n",
            "51/168, train_loss: 0.6486\n",
            "52/168, train_loss: 0.7296\n",
            "53/168, train_loss: 0.5712\n",
            "54/168, train_loss: 0.4679\n",
            "55/168, train_loss: 0.5394\n",
            "56/168, train_loss: 0.2371\n",
            "57/168, train_loss: 0.7358\n",
            "58/168, train_loss: 0.7354\n",
            "59/168, train_loss: 0.4312\n",
            "60/168, train_loss: 0.4500\n",
            "61/168, train_loss: 0.1738\n",
            "62/168, train_loss: 0.7211\n",
            "63/168, train_loss: 1.0674\n",
            "64/168, train_loss: 0.3986\n",
            "65/168, train_loss: 0.3907\n",
            "66/168, train_loss: 0.4382\n",
            "67/168, train_loss: 0.6906\n",
            "68/168, train_loss: 0.2194\n",
            "69/168, train_loss: 0.4000\n",
            "70/168, train_loss: 0.3906\n",
            "71/168, train_loss: 0.4356\n",
            "72/168, train_loss: 0.3968\n",
            "73/168, train_loss: 0.4078\n",
            "74/168, train_loss: 0.3911\n",
            "75/168, train_loss: 1.0770\n",
            "76/168, train_loss: 0.3865\n",
            "77/168, train_loss: 0.3352\n",
            "78/168, train_loss: 0.4200\n",
            "79/168, train_loss: 0.3892\n",
            "80/168, train_loss: 0.3931\n",
            "81/168, train_loss: 1.3586\n",
            "82/168, train_loss: 1.1627\n",
            "83/168, train_loss: 0.7003\n",
            "84/168, train_loss: 0.3889\n",
            "85/168, train_loss: 1.3997\n",
            "86/168, train_loss: 0.6970\n",
            "87/168, train_loss: 0.2027\n",
            "88/168, train_loss: 0.4377\n",
            "89/168, train_loss: 0.3202\n",
            "90/168, train_loss: 0.5005\n",
            "91/168, train_loss: 0.3894\n",
            "92/168, train_loss: 0.3788\n",
            "93/168, train_loss: 0.4587\n",
            "94/168, train_loss: 0.4185\n",
            "95/168, train_loss: 0.4047\n",
            "96/168, train_loss: 1.2078\n",
            "97/168, train_loss: 0.5657\n",
            "98/168, train_loss: 0.5801\n",
            "99/168, train_loss: 0.3841\n",
            "100/168, train_loss: 0.2404\n",
            "101/168, train_loss: 1.1523\n",
            "102/168, train_loss: 0.4822\n",
            "103/168, train_loss: 0.4859\n",
            "104/168, train_loss: 0.1953\n",
            "105/168, train_loss: 0.4379\n",
            "106/168, train_loss: 0.4377\n",
            "107/168, train_loss: 0.6497\n",
            "108/168, train_loss: 0.4466\n",
            "109/168, train_loss: 0.4695\n",
            "110/168, train_loss: 1.0428\n",
            "111/168, train_loss: 1.0243\n",
            "112/168, train_loss: 0.2697\n",
            "113/168, train_loss: 0.5084\n",
            "114/168, train_loss: 0.3891\n",
            "115/168, train_loss: 0.4801\n",
            "116/168, train_loss: 0.3859\n",
            "117/168, train_loss: 0.4556\n",
            "118/168, train_loss: 0.9823\n",
            "119/168, train_loss: 0.3784\n",
            "120/168, train_loss: 0.4228\n",
            "121/168, train_loss: 0.3675\n",
            "122/168, train_loss: 0.9970\n",
            "123/168, train_loss: 0.3198\n",
            "124/168, train_loss: 0.3954\n",
            "125/168, train_loss: 0.7863\n",
            "126/168, train_loss: 0.3942\n",
            "127/168, train_loss: 0.2085\n",
            "128/168, train_loss: 0.7734\n",
            "129/168, train_loss: 0.5276\n",
            "130/168, train_loss: 0.4746\n",
            "131/168, train_loss: 0.5999\n",
            "132/168, train_loss: 0.1706\n",
            "133/168, train_loss: 0.7131\n",
            "134/168, train_loss: 0.4492\n",
            "135/168, train_loss: 0.3093\n",
            "136/168, train_loss: 0.4145\n",
            "137/168, train_loss: 0.3587\n",
            "138/168, train_loss: 0.5833\n",
            "139/168, train_loss: 0.6460\n",
            "140/168, train_loss: 0.4281\n",
            "141/168, train_loss: 1.1048\n",
            "142/168, train_loss: 0.5703\n",
            "143/168, train_loss: 0.3997\n",
            "144/168, train_loss: 0.2822\n",
            "145/168, train_loss: 0.7427\n",
            "146/168, train_loss: 0.4317\n",
            "147/168, train_loss: 0.4530\n",
            "148/168, train_loss: 0.4038\n",
            "149/168, train_loss: 1.0540\n",
            "150/168, train_loss: 0.4233\n",
            "151/168, train_loss: 0.1351\n",
            "152/168, train_loss: 0.4133\n",
            "153/168, train_loss: 1.1037\n",
            "154/168, train_loss: 0.8557\n",
            "155/168, train_loss: 0.4241\n",
            "156/168, train_loss: 1.1818\n",
            "157/168, train_loss: 0.5230\n",
            "158/168, train_loss: 0.2370\n",
            "159/168, train_loss: 0.4538\n",
            "160/168, train_loss: 1.1176\n",
            "161/168, train_loss: 0.3896\n",
            "162/168, train_loss: 0.3777\n",
            "163/168, train_loss: 0.2862\n",
            "164/168, train_loss: 0.3733\n",
            "165/168, train_loss: 0.3893\n",
            "166/168, train_loss: 0.5665\n",
            "167/168, train_loss: 0.1941\n",
            "168/168, train_loss: 1.0222\n",
            "169/168, train_loss: 1.0449\n",
            "epoch 30 average loss: 0.5513\n",
            "current epoch: 30 current accuracy: 0.4706 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 31/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.2963\n",
            "2/168, train_loss: 0.2782\n",
            "3/168, train_loss: 0.5244\n",
            "4/168, train_loss: 0.5776\n",
            "5/168, train_loss: 0.3571\n",
            "6/168, train_loss: 0.7322\n",
            "7/168, train_loss: 0.3944\n",
            "8/168, train_loss: 0.4729\n",
            "9/168, train_loss: 1.0790\n",
            "10/168, train_loss: 0.3940\n",
            "11/168, train_loss: 0.4589\n",
            "12/168, train_loss: 0.5732\n",
            "13/168, train_loss: 0.2455\n",
            "14/168, train_loss: 0.3233\n",
            "15/168, train_loss: 0.7285\n",
            "16/168, train_loss: 0.4676\n",
            "17/168, train_loss: 1.0043\n",
            "18/168, train_loss: 0.4722\n",
            "19/168, train_loss: 0.4027\n",
            "20/168, train_loss: 0.5173\n",
            "21/168, train_loss: 0.5513\n",
            "22/168, train_loss: 0.1628\n",
            "23/168, train_loss: 0.1901\n",
            "24/168, train_loss: 1.2946\n",
            "25/168, train_loss: 0.3733\n",
            "26/168, train_loss: 0.4704\n",
            "27/168, train_loss: 0.4524\n",
            "28/168, train_loss: 0.2373\n",
            "29/168, train_loss: 0.5438\n",
            "30/168, train_loss: 0.1905\n",
            "31/168, train_loss: 1.2198\n",
            "32/168, train_loss: 0.4985\n",
            "33/168, train_loss: 0.7456\n",
            "34/168, train_loss: 0.1856\n",
            "35/168, train_loss: 0.3567\n",
            "36/168, train_loss: 0.4081\n",
            "37/168, train_loss: 0.4014\n",
            "38/168, train_loss: 0.4321\n",
            "39/168, train_loss: 0.7681\n",
            "40/168, train_loss: 0.6369\n",
            "41/168, train_loss: 0.9914\n",
            "42/168, train_loss: 0.4627\n",
            "43/168, train_loss: 0.7504\n",
            "44/168, train_loss: 0.3428\n",
            "45/168, train_loss: 0.4113\n",
            "46/168, train_loss: 0.7458\n",
            "47/168, train_loss: 0.2343\n",
            "48/168, train_loss: 0.3249\n",
            "49/168, train_loss: 0.1477\n",
            "50/168, train_loss: 1.0085\n",
            "51/168, train_loss: 0.1524\n",
            "52/168, train_loss: 1.1486\n",
            "53/168, train_loss: 0.4400\n",
            "54/168, train_loss: 1.1200\n",
            "55/168, train_loss: 0.7339\n",
            "56/168, train_loss: 0.4987\n",
            "57/168, train_loss: 0.4474\n",
            "58/168, train_loss: 0.4633\n",
            "59/168, train_loss: 0.4803\n",
            "60/168, train_loss: 1.0233\n",
            "61/168, train_loss: 0.4107\n",
            "62/168, train_loss: 0.4357\n",
            "63/168, train_loss: 1.0419\n",
            "64/168, train_loss: 0.2618\n",
            "65/168, train_loss: 0.7257\n",
            "66/168, train_loss: 0.4585\n",
            "67/168, train_loss: 0.4102\n",
            "68/168, train_loss: 0.5425\n",
            "69/168, train_loss: 0.4304\n",
            "70/168, train_loss: 0.4852\n",
            "71/168, train_loss: 0.4371\n",
            "72/168, train_loss: 0.5610\n",
            "73/168, train_loss: 0.2145\n",
            "74/168, train_loss: 0.3814\n",
            "75/168, train_loss: 0.2562\n",
            "76/168, train_loss: 0.5097\n",
            "77/168, train_loss: 0.2134\n",
            "78/168, train_loss: 0.4478\n",
            "79/168, train_loss: 0.3786\n",
            "80/168, train_loss: 0.2813\n",
            "81/168, train_loss: 0.4239\n",
            "82/168, train_loss: 0.7489\n",
            "83/168, train_loss: 0.3949\n",
            "84/168, train_loss: 1.0982\n",
            "85/168, train_loss: 0.4201\n",
            "86/168, train_loss: 0.7497\n",
            "87/168, train_loss: 0.3354\n",
            "88/168, train_loss: 0.5211\n",
            "89/168, train_loss: 0.2683\n",
            "90/168, train_loss: 0.4527\n",
            "91/168, train_loss: 0.4774\n",
            "92/168, train_loss: 0.1960\n",
            "93/168, train_loss: 0.4859\n",
            "94/168, train_loss: 0.1641\n",
            "95/168, train_loss: 0.4226\n",
            "96/168, train_loss: 0.4957\n",
            "97/168, train_loss: 0.7434\n",
            "98/168, train_loss: 0.4450\n",
            "99/168, train_loss: 0.7390\n",
            "100/168, train_loss: 0.2800\n",
            "101/168, train_loss: 0.2127\n",
            "102/168, train_loss: 0.4361\n",
            "103/168, train_loss: 0.1002\n",
            "104/168, train_loss: 0.4940\n",
            "105/168, train_loss: 1.0147\n",
            "106/168, train_loss: 0.4325\n",
            "107/168, train_loss: 0.4977\n",
            "108/168, train_loss: 0.1703\n",
            "109/168, train_loss: 0.4444\n",
            "110/168, train_loss: 0.4482\n",
            "111/168, train_loss: 0.4574\n",
            "112/168, train_loss: 0.4225\n",
            "113/168, train_loss: 0.2338\n",
            "114/168, train_loss: 1.0336\n",
            "115/168, train_loss: 0.2955\n",
            "116/168, train_loss: 0.4815\n",
            "117/168, train_loss: 0.3974\n",
            "118/168, train_loss: 0.2600\n",
            "119/168, train_loss: 0.7331\n",
            "120/168, train_loss: 0.2664\n",
            "121/168, train_loss: 0.4413\n",
            "122/168, train_loss: 0.5413\n",
            "123/168, train_loss: 0.4897\n",
            "124/168, train_loss: 0.3337\n",
            "125/168, train_loss: 0.8866\n",
            "126/168, train_loss: 0.3997\n",
            "127/168, train_loss: 1.2120\n",
            "128/168, train_loss: 0.4800\n",
            "129/168, train_loss: 0.1447\n",
            "130/168, train_loss: 1.0160\n",
            "131/168, train_loss: 1.0005\n",
            "132/168, train_loss: 0.3027\n",
            "133/168, train_loss: 0.5766\n",
            "134/168, train_loss: 0.4332\n",
            "135/168, train_loss: 0.3244\n",
            "136/168, train_loss: 0.2184\n",
            "137/168, train_loss: 0.4341\n",
            "138/168, train_loss: 0.4707\n",
            "139/168, train_loss: 0.6048\n",
            "140/168, train_loss: 0.3379\n",
            "141/168, train_loss: 0.1509\n",
            "142/168, train_loss: 0.4820\n",
            "143/168, train_loss: 0.4252\n",
            "144/168, train_loss: 0.2687\n",
            "145/168, train_loss: 0.7042\n",
            "146/168, train_loss: 0.4426\n",
            "147/168, train_loss: 0.9216\n",
            "148/168, train_loss: 0.4256\n",
            "149/168, train_loss: 0.5497\n",
            "150/168, train_loss: 1.0909\n",
            "151/168, train_loss: 0.2274\n",
            "152/168, train_loss: 0.5929\n",
            "153/168, train_loss: 0.3109\n",
            "154/168, train_loss: 0.1261\n",
            "155/168, train_loss: 0.4465\n",
            "156/168, train_loss: 0.4080\n",
            "157/168, train_loss: 0.4234\n",
            "158/168, train_loss: 0.4147\n",
            "159/168, train_loss: 0.4338\n",
            "160/168, train_loss: 0.4196\n",
            "161/168, train_loss: 0.4750\n",
            "162/168, train_loss: 0.4094\n",
            "163/168, train_loss: 1.1191\n",
            "164/168, train_loss: 0.4341\n",
            "165/168, train_loss: 0.6803\n",
            "166/168, train_loss: 0.4222\n",
            "167/168, train_loss: 0.1054\n",
            "168/168, train_loss: 0.3867\n",
            "169/168, train_loss: 1.0506\n",
            "epoch 31 average loss: 0.5025\n",
            "----------\n",
            "epoch 32/5\n",
            "1/168, train_loss: 0.6995\n",
            "2/168, train_loss: 0.4020\n",
            "3/168, train_loss: 0.5439\n",
            "4/168, train_loss: 0.1356\n",
            "5/168, train_loss: 0.4014\n",
            "6/168, train_loss: 0.3891\n",
            "7/168, train_loss: 1.1017\n",
            "8/168, train_loss: 0.4059\n",
            "9/168, train_loss: 0.4355\n",
            "10/168, train_loss: 0.7437\n",
            "11/168, train_loss: 0.2059\n",
            "12/168, train_loss: 1.2246\n",
            "13/168, train_loss: 0.3323\n",
            "14/168, train_loss: 0.0795\n",
            "15/168, train_loss: 0.4147\n",
            "16/168, train_loss: 0.7375\n",
            "17/168, train_loss: 0.2201\n",
            "18/168, train_loss: 0.4515\n",
            "19/168, train_loss: 0.6693\n",
            "20/168, train_loss: 1.1510\n",
            "21/168, train_loss: 0.4560\n",
            "22/168, train_loss: 1.1198\n",
            "23/168, train_loss: 0.3886\n",
            "24/168, train_loss: 1.0689\n",
            "25/168, train_loss: 0.5005\n",
            "26/168, train_loss: 0.2636\n",
            "27/168, train_loss: 1.1945\n",
            "28/168, train_loss: 0.5047\n",
            "29/168, train_loss: 1.1757\n",
            "30/168, train_loss: 0.3575\n",
            "31/168, train_loss: 0.4583\n",
            "32/168, train_loss: 0.6125\n",
            "33/168, train_loss: 0.4494\n",
            "34/168, train_loss: 1.0625\n",
            "35/168, train_loss: 0.4761\n",
            "36/168, train_loss: 1.1007\n",
            "37/168, train_loss: 0.4329\n",
            "38/168, train_loss: 0.4230\n",
            "39/168, train_loss: 0.7497\n",
            "40/168, train_loss: 0.6215\n",
            "41/168, train_loss: 0.4279\n",
            "42/168, train_loss: 0.7523\n",
            "43/168, train_loss: 0.4067\n",
            "44/168, train_loss: 0.3142\n",
            "45/168, train_loss: 0.4529\n",
            "46/168, train_loss: 0.4238\n",
            "47/168, train_loss: 0.5988\n",
            "48/168, train_loss: 0.4092\n",
            "49/168, train_loss: 0.6628\n",
            "50/168, train_loss: 1.0694\n",
            "51/168, train_loss: 0.4055\n",
            "52/168, train_loss: 0.5515\n",
            "53/168, train_loss: 0.4894\n",
            "54/168, train_loss: 0.5241\n",
            "55/168, train_loss: 0.4346\n",
            "56/168, train_loss: 1.1047\n",
            "57/168, train_loss: 0.6263\n",
            "58/168, train_loss: 0.3983\n",
            "59/168, train_loss: 0.3653\n",
            "60/168, train_loss: 0.5022\n",
            "61/168, train_loss: 0.4435\n",
            "62/168, train_loss: 0.4286\n",
            "63/168, train_loss: 0.5232\n",
            "64/168, train_loss: 0.3866\n",
            "65/168, train_loss: 0.3253\n",
            "66/168, train_loss: 0.4610\n",
            "67/168, train_loss: 0.4079\n",
            "68/168, train_loss: 0.4452\n",
            "69/168, train_loss: 0.3475\n",
            "70/168, train_loss: 0.7554\n",
            "71/168, train_loss: 0.5696\n",
            "72/168, train_loss: 0.3274\n",
            "73/168, train_loss: 0.5612\n",
            "74/168, train_loss: 1.0686\n",
            "75/168, train_loss: 1.0325\n",
            "76/168, train_loss: 0.5422\n",
            "77/168, train_loss: 0.3993\n",
            "78/168, train_loss: 1.0632\n",
            "79/168, train_loss: 0.5353\n",
            "80/168, train_loss: 0.4825\n",
            "81/168, train_loss: 0.4276\n",
            "82/168, train_loss: 0.3231\n",
            "83/168, train_loss: 0.4143\n",
            "84/168, train_loss: 0.2455\n",
            "85/168, train_loss: 0.4497\n",
            "86/168, train_loss: 0.9600\n",
            "87/168, train_loss: 0.4353\n",
            "88/168, train_loss: 0.4038\n",
            "89/168, train_loss: 1.0444\n",
            "90/168, train_loss: 0.5575\n",
            "91/168, train_loss: 0.4661\n",
            "92/168, train_loss: 0.4932\n",
            "93/168, train_loss: 0.7495\n",
            "94/168, train_loss: 0.4162\n",
            "95/168, train_loss: 1.0901\n",
            "96/168, train_loss: 0.4227\n",
            "97/168, train_loss: 0.4923\n",
            "98/168, train_loss: 0.9313\n",
            "99/168, train_loss: 0.4931\n",
            "100/168, train_loss: 0.2679\n",
            "101/168, train_loss: 1.0354\n",
            "102/168, train_loss: 0.1237\n",
            "103/168, train_loss: 0.4660\n",
            "104/168, train_loss: 0.2639\n",
            "105/168, train_loss: 0.4697\n",
            "106/168, train_loss: 0.3226\n",
            "107/168, train_loss: 0.4971\n",
            "108/168, train_loss: 0.4376\n",
            "109/168, train_loss: 0.5178\n",
            "110/168, train_loss: 1.0496\n",
            "111/168, train_loss: 0.3065\n",
            "112/168, train_loss: 0.3937\n",
            "113/168, train_loss: 0.0972\n",
            "114/168, train_loss: 1.0210\n",
            "115/168, train_loss: 0.5035\n",
            "116/168, train_loss: 0.4122\n",
            "117/168, train_loss: 0.1715\n",
            "118/168, train_loss: 0.4716\n",
            "119/168, train_loss: 0.3898\n",
            "120/168, train_loss: 0.6780\n",
            "121/168, train_loss: 0.1568\n",
            "122/168, train_loss: 0.4150\n",
            "123/168, train_loss: 0.7713\n",
            "124/168, train_loss: 0.4142\n",
            "125/168, train_loss: 1.1367\n",
            "126/168, train_loss: 0.3471\n",
            "127/168, train_loss: 0.4128\n",
            "128/168, train_loss: 0.3889\n",
            "129/168, train_loss: 1.0963\n",
            "130/168, train_loss: 1.1364\n",
            "131/168, train_loss: 0.4800\n",
            "132/168, train_loss: 0.4248\n",
            "133/168, train_loss: 0.1605\n",
            "134/168, train_loss: 0.7740\n",
            "135/168, train_loss: 0.4377\n",
            "136/168, train_loss: 0.3526\n",
            "137/168, train_loss: 0.7668\n",
            "138/168, train_loss: 0.5021\n",
            "139/168, train_loss: 0.4553\n",
            "140/168, train_loss: 1.0456\n",
            "141/168, train_loss: 0.5778\n",
            "142/168, train_loss: 0.3984\n",
            "143/168, train_loss: 0.4501\n",
            "144/168, train_loss: 1.0029\n",
            "145/168, train_loss: 0.1199\n",
            "146/168, train_loss: 0.1708\n",
            "147/168, train_loss: 0.3454\n",
            "148/168, train_loss: 0.4064\n",
            "149/168, train_loss: 0.2509\n",
            "150/168, train_loss: 1.1509\n",
            "151/168, train_loss: 0.1438\n",
            "152/168, train_loss: 0.1614\n",
            "153/168, train_loss: 0.3969\n",
            "154/168, train_loss: 1.1036\n",
            "155/168, train_loss: 0.4591\n",
            "156/168, train_loss: 0.3937\n",
            "157/168, train_loss: 0.1479\n",
            "158/168, train_loss: 0.7926\n",
            "159/168, train_loss: 0.4170\n",
            "160/168, train_loss: 0.4238\n",
            "161/168, train_loss: 0.4248\n",
            "162/168, train_loss: 0.5320\n",
            "163/168, train_loss: 0.1403\n",
            "164/168, train_loss: 0.3667\n",
            "165/168, train_loss: 0.4346\n",
            "166/168, train_loss: 0.4535\n",
            "167/168, train_loss: 0.1293\n",
            "168/168, train_loss: 0.4859\n",
            "169/168, train_loss: 1.1207\n",
            "epoch 32 average loss: 0.5442\n",
            "current epoch: 32 current accuracy: 0.4235 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 33/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.4203\n",
            "2/168, train_loss: 0.9265\n",
            "3/168, train_loss: 0.1815\n",
            "4/168, train_loss: 0.1737\n",
            "5/168, train_loss: 0.4082\n",
            "6/168, train_loss: 1.5558\n",
            "7/168, train_loss: 0.7440\n",
            "8/168, train_loss: 0.3734\n",
            "9/168, train_loss: 0.4008\n",
            "10/168, train_loss: 0.3385\n",
            "11/168, train_loss: 0.4344\n",
            "12/168, train_loss: 0.1043\n",
            "13/168, train_loss: 0.3039\n",
            "14/168, train_loss: 0.3667\n",
            "15/168, train_loss: 0.2897\n",
            "16/168, train_loss: 1.0366\n",
            "17/168, train_loss: 0.4442\n",
            "18/168, train_loss: 0.4358\n",
            "19/168, train_loss: 0.2492\n",
            "20/168, train_loss: 0.2120\n",
            "21/168, train_loss: 0.3957\n",
            "22/168, train_loss: 1.0889\n",
            "23/168, train_loss: 0.6577\n",
            "24/168, train_loss: 0.7044\n",
            "25/168, train_loss: 0.5133\n",
            "26/168, train_loss: 0.3547\n",
            "27/168, train_loss: 0.3937\n",
            "28/168, train_loss: 1.1023\n",
            "29/168, train_loss: 0.7390\n",
            "30/168, train_loss: 0.4017\n",
            "31/168, train_loss: 0.4735\n",
            "32/168, train_loss: 0.5476\n",
            "33/168, train_loss: 0.5285\n",
            "34/168, train_loss: 0.4431\n",
            "35/168, train_loss: 0.7350\n",
            "36/168, train_loss: 0.3743\n",
            "37/168, train_loss: 1.0444\n",
            "38/168, train_loss: 0.1730\n",
            "39/168, train_loss: 0.4276\n",
            "40/168, train_loss: 0.8911\n",
            "41/168, train_loss: 0.1818\n",
            "42/168, train_loss: 1.3406\n",
            "43/168, train_loss: 0.4698\n",
            "44/168, train_loss: 1.0504\n",
            "45/168, train_loss: 0.4214\n",
            "46/168, train_loss: 0.9226\n",
            "47/168, train_loss: 0.7250\n",
            "48/168, train_loss: 0.4566\n",
            "49/168, train_loss: 0.4049\n",
            "50/168, train_loss: 0.0895\n",
            "51/168, train_loss: 1.1012\n",
            "52/168, train_loss: 0.4816\n",
            "53/168, train_loss: 0.4267\n",
            "54/168, train_loss: 0.4489\n",
            "55/168, train_loss: 1.1643\n",
            "56/168, train_loss: 0.4258\n",
            "57/168, train_loss: 0.4354\n",
            "58/168, train_loss: 0.6028\n",
            "59/168, train_loss: 1.0770\n",
            "60/168, train_loss: 0.5295\n",
            "61/168, train_loss: 0.4858\n",
            "62/168, train_loss: 0.3444\n",
            "63/168, train_loss: 0.3516\n",
            "64/168, train_loss: 0.5816\n",
            "65/168, train_loss: 0.3570\n",
            "66/168, train_loss: 0.3138\n",
            "67/168, train_loss: 0.2643\n",
            "68/168, train_loss: 0.4673\n",
            "69/168, train_loss: 0.4480\n",
            "70/168, train_loss: 0.4902\n",
            "71/168, train_loss: 0.9716\n",
            "72/168, train_loss: 0.6688\n",
            "73/168, train_loss: 1.0738\n",
            "74/168, train_loss: 0.5604\n",
            "75/168, train_loss: 0.5101\n",
            "76/168, train_loss: 0.5862\n",
            "77/168, train_loss: 0.3785\n",
            "78/168, train_loss: 1.0653\n",
            "79/168, train_loss: 0.4168\n",
            "80/168, train_loss: 0.2062\n",
            "81/168, train_loss: 0.5325\n",
            "82/168, train_loss: 0.9831\n",
            "83/168, train_loss: 0.4486\n",
            "84/168, train_loss: 0.6029\n",
            "85/168, train_loss: 0.4279\n",
            "86/168, train_loss: 1.0309\n",
            "87/168, train_loss: 0.4751\n",
            "88/168, train_loss: 0.5936\n",
            "89/168, train_loss: 0.6888\n",
            "90/168, train_loss: 1.0729\n",
            "91/168, train_loss: 0.3967\n",
            "92/168, train_loss: 0.6698\n",
            "93/168, train_loss: 0.4737\n",
            "94/168, train_loss: 0.9942\n",
            "95/168, train_loss: 0.4893\n",
            "96/168, train_loss: 0.5974\n",
            "97/168, train_loss: 0.9990\n",
            "98/168, train_loss: 0.4390\n",
            "99/168, train_loss: 0.4853\n",
            "100/168, train_loss: 0.4728\n",
            "101/168, train_loss: 0.4273\n",
            "102/168, train_loss: 0.4873\n",
            "103/168, train_loss: 0.3700\n",
            "104/168, train_loss: 0.3496\n",
            "105/168, train_loss: 1.0909\n",
            "106/168, train_loss: 0.3927\n",
            "107/168, train_loss: 0.4283\n",
            "108/168, train_loss: 0.2315\n",
            "109/168, train_loss: 0.4679\n",
            "110/168, train_loss: 0.4813\n",
            "111/168, train_loss: 0.7879\n",
            "112/168, train_loss: 0.5099\n",
            "113/168, train_loss: 0.5719\n",
            "114/168, train_loss: 0.1881\n",
            "115/168, train_loss: 0.9722\n",
            "116/168, train_loss: 0.4488\n",
            "117/168, train_loss: 0.4058\n",
            "118/168, train_loss: 0.6176\n",
            "119/168, train_loss: 0.4165\n",
            "120/168, train_loss: 0.9530\n",
            "121/168, train_loss: 0.4188\n",
            "122/168, train_loss: 0.4373\n",
            "123/168, train_loss: 0.3892\n",
            "124/168, train_loss: 0.1630\n",
            "125/168, train_loss: 0.4117\n",
            "126/168, train_loss: 0.4006\n",
            "127/168, train_loss: 0.3964\n",
            "128/168, train_loss: 0.3622\n",
            "129/168, train_loss: 0.4712\n",
            "130/168, train_loss: 0.3771\n",
            "131/168, train_loss: 0.3942\n",
            "132/168, train_loss: 0.1401\n",
            "133/168, train_loss: 1.1973\n",
            "134/168, train_loss: 0.7962\n",
            "135/168, train_loss: 0.1078\n",
            "136/168, train_loss: 0.3885\n",
            "137/168, train_loss: 0.1820\n",
            "138/168, train_loss: 0.4198\n",
            "139/168, train_loss: 0.1428\n",
            "140/168, train_loss: 0.4045\n",
            "141/168, train_loss: 0.3719\n",
            "142/168, train_loss: 0.4573\n",
            "143/168, train_loss: 0.4053\n",
            "144/168, train_loss: 1.1590\n",
            "145/168, train_loss: 1.2255\n",
            "146/168, train_loss: 0.6911\n",
            "147/168, train_loss: 0.0740\n",
            "148/168, train_loss: 0.3150\n",
            "149/168, train_loss: 0.7819\n",
            "150/168, train_loss: 1.2320\n",
            "151/168, train_loss: 0.3661\n",
            "152/168, train_loss: 0.3908\n",
            "153/168, train_loss: 0.3503\n",
            "154/168, train_loss: 0.3889\n",
            "155/168, train_loss: 0.7790\n",
            "156/168, train_loss: 1.0425\n",
            "157/168, train_loss: 0.4196\n",
            "158/168, train_loss: 0.4405\n",
            "159/168, train_loss: 0.4012\n",
            "160/168, train_loss: 0.7689\n",
            "161/168, train_loss: 0.7655\n",
            "162/168, train_loss: 0.3973\n",
            "163/168, train_loss: 1.2996\n",
            "164/168, train_loss: 1.1010\n",
            "165/168, train_loss: 0.7542\n",
            "166/168, train_loss: 0.2140\n",
            "167/168, train_loss: 0.6374\n",
            "168/168, train_loss: 0.4906\n",
            "169/168, train_loss: 1.0534\n",
            "epoch 33 average loss: 0.5617\n",
            "----------\n",
            "epoch 34/5\n",
            "1/168, train_loss: 0.3727\n",
            "2/168, train_loss: 0.4246\n",
            "3/168, train_loss: 0.5593\n",
            "4/168, train_loss: 0.7400\n",
            "5/168, train_loss: 0.3734\n",
            "6/168, train_loss: 1.2075\n",
            "7/168, train_loss: 1.0787\n",
            "8/168, train_loss: 0.4196\n",
            "9/168, train_loss: 0.2393\n",
            "10/168, train_loss: 0.3601\n",
            "11/168, train_loss: 0.3325\n",
            "12/168, train_loss: 0.3761\n",
            "13/168, train_loss: 0.4646\n",
            "14/168, train_loss: 0.7286\n",
            "15/168, train_loss: 0.3926\n",
            "16/168, train_loss: 0.4321\n",
            "17/168, train_loss: 0.4454\n",
            "18/168, train_loss: 0.5564\n",
            "19/168, train_loss: 0.2015\n",
            "20/168, train_loss: 0.9797\n",
            "21/168, train_loss: 0.5592\n",
            "22/168, train_loss: 0.3105\n",
            "23/168, train_loss: 0.3367\n",
            "24/168, train_loss: 0.5806\n",
            "25/168, train_loss: 1.1731\n",
            "26/168, train_loss: 0.2585\n",
            "27/168, train_loss: 0.2125\n",
            "28/168, train_loss: 0.2348\n",
            "29/168, train_loss: 0.2830\n",
            "30/168, train_loss: 0.4732\n",
            "31/168, train_loss: 0.3769\n",
            "32/168, train_loss: 0.5936\n",
            "33/168, train_loss: 0.6761\n",
            "34/168, train_loss: 0.3841\n",
            "35/168, train_loss: 0.7454\n",
            "36/168, train_loss: 0.3827\n",
            "37/168, train_loss: 0.2016\n",
            "38/168, train_loss: 0.5052\n",
            "39/168, train_loss: 0.7457\n",
            "40/168, train_loss: 0.4815\n",
            "41/168, train_loss: 0.3357\n",
            "42/168, train_loss: 0.4464\n",
            "43/168, train_loss: 1.0549\n",
            "44/168, train_loss: 1.0790\n",
            "45/168, train_loss: 0.4080\n",
            "46/168, train_loss: 0.3351\n",
            "47/168, train_loss: 0.7338\n",
            "48/168, train_loss: 0.7317\n",
            "49/168, train_loss: 0.9954\n",
            "50/168, train_loss: 0.3600\n",
            "51/168, train_loss: 0.4840\n",
            "52/168, train_loss: 0.2787\n",
            "53/168, train_loss: 0.3471\n",
            "54/168, train_loss: 0.4470\n",
            "55/168, train_loss: 0.2761\n",
            "56/168, train_loss: 0.3373\n",
            "57/168, train_loss: 0.0900\n",
            "58/168, train_loss: 0.4918\n",
            "59/168, train_loss: 0.4195\n",
            "60/168, train_loss: 0.4172\n",
            "61/168, train_loss: 1.0578\n",
            "62/168, train_loss: 0.4054\n",
            "63/168, train_loss: 0.2634\n",
            "64/168, train_loss: 0.5811\n",
            "65/168, train_loss: 0.3840\n",
            "66/168, train_loss: 0.5400\n",
            "67/168, train_loss: 0.1964\n",
            "68/168, train_loss: 1.2703\n",
            "69/168, train_loss: 0.7039\n",
            "70/168, train_loss: 0.3946\n",
            "71/168, train_loss: 0.5016\n",
            "72/168, train_loss: 0.1433\n",
            "73/168, train_loss: 0.7443\n",
            "74/168, train_loss: 1.4221\n",
            "75/168, train_loss: 0.3652\n",
            "76/168, train_loss: 0.3524\n",
            "77/168, train_loss: 0.4724\n",
            "78/168, train_loss: 1.1879\n",
            "79/168, train_loss: 0.4814\n",
            "80/168, train_loss: 0.1402\n",
            "81/168, train_loss: 0.5465\n",
            "82/168, train_loss: 0.4931\n",
            "83/168, train_loss: 0.1799\n",
            "84/168, train_loss: 0.9918\n",
            "85/168, train_loss: 0.5176\n",
            "86/168, train_loss: 1.0620\n",
            "87/168, train_loss: 0.4238\n",
            "88/168, train_loss: 0.4566\n",
            "89/168, train_loss: 0.2495\n",
            "90/168, train_loss: 0.3978\n",
            "91/168, train_loss: 0.4858\n",
            "92/168, train_loss: 0.3194\n",
            "93/168, train_loss: 0.2086\n",
            "94/168, train_loss: 0.4476\n",
            "95/168, train_loss: 0.5167\n",
            "96/168, train_loss: 0.4761\n",
            "97/168, train_loss: 1.1285\n",
            "98/168, train_loss: 0.4316\n",
            "99/168, train_loss: 0.2791\n",
            "100/168, train_loss: 0.4870\n",
            "101/168, train_loss: 0.3930\n",
            "102/168, train_loss: 0.1619\n",
            "103/168, train_loss: 0.3978\n",
            "104/168, train_loss: 1.3798\n",
            "105/168, train_loss: 0.5687\n",
            "106/168, train_loss: 0.4481\n",
            "107/168, train_loss: 0.3081\n",
            "108/168, train_loss: 0.4481\n",
            "109/168, train_loss: 0.2416\n",
            "110/168, train_loss: 0.4718\n",
            "111/168, train_loss: 0.7891\n",
            "112/168, train_loss: 1.3056\n",
            "113/168, train_loss: 1.0548\n",
            "114/168, train_loss: 0.5041\n",
            "115/168, train_loss: 0.3937\n",
            "116/168, train_loss: 1.2381\n",
            "117/168, train_loss: 0.5486\n",
            "118/168, train_loss: 1.0188\n",
            "119/168, train_loss: 0.4576\n",
            "120/168, train_loss: 0.5283\n",
            "121/168, train_loss: 0.4413\n",
            "122/168, train_loss: 0.3974\n",
            "123/168, train_loss: 1.0551\n",
            "124/168, train_loss: 0.5190\n",
            "125/168, train_loss: 0.5095\n",
            "126/168, train_loss: 0.4153\n",
            "127/168, train_loss: 1.1009\n",
            "128/168, train_loss: 0.4533\n",
            "129/168, train_loss: 0.4354\n",
            "130/168, train_loss: 1.0128\n",
            "131/168, train_loss: 0.0826\n",
            "132/168, train_loss: 0.4090\n",
            "133/168, train_loss: 0.5879\n",
            "134/168, train_loss: 0.7236\n",
            "135/168, train_loss: 0.4634\n",
            "136/168, train_loss: 0.4332\n",
            "137/168, train_loss: 0.1308\n",
            "138/168, train_loss: 0.1490\n",
            "139/168, train_loss: 0.4836\n",
            "140/168, train_loss: 0.7255\n",
            "141/168, train_loss: 0.7274\n",
            "142/168, train_loss: 0.1595\n",
            "143/168, train_loss: 0.4461\n",
            "144/168, train_loss: 1.0080\n",
            "145/168, train_loss: 0.4631\n",
            "146/168, train_loss: 0.1551\n",
            "147/168, train_loss: 0.4426\n",
            "148/168, train_loss: 0.2823\n",
            "149/168, train_loss: 0.2016\n",
            "150/168, train_loss: 0.4108\n",
            "151/168, train_loss: 0.2891\n",
            "152/168, train_loss: 0.4226\n",
            "153/168, train_loss: 1.1086\n",
            "154/168, train_loss: 0.1824\n",
            "155/168, train_loss: 0.4042\n",
            "156/168, train_loss: 0.7579\n",
            "157/168, train_loss: 0.4139\n",
            "158/168, train_loss: 0.2176\n",
            "159/168, train_loss: 0.8281\n",
            "160/168, train_loss: 0.3909\n",
            "161/168, train_loss: 1.2884\n",
            "162/168, train_loss: 0.4633\n",
            "163/168, train_loss: 0.7698\n",
            "164/168, train_loss: 0.4119\n",
            "165/168, train_loss: 0.2402\n",
            "166/168, train_loss: 0.4054\n",
            "167/168, train_loss: 0.5644\n",
            "168/168, train_loss: 0.2768\n",
            "169/168, train_loss: 0.3799\n",
            "epoch 34 average loss: 0.5282\n",
            "current epoch: 34 current accuracy: 0.4235 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 35/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.4031\n",
            "2/168, train_loss: 0.3956\n",
            "3/168, train_loss: 0.4682\n",
            "4/168, train_loss: 0.4063\n",
            "5/168, train_loss: 0.3827\n",
            "6/168, train_loss: 0.3880\n",
            "7/168, train_loss: 0.1851\n",
            "8/168, train_loss: 0.1493\n",
            "9/168, train_loss: 0.2626\n",
            "10/168, train_loss: 0.3708\n",
            "11/168, train_loss: 0.9897\n",
            "12/168, train_loss: 0.7340\n",
            "13/168, train_loss: 0.7836\n",
            "14/168, train_loss: 1.0484\n",
            "15/168, train_loss: 0.2585\n",
            "16/168, train_loss: 1.3673\n",
            "17/168, train_loss: 1.1795\n",
            "18/168, train_loss: 0.3852\n",
            "19/168, train_loss: 0.4991\n",
            "20/168, train_loss: 0.4265\n",
            "21/168, train_loss: 0.5167\n",
            "22/168, train_loss: 1.2906\n",
            "23/168, train_loss: 0.9519\n",
            "24/168, train_loss: 1.1073\n",
            "25/168, train_loss: 0.1272\n",
            "26/168, train_loss: 0.3881\n",
            "27/168, train_loss: 0.4744\n",
            "28/168, train_loss: 0.4695\n",
            "29/168, train_loss: 0.5558\n",
            "30/168, train_loss: 0.7365\n",
            "31/168, train_loss: 0.1577\n",
            "32/168, train_loss: 0.4132\n",
            "33/168, train_loss: 0.1986\n",
            "34/168, train_loss: 0.4470\n",
            "35/168, train_loss: 0.4921\n",
            "36/168, train_loss: 0.4570\n",
            "37/168, train_loss: 0.4647\n",
            "38/168, train_loss: 0.3922\n",
            "39/168, train_loss: 0.3857\n",
            "40/168, train_loss: 0.2746\n",
            "41/168, train_loss: 0.4316\n",
            "42/168, train_loss: 0.3917\n",
            "43/168, train_loss: 0.4172\n",
            "44/168, train_loss: 0.4141\n",
            "45/168, train_loss: 0.4754\n",
            "46/168, train_loss: 0.4084\n",
            "47/168, train_loss: 0.4429\n",
            "48/168, train_loss: 1.0761\n",
            "49/168, train_loss: 0.5093\n",
            "50/168, train_loss: 0.3421\n",
            "51/168, train_loss: 0.1165\n",
            "52/168, train_loss: 0.1686\n",
            "53/168, train_loss: 0.7661\n",
            "54/168, train_loss: 0.4627\n",
            "55/168, train_loss: 0.3857\n",
            "56/168, train_loss: 0.5165\n",
            "57/168, train_loss: 0.3950\n",
            "58/168, train_loss: 0.4931\n",
            "59/168, train_loss: 0.1801\n",
            "60/168, train_loss: 0.5255\n",
            "61/168, train_loss: 1.0874\n",
            "62/168, train_loss: 1.1129\n",
            "63/168, train_loss: 1.0462\n",
            "64/168, train_loss: 1.1792\n",
            "65/168, train_loss: 1.0872\n",
            "66/168, train_loss: 0.3689\n",
            "67/168, train_loss: 0.2246\n",
            "68/168, train_loss: 0.3789\n",
            "69/168, train_loss: 0.4103\n",
            "70/168, train_loss: 0.1553\n",
            "71/168, train_loss: 0.5600\n",
            "72/168, train_loss: 1.0671\n",
            "73/168, train_loss: 0.4507\n",
            "74/168, train_loss: 0.3955\n",
            "75/168, train_loss: 0.3719\n",
            "76/168, train_loss: 0.2987\n",
            "77/168, train_loss: 0.4360\n",
            "78/168, train_loss: 0.9957\n",
            "79/168, train_loss: 0.3916\n",
            "80/168, train_loss: 0.4153\n",
            "81/168, train_loss: 0.7491\n",
            "82/168, train_loss: 0.5754\n",
            "83/168, train_loss: 1.0195\n",
            "84/168, train_loss: 0.5952\n",
            "85/168, train_loss: 0.4781\n",
            "86/168, train_loss: 0.3051\n",
            "87/168, train_loss: 0.1276\n",
            "88/168, train_loss: 0.4785\n",
            "89/168, train_loss: 0.1570\n",
            "90/168, train_loss: 1.0971\n",
            "91/168, train_loss: 0.3690\n",
            "92/168, train_loss: 0.1565\n",
            "93/168, train_loss: 0.2788\n",
            "94/168, train_loss: 0.5429\n",
            "95/168, train_loss: 0.7406\n",
            "96/168, train_loss: 0.6566\n",
            "97/168, train_loss: 0.1958\n",
            "98/168, train_loss: 0.3984\n",
            "99/168, train_loss: 0.7337\n",
            "100/168, train_loss: 0.4046\n",
            "101/168, train_loss: 0.3445\n",
            "102/168, train_loss: 0.5533\n",
            "103/168, train_loss: 0.6377\n",
            "104/168, train_loss: 0.9863\n",
            "105/168, train_loss: 0.2890\n",
            "106/168, train_loss: 0.4978\n",
            "107/168, train_loss: 0.4859\n",
            "108/168, train_loss: 0.4158\n",
            "109/168, train_loss: 1.0397\n",
            "110/168, train_loss: 0.2143\n",
            "111/168, train_loss: 0.3969\n",
            "112/168, train_loss: 0.2569\n",
            "113/168, train_loss: 0.4017\n",
            "114/168, train_loss: 1.0603\n",
            "115/168, train_loss: 0.4397\n",
            "116/168, train_loss: 0.6295\n",
            "117/168, train_loss: 0.7437\n",
            "118/168, train_loss: 1.0508\n",
            "119/168, train_loss: 0.4035\n",
            "120/168, train_loss: 0.2795\n",
            "121/168, train_loss: 0.5189\n",
            "122/168, train_loss: 0.3798\n",
            "123/168, train_loss: 0.3787\n",
            "124/168, train_loss: 0.4039\n",
            "125/168, train_loss: 0.3960\n",
            "126/168, train_loss: 0.4914\n",
            "127/168, train_loss: 0.3893\n",
            "128/168, train_loss: 0.7440\n",
            "129/168, train_loss: 0.5307\n",
            "130/168, train_loss: 0.7399\n",
            "131/168, train_loss: 0.7370\n",
            "132/168, train_loss: 0.2004\n",
            "133/168, train_loss: 0.4528\n",
            "134/168, train_loss: 1.1150\n",
            "135/168, train_loss: 0.3761\n",
            "136/168, train_loss: 0.2009\n",
            "137/168, train_loss: 0.2146\n",
            "138/168, train_loss: 0.7269\n",
            "139/168, train_loss: 0.3840\n",
            "140/168, train_loss: 0.7239\n",
            "141/168, train_loss: 0.5630\n",
            "142/168, train_loss: 0.5600\n",
            "143/168, train_loss: 0.5445\n",
            "144/168, train_loss: 0.5737\n",
            "145/168, train_loss: 0.4691\n",
            "146/168, train_loss: 0.2950\n",
            "147/168, train_loss: 0.1833\n",
            "148/168, train_loss: 0.4958\n",
            "149/168, train_loss: 1.0963\n",
            "150/168, train_loss: 0.4733\n",
            "151/168, train_loss: 0.4561\n",
            "152/168, train_loss: 0.1343\n",
            "153/168, train_loss: 0.5282\n",
            "154/168, train_loss: 0.4222\n",
            "155/168, train_loss: 0.5101\n",
            "156/168, train_loss: 0.4216\n",
            "157/168, train_loss: 0.3956\n",
            "158/168, train_loss: 0.7586\n",
            "159/168, train_loss: 0.2283\n",
            "160/168, train_loss: 0.4339\n",
            "161/168, train_loss: 0.7554\n",
            "162/168, train_loss: 0.3911\n",
            "163/168, train_loss: 1.1818\n",
            "164/168, train_loss: 0.4157\n",
            "165/168, train_loss: 1.0943\n",
            "166/168, train_loss: 0.4229\n",
            "167/168, train_loss: 0.1463\n",
            "168/168, train_loss: 1.1112\n",
            "169/168, train_loss: 0.4211\n",
            "epoch 35 average loss: 0.5296\n",
            "----------\n",
            "epoch 36/5\n",
            "1/168, train_loss: 0.4215\n",
            "2/168, train_loss: 0.1293\n",
            "3/168, train_loss: 0.5785\n",
            "4/168, train_loss: 0.3010\n",
            "5/168, train_loss: 0.1002\n",
            "6/168, train_loss: 0.1667\n",
            "7/168, train_loss: 0.2774\n",
            "8/168, train_loss: 0.4144\n",
            "9/168, train_loss: 0.4090\n",
            "10/168, train_loss: 0.7551\n",
            "11/168, train_loss: 0.3858\n",
            "12/168, train_loss: 0.4214\n",
            "13/168, train_loss: 1.1093\n",
            "14/168, train_loss: 0.5587\n",
            "15/168, train_loss: 0.5164\n",
            "16/168, train_loss: 1.0840\n",
            "17/168, train_loss: 0.4467\n",
            "18/168, train_loss: 0.5079\n",
            "19/168, train_loss: 0.4207\n",
            "20/168, train_loss: 0.7467\n",
            "21/168, train_loss: 0.4080\n",
            "22/168, train_loss: 0.1124\n",
            "23/168, train_loss: 0.1411\n",
            "24/168, train_loss: 0.7408\n",
            "25/168, train_loss: 1.3684\n",
            "26/168, train_loss: 1.0765\n",
            "27/168, train_loss: 0.3762\n",
            "28/168, train_loss: 0.4126\n",
            "29/168, train_loss: 0.3939\n",
            "30/168, train_loss: 0.4501\n",
            "31/168, train_loss: 1.0759\n",
            "32/168, train_loss: 0.4198\n",
            "33/168, train_loss: 1.0589\n",
            "34/168, train_loss: 0.4620\n",
            "35/168, train_loss: 0.6111\n",
            "36/168, train_loss: 0.3866\n",
            "37/168, train_loss: 0.5400\n",
            "38/168, train_loss: 0.4480\n",
            "39/168, train_loss: 1.0151\n",
            "40/168, train_loss: 0.1240\n",
            "41/168, train_loss: 1.0192\n",
            "42/168, train_loss: 0.3967\n",
            "43/168, train_loss: 0.7233\n",
            "44/168, train_loss: 0.3815\n",
            "45/168, train_loss: 1.0785\n",
            "46/168, train_loss: 0.2805\n",
            "47/168, train_loss: 0.7236\n",
            "48/168, train_loss: 0.2854\n",
            "49/168, train_loss: 0.4593\n",
            "50/168, train_loss: 0.6359\n",
            "51/168, train_loss: 0.4236\n",
            "52/168, train_loss: 0.4781\n",
            "53/168, train_loss: 0.5831\n",
            "54/168, train_loss: 0.5630\n",
            "55/168, train_loss: 0.2104\n",
            "56/168, train_loss: 0.4747\n",
            "57/168, train_loss: 0.5338\n",
            "58/168, train_loss: 0.1399\n",
            "59/168, train_loss: 0.3725\n",
            "60/168, train_loss: 0.1668\n",
            "61/168, train_loss: 0.4043\n",
            "62/168, train_loss: 0.4437\n",
            "63/168, train_loss: 0.4691\n",
            "64/168, train_loss: 0.3685\n",
            "65/168, train_loss: 1.0212\n",
            "66/168, train_loss: 0.1494\n",
            "67/168, train_loss: 0.1607\n",
            "68/168, train_loss: 0.4153\n",
            "69/168, train_loss: 0.4362\n",
            "70/168, train_loss: 0.4100\n",
            "71/168, train_loss: 0.3499\n",
            "72/168, train_loss: 0.3536\n",
            "73/168, train_loss: 0.3843\n",
            "74/168, train_loss: 0.3845\n",
            "75/168, train_loss: 0.4858\n",
            "76/168, train_loss: 0.4163\n",
            "77/168, train_loss: 0.4493\n",
            "78/168, train_loss: 0.4805\n",
            "79/168, train_loss: 0.3922\n",
            "80/168, train_loss: 0.1406\n",
            "81/168, train_loss: 0.1451\n",
            "82/168, train_loss: 0.4266\n",
            "83/168, train_loss: 0.7212\n",
            "84/168, train_loss: 0.4523\n",
            "85/168, train_loss: 0.2705\n",
            "86/168, train_loss: 0.5000\n",
            "87/168, train_loss: 0.4440\n",
            "88/168, train_loss: 0.1299\n",
            "89/168, train_loss: 0.4405\n",
            "90/168, train_loss: 0.7458\n",
            "91/168, train_loss: 0.3613\n",
            "92/168, train_loss: 0.4743\n",
            "93/168, train_loss: 0.1462\n",
            "94/168, train_loss: 0.4159\n",
            "95/168, train_loss: 0.5235\n",
            "96/168, train_loss: 0.4120\n",
            "97/168, train_loss: 0.4008\n",
            "98/168, train_loss: 0.2438\n",
            "99/168, train_loss: 1.2262\n",
            "100/168, train_loss: 1.1211\n",
            "101/168, train_loss: 0.1870\n",
            "102/168, train_loss: 1.2211\n",
            "103/168, train_loss: 0.7192\n",
            "104/168, train_loss: 0.7569\n",
            "105/168, train_loss: 0.4013\n",
            "106/168, train_loss: 1.6265\n",
            "107/168, train_loss: 1.0453\n",
            "108/168, train_loss: 0.4292\n",
            "109/168, train_loss: 0.4386\n",
            "110/168, train_loss: 1.1926\n",
            "111/168, train_loss: 0.4565\n",
            "112/168, train_loss: 0.4887\n",
            "113/168, train_loss: 0.1634\n",
            "114/168, train_loss: 0.4726\n",
            "115/168, train_loss: 0.4975\n",
            "116/168, train_loss: 0.5093\n",
            "117/168, train_loss: 0.3066\n",
            "118/168, train_loss: 0.3795\n",
            "119/168, train_loss: 0.5078\n",
            "120/168, train_loss: 0.6514\n",
            "121/168, train_loss: 0.4734\n",
            "122/168, train_loss: 0.4982\n",
            "123/168, train_loss: 0.2691\n",
            "124/168, train_loss: 0.4187\n",
            "125/168, train_loss: 1.0506\n",
            "126/168, train_loss: 1.4369\n",
            "127/168, train_loss: 0.4497\n",
            "128/168, train_loss: 0.7353\n",
            "129/168, train_loss: 0.5997\n",
            "130/168, train_loss: 0.5871\n",
            "131/168, train_loss: 0.4261\n",
            "132/168, train_loss: 0.3744\n",
            "133/168, train_loss: 1.0412\n",
            "134/168, train_loss: 0.4437\n",
            "135/168, train_loss: 0.3803\n",
            "136/168, train_loss: 0.5792\n",
            "137/168, train_loss: 0.5186\n",
            "138/168, train_loss: 0.6793\n",
            "139/168, train_loss: 0.4617\n",
            "140/168, train_loss: 1.2245\n",
            "141/168, train_loss: 0.5298\n",
            "142/168, train_loss: 0.4385\n",
            "143/168, train_loss: 0.4823\n",
            "144/168, train_loss: 0.1283\n",
            "145/168, train_loss: 0.2548\n",
            "146/168, train_loss: 1.0869\n",
            "147/168, train_loss: 0.3803\n",
            "148/168, train_loss: 0.1392\n",
            "149/168, train_loss: 0.4206\n",
            "150/168, train_loss: 0.4659\n",
            "151/168, train_loss: 0.4367\n",
            "152/168, train_loss: 0.1161\n",
            "153/168, train_loss: 0.3706\n",
            "154/168, train_loss: 1.2160\n",
            "155/168, train_loss: 0.3715\n",
            "156/168, train_loss: 1.0473\n",
            "157/168, train_loss: 0.1574\n",
            "158/168, train_loss: 1.1171\n",
            "159/168, train_loss: 0.7478\n",
            "160/168, train_loss: 0.3785\n",
            "161/168, train_loss: 1.1402\n",
            "162/168, train_loss: 0.2428\n",
            "163/168, train_loss: 0.1934\n",
            "164/168, train_loss: 0.3866\n",
            "165/168, train_loss: 0.7350\n",
            "166/168, train_loss: 0.4592\n",
            "167/168, train_loss: 0.4531\n",
            "168/168, train_loss: 0.7344\n",
            "169/168, train_loss: 0.3685\n",
            "epoch 36 average loss: 0.5273\n",
            "current epoch: 36 current accuracy: 0.4235 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 37/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.6079\n",
            "2/168, train_loss: 0.1733\n",
            "3/168, train_loss: 0.3975\n",
            "4/168, train_loss: 0.8480\n",
            "5/168, train_loss: 1.0365\n",
            "6/168, train_loss: 0.1423\n",
            "7/168, train_loss: 0.4039\n",
            "8/168, train_loss: 0.3726\n",
            "9/168, train_loss: 0.3546\n",
            "10/168, train_loss: 0.3732\n",
            "11/168, train_loss: 0.1345\n",
            "12/168, train_loss: 0.4693\n",
            "13/168, train_loss: 0.3620\n",
            "14/168, train_loss: 0.1255\n",
            "15/168, train_loss: 0.4085\n",
            "16/168, train_loss: 0.3490\n",
            "17/168, train_loss: 0.6300\n",
            "18/168, train_loss: 0.7293\n",
            "19/168, train_loss: 0.3926\n",
            "20/168, train_loss: 0.4864\n",
            "21/168, train_loss: 0.7258\n",
            "22/168, train_loss: 0.2704\n",
            "23/168, train_loss: 0.7224\n",
            "24/168, train_loss: 0.2006\n",
            "25/168, train_loss: 1.1213\n",
            "26/168, train_loss: 0.3669\n",
            "27/168, train_loss: 0.3729\n",
            "28/168, train_loss: 0.7164\n",
            "29/168, train_loss: 0.3900\n",
            "30/168, train_loss: 0.3600\n",
            "31/168, train_loss: 0.4296\n",
            "32/168, train_loss: 0.1046\n",
            "33/168, train_loss: 0.1742\n",
            "34/168, train_loss: 0.5131\n",
            "35/168, train_loss: 0.3613\n",
            "36/168, train_loss: 0.3776\n",
            "37/168, train_loss: 0.3539\n",
            "38/168, train_loss: 0.6777\n",
            "39/168, train_loss: 0.3511\n",
            "40/168, train_loss: 0.7260\n",
            "41/168, train_loss: 0.3600\n",
            "42/168, train_loss: 0.3707\n",
            "43/168, train_loss: 0.6356\n",
            "44/168, train_loss: 0.4656\n",
            "45/168, train_loss: 0.4534\n",
            "46/168, train_loss: 0.2045\n",
            "47/168, train_loss: 0.3484\n",
            "48/168, train_loss: 0.1925\n",
            "49/168, train_loss: 0.1000\n",
            "50/168, train_loss: 0.2340\n",
            "51/168, train_loss: 0.7431\n",
            "52/168, train_loss: 0.7564\n",
            "53/168, train_loss: 0.1706\n",
            "54/168, train_loss: 1.1103\n",
            "55/168, train_loss: 0.1846\n",
            "56/168, train_loss: 0.3650\n",
            "57/168, train_loss: 0.3527\n",
            "58/168, train_loss: 0.2263\n",
            "59/168, train_loss: 1.0591\n",
            "60/168, train_loss: 0.9304\n",
            "61/168, train_loss: 0.4131\n",
            "62/168, train_loss: 0.4519\n",
            "63/168, train_loss: 0.3710\n",
            "64/168, train_loss: 0.1248\n",
            "65/168, train_loss: 0.3588\n",
            "66/168, train_loss: 0.4629\n",
            "67/168, train_loss: 0.0994\n",
            "68/168, train_loss: 0.4307\n",
            "69/168, train_loss: 0.5321\n",
            "70/168, train_loss: 1.0908\n",
            "71/168, train_loss: 1.0654\n",
            "72/168, train_loss: 1.1543\n",
            "73/168, train_loss: 0.4575\n",
            "74/168, train_loss: 0.7310\n",
            "75/168, train_loss: 0.4593\n",
            "76/168, train_loss: 0.1873\n",
            "77/168, train_loss: 0.3231\n",
            "78/168, train_loss: 0.4172\n",
            "79/168, train_loss: 0.1750\n",
            "80/168, train_loss: 1.2775\n",
            "81/168, train_loss: 0.7311\n",
            "82/168, train_loss: 1.1333\n",
            "83/168, train_loss: 0.3847\n",
            "84/168, train_loss: 0.3760\n",
            "85/168, train_loss: 0.4069\n",
            "86/168, train_loss: 0.9810\n",
            "87/168, train_loss: 0.3979\n",
            "88/168, train_loss: 0.1583\n",
            "89/168, train_loss: 0.3753\n",
            "90/168, train_loss: 1.0794\n",
            "91/168, train_loss: 0.1405\n",
            "92/168, train_loss: 0.5034\n",
            "93/168, train_loss: 0.3973\n",
            "94/168, train_loss: 0.0800\n",
            "95/168, train_loss: 0.3717\n",
            "96/168, train_loss: 0.6918\n",
            "97/168, train_loss: 0.2788\n",
            "98/168, train_loss: 0.3784\n",
            "99/168, train_loss: 0.9597\n",
            "100/168, train_loss: 0.4214\n",
            "101/168, train_loss: 0.4879\n",
            "102/168, train_loss: 0.4316\n",
            "103/168, train_loss: 1.0034\n",
            "104/168, train_loss: 0.3794\n",
            "105/168, train_loss: 0.4096\n",
            "106/168, train_loss: 0.4958\n",
            "107/168, train_loss: 0.2500\n",
            "108/168, train_loss: 1.3486\n",
            "109/168, train_loss: 0.3567\n",
            "110/168, train_loss: 0.4413\n",
            "111/168, train_loss: 0.4038\n",
            "112/168, train_loss: 1.1079\n",
            "113/168, train_loss: 0.4498\n",
            "114/168, train_loss: 0.2477\n",
            "115/168, train_loss: 0.3783\n",
            "116/168, train_loss: 0.4362\n",
            "117/168, train_loss: 0.3847\n",
            "118/168, train_loss: 0.4283\n",
            "119/168, train_loss: 1.4036\n",
            "120/168, train_loss: 0.5717\n",
            "121/168, train_loss: 0.1314\n",
            "122/168, train_loss: 0.4593\n",
            "123/168, train_loss: 1.0492\n",
            "124/168, train_loss: 0.5132\n",
            "125/168, train_loss: 0.6635\n",
            "126/168, train_loss: 0.3301\n",
            "127/168, train_loss: 1.0318\n",
            "128/168, train_loss: 1.0934\n",
            "129/168, train_loss: 0.1520\n",
            "130/168, train_loss: 0.3426\n",
            "131/168, train_loss: 0.0748\n",
            "132/168, train_loss: 0.5054\n",
            "133/168, train_loss: 0.4225\n",
            "134/168, train_loss: 0.2878\n",
            "135/168, train_loss: 0.1412\n",
            "136/168, train_loss: 1.0221\n",
            "137/168, train_loss: 0.4543\n",
            "138/168, train_loss: 0.5159\n",
            "139/168, train_loss: 0.4344\n",
            "140/168, train_loss: 0.4079\n",
            "141/168, train_loss: 0.1561\n",
            "142/168, train_loss: 0.1983\n",
            "143/168, train_loss: 0.1573\n",
            "144/168, train_loss: 0.3783\n",
            "145/168, train_loss: 0.7102\n",
            "146/168, train_loss: 1.0976\n",
            "147/168, train_loss: 0.4858\n",
            "148/168, train_loss: 0.5925\n",
            "149/168, train_loss: 0.4262\n",
            "150/168, train_loss: 0.3867\n",
            "151/168, train_loss: 0.7415\n",
            "152/168, train_loss: 0.4206\n",
            "153/168, train_loss: 0.3784\n",
            "154/168, train_loss: 0.7376\n",
            "155/168, train_loss: 0.6311\n",
            "156/168, train_loss: 0.4593\n",
            "157/168, train_loss: 0.3016\n",
            "158/168, train_loss: 0.3927\n",
            "159/168, train_loss: 0.3294\n",
            "160/168, train_loss: 0.3937\n",
            "161/168, train_loss: 1.1635\n",
            "162/168, train_loss: 0.8130\n",
            "163/168, train_loss: 0.7310\n",
            "164/168, train_loss: 0.4503\n",
            "165/168, train_loss: 0.5335\n",
            "166/168, train_loss: 0.4287\n",
            "167/168, train_loss: 0.3974\n",
            "168/168, train_loss: 0.2567\n",
            "169/168, train_loss: 0.4707\n",
            "epoch 37 average loss: 0.4987\n",
            "----------\n",
            "epoch 38/5\n",
            "1/168, train_loss: 0.2935\n",
            "2/168, train_loss: 1.1756\n",
            "3/168, train_loss: 0.9976\n",
            "4/168, train_loss: 1.0561\n",
            "5/168, train_loss: 0.7360\n",
            "6/168, train_loss: 0.9789\n",
            "7/168, train_loss: 0.3749\n",
            "8/168, train_loss: 0.1714\n",
            "9/168, train_loss: 0.7330\n",
            "10/168, train_loss: 0.6626\n",
            "11/168, train_loss: 0.4259\n",
            "12/168, train_loss: 0.4170\n",
            "13/168, train_loss: 0.2452\n",
            "14/168, train_loss: 1.2723\n",
            "15/168, train_loss: 0.0973\n",
            "16/168, train_loss: 0.6203\n",
            "17/168, train_loss: 0.4001\n",
            "18/168, train_loss: 0.5116\n",
            "19/168, train_loss: 0.6091\n",
            "20/168, train_loss: 0.6327\n",
            "21/168, train_loss: 0.4960\n",
            "22/168, train_loss: 0.7250\n",
            "23/168, train_loss: 0.7238\n",
            "24/168, train_loss: 0.1199\n",
            "25/168, train_loss: 0.4852\n",
            "26/168, train_loss: 0.2239\n",
            "27/168, train_loss: 0.4719\n",
            "28/168, train_loss: 0.5032\n",
            "29/168, train_loss: 0.3868\n",
            "30/168, train_loss: 1.2676\n",
            "31/168, train_loss: 0.4379\n",
            "32/168, train_loss: 0.3839\n",
            "33/168, train_loss: 0.8491\n",
            "34/168, train_loss: 0.1516\n",
            "35/168, train_loss: 1.0663\n",
            "36/168, train_loss: 0.7380\n",
            "37/168, train_loss: 1.1424\n",
            "38/168, train_loss: 1.0439\n",
            "39/168, train_loss: 0.3813\n",
            "40/168, train_loss: 0.4154\n",
            "41/168, train_loss: 0.4157\n",
            "42/168, train_loss: 0.1599\n",
            "43/168, train_loss: 0.3915\n",
            "44/168, train_loss: 0.4191\n",
            "45/168, train_loss: 0.1943\n",
            "46/168, train_loss: 0.4658\n",
            "47/168, train_loss: 0.4230\n",
            "48/168, train_loss: 0.3731\n",
            "49/168, train_loss: 0.3970\n",
            "50/168, train_loss: 0.2021\n",
            "51/168, train_loss: 0.7045\n",
            "52/168, train_loss: 1.2037\n",
            "53/168, train_loss: 0.3841\n",
            "54/168, train_loss: 0.4485\n",
            "55/168, train_loss: 0.3640\n",
            "56/168, train_loss: 0.1794\n",
            "57/168, train_loss: 0.4003\n",
            "58/168, train_loss: 0.4153\n",
            "59/168, train_loss: 1.1081\n",
            "60/168, train_loss: 0.1535\n",
            "61/168, train_loss: 1.0661\n",
            "62/168, train_loss: 0.1698\n",
            "63/168, train_loss: 0.8020\n",
            "64/168, train_loss: 1.0353\n",
            "65/168, train_loss: 0.3818\n",
            "66/168, train_loss: 0.1737\n",
            "67/168, train_loss: 0.6762\n",
            "68/168, train_loss: 0.4049\n",
            "69/168, train_loss: 0.2724\n",
            "70/168, train_loss: 0.4928\n",
            "71/168, train_loss: 1.2042\n",
            "72/168, train_loss: 0.4556\n",
            "73/168, train_loss: 0.4182\n",
            "74/168, train_loss: 0.3916\n",
            "75/168, train_loss: 1.0631\n",
            "76/168, train_loss: 0.7317\n",
            "77/168, train_loss: 0.3729\n",
            "78/168, train_loss: 0.4153\n",
            "79/168, train_loss: 0.3442\n",
            "80/168, train_loss: 0.3894\n",
            "81/168, train_loss: 0.9714\n",
            "82/168, train_loss: 1.0044\n",
            "83/168, train_loss: 0.1409\n",
            "84/168, train_loss: 0.7715\n",
            "85/168, train_loss: 0.4803\n",
            "86/168, train_loss: 0.2284\n",
            "87/168, train_loss: 0.7648\n",
            "88/168, train_loss: 0.3892\n",
            "89/168, train_loss: 0.9014\n",
            "90/168, train_loss: 0.1425\n",
            "91/168, train_loss: 0.9696\n",
            "92/168, train_loss: 0.4396\n",
            "93/168, train_loss: 0.3275\n",
            "94/168, train_loss: 0.1669\n",
            "95/168, train_loss: 0.7643\n",
            "96/168, train_loss: 0.4105\n",
            "97/168, train_loss: 0.3717\n",
            "98/168, train_loss: 0.1335\n",
            "99/168, train_loss: 0.3834\n",
            "100/168, train_loss: 0.4895\n",
            "101/168, train_loss: 1.1042\n",
            "102/168, train_loss: 0.2417\n",
            "103/168, train_loss: 0.4279\n",
            "104/168, train_loss: 0.4253\n",
            "105/168, train_loss: 0.5426\n",
            "106/168, train_loss: 0.3986\n",
            "107/168, train_loss: 0.3574\n",
            "108/168, train_loss: 1.2232\n",
            "109/168, train_loss: 0.1690\n",
            "110/168, train_loss: 0.7036\n",
            "111/168, train_loss: 0.4355\n",
            "112/168, train_loss: 0.2229\n",
            "113/168, train_loss: 0.0905\n",
            "114/168, train_loss: 0.4164\n",
            "115/168, train_loss: 0.7490\n",
            "116/168, train_loss: 1.1454\n",
            "117/168, train_loss: 0.3961\n",
            "118/168, train_loss: 0.4242\n",
            "119/168, train_loss: 0.4114\n",
            "120/168, train_loss: 0.4853\n",
            "121/168, train_loss: 0.2341\n",
            "122/168, train_loss: 0.4054\n",
            "123/168, train_loss: 0.7529\n",
            "124/168, train_loss: 0.4326\n",
            "125/168, train_loss: 0.4545\n",
            "126/168, train_loss: 0.3901\n",
            "127/168, train_loss: 0.4853\n",
            "128/168, train_loss: 0.6604\n",
            "129/168, train_loss: 0.1908\n",
            "130/168, train_loss: 0.4388\n",
            "131/168, train_loss: 0.3857\n",
            "132/168, train_loss: 0.4381\n",
            "133/168, train_loss: 0.4368\n",
            "134/168, train_loss: 0.5500\n",
            "135/168, train_loss: 0.3944\n",
            "136/168, train_loss: 0.5238\n",
            "137/168, train_loss: 0.3076\n",
            "138/168, train_loss: 0.7516\n",
            "139/168, train_loss: 0.7494\n",
            "140/168, train_loss: 0.5400\n",
            "141/168, train_loss: 0.3622\n",
            "142/168, train_loss: 1.0771\n",
            "143/168, train_loss: 0.4473\n",
            "144/168, train_loss: 0.4607\n",
            "145/168, train_loss: 0.5222\n",
            "146/168, train_loss: 1.0220\n",
            "147/168, train_loss: 0.4337\n",
            "148/168, train_loss: 1.2803\n",
            "149/168, train_loss: 0.2787\n",
            "150/168, train_loss: 0.4046\n",
            "151/168, train_loss: 0.4613\n",
            "152/168, train_loss: 0.4697\n",
            "153/168, train_loss: 1.0869\n",
            "154/168, train_loss: 0.9677\n",
            "155/168, train_loss: 0.4395\n",
            "156/168, train_loss: 0.3929\n",
            "157/168, train_loss: 0.5668\n",
            "158/168, train_loss: 0.4036\n",
            "159/168, train_loss: 0.4037\n",
            "160/168, train_loss: 0.4726\n",
            "161/168, train_loss: 0.2249\n",
            "162/168, train_loss: 0.7655\n",
            "163/168, train_loss: 0.9637\n",
            "164/168, train_loss: 1.1289\n",
            "165/168, train_loss: 0.4013\n",
            "166/168, train_loss: 0.3305\n",
            "167/168, train_loss: 0.3811\n",
            "168/168, train_loss: 0.4759\n",
            "169/168, train_loss: 0.3917\n",
            "epoch 38 average loss: 0.5423\n",
            "current epoch: 38 current accuracy: 0.4000 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 39/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.8971\n",
            "2/168, train_loss: 0.1197\n",
            "3/168, train_loss: 0.5740\n",
            "4/168, train_loss: 0.1607\n",
            "5/168, train_loss: 0.0745\n",
            "6/168, train_loss: 0.9465\n",
            "7/168, train_loss: 0.1428\n",
            "8/168, train_loss: 0.3302\n",
            "9/168, train_loss: 0.4133\n",
            "10/168, train_loss: 0.3970\n",
            "11/168, train_loss: 0.5135\n",
            "12/168, train_loss: 0.5381\n",
            "13/168, train_loss: 0.3419\n",
            "14/168, train_loss: 0.1838\n",
            "15/168, train_loss: 0.5531\n",
            "16/168, train_loss: 0.5941\n",
            "17/168, train_loss: 0.6142\n",
            "18/168, train_loss: 0.4793\n",
            "19/168, train_loss: 0.3408\n",
            "20/168, train_loss: 0.7733\n",
            "21/168, train_loss: 0.3035\n",
            "22/168, train_loss: 1.1565\n",
            "23/168, train_loss: 0.7580\n",
            "24/168, train_loss: 0.5608\n",
            "25/168, train_loss: 0.5099\n",
            "26/168, train_loss: 0.4710\n",
            "27/168, train_loss: 0.4102\n",
            "28/168, train_loss: 0.3505\n",
            "29/168, train_loss: 0.7468\n",
            "30/168, train_loss: 0.4416\n",
            "31/168, train_loss: 0.4436\n",
            "32/168, train_loss: 0.2138\n",
            "33/168, train_loss: 0.1954\n",
            "34/168, train_loss: 0.7436\n",
            "35/168, train_loss: 0.5235\n",
            "36/168, train_loss: 0.6724\n",
            "37/168, train_loss: 0.4234\n",
            "38/168, train_loss: 0.4341\n",
            "39/168, train_loss: 0.0826\n",
            "40/168, train_loss: 0.2880\n",
            "41/168, train_loss: 0.1155\n",
            "42/168, train_loss: 0.3727\n",
            "43/168, train_loss: 0.5067\n",
            "44/168, train_loss: 1.0237\n",
            "45/168, train_loss: 1.2627\n",
            "46/168, train_loss: 0.1653\n",
            "47/168, train_loss: 0.4734\n",
            "48/168, train_loss: 0.6348\n",
            "49/168, train_loss: 0.2812\n",
            "50/168, train_loss: 0.3692\n",
            "51/168, train_loss: 0.1707\n",
            "52/168, train_loss: 1.2140\n",
            "53/168, train_loss: 0.7251\n",
            "54/168, train_loss: 0.7238\n",
            "55/168, train_loss: 1.0265\n",
            "56/168, train_loss: 1.0602\n",
            "57/168, train_loss: 0.7179\n",
            "58/168, train_loss: 0.4165\n",
            "59/168, train_loss: 0.5105\n",
            "60/168, train_loss: 0.5350\n",
            "61/168, train_loss: 0.4037\n",
            "62/168, train_loss: 0.6434\n",
            "63/168, train_loss: 0.4777\n",
            "64/168, train_loss: 0.4200\n",
            "65/168, train_loss: 0.4607\n",
            "66/168, train_loss: 0.4193\n",
            "67/168, train_loss: 1.0993\n",
            "68/168, train_loss: 0.3822\n",
            "69/168, train_loss: 0.2304\n",
            "70/168, train_loss: 0.4397\n",
            "71/168, train_loss: 0.3532\n",
            "72/168, train_loss: 0.3808\n",
            "73/168, train_loss: 0.4884\n",
            "74/168, train_loss: 0.3758\n",
            "75/168, train_loss: 0.7282\n",
            "76/168, train_loss: 0.5376\n",
            "77/168, train_loss: 1.2451\n",
            "78/168, train_loss: 0.4698\n",
            "79/168, train_loss: 0.1281\n",
            "80/168, train_loss: 0.4024\n",
            "81/168, train_loss: 0.3776\n",
            "82/168, train_loss: 0.3602\n",
            "83/168, train_loss: 0.0795\n",
            "84/168, train_loss: 0.3677\n",
            "85/168, train_loss: 0.4290\n",
            "86/168, train_loss: 0.1043\n",
            "87/168, train_loss: 0.4173\n",
            "88/168, train_loss: 0.5640\n",
            "89/168, train_loss: 0.0918\n",
            "90/168, train_loss: 0.2733\n",
            "91/168, train_loss: 0.1737\n",
            "92/168, train_loss: 0.3737\n",
            "93/168, train_loss: 1.0804\n",
            "94/168, train_loss: 0.1853\n",
            "95/168, train_loss: 0.2294\n",
            "96/168, train_loss: 0.2042\n",
            "97/168, train_loss: 0.1628\n",
            "98/168, train_loss: 0.4155\n",
            "99/168, train_loss: 0.4253\n",
            "100/168, train_loss: 0.4235\n",
            "101/168, train_loss: 0.4347\n",
            "102/168, train_loss: 0.1218\n",
            "103/168, train_loss: 0.3846\n",
            "104/168, train_loss: 0.3732\n",
            "105/168, train_loss: 0.4264\n",
            "106/168, train_loss: 0.1237\n",
            "107/168, train_loss: 0.5155\n",
            "108/168, train_loss: 0.7369\n",
            "109/168, train_loss: 0.3467\n",
            "110/168, train_loss: 0.4307\n",
            "111/168, train_loss: 1.2315\n",
            "112/168, train_loss: 0.2547\n",
            "113/168, train_loss: 0.1243\n",
            "114/168, train_loss: 0.7312\n",
            "115/168, train_loss: 0.7292\n",
            "116/168, train_loss: 0.3979\n",
            "117/168, train_loss: 1.7520\n",
            "118/168, train_loss: 0.4417\n",
            "119/168, train_loss: 1.4586\n",
            "120/168, train_loss: 0.3663\n",
            "121/168, train_loss: 0.7213\n",
            "122/168, train_loss: 0.4110\n",
            "123/168, train_loss: 0.7192\n",
            "124/168, train_loss: 0.3774\n",
            "125/168, train_loss: 0.1538\n",
            "126/168, train_loss: 1.0731\n",
            "127/168, train_loss: 0.2216\n",
            "128/168, train_loss: 0.1671\n",
            "129/168, train_loss: 0.0977\n",
            "130/168, train_loss: 0.9239\n",
            "131/168, train_loss: 0.0854\n",
            "132/168, train_loss: 0.5717\n",
            "133/168, train_loss: 0.5584\n",
            "134/168, train_loss: 0.3617\n",
            "135/168, train_loss: 0.5771\n",
            "136/168, train_loss: 0.4527\n",
            "137/168, train_loss: 0.4299\n",
            "138/168, train_loss: 0.4240\n",
            "139/168, train_loss: 1.0635\n",
            "140/168, train_loss: 0.1235\n",
            "141/168, train_loss: 0.9406\n",
            "142/168, train_loss: 0.4243\n",
            "143/168, train_loss: 0.4340\n",
            "144/168, train_loss: 0.3529\n",
            "145/168, train_loss: 0.7091\n",
            "146/168, train_loss: 0.4481\n",
            "147/168, train_loss: 0.4017\n",
            "148/168, train_loss: 0.5139\n",
            "149/168, train_loss: 0.0807\n",
            "150/168, train_loss: 0.4186\n",
            "151/168, train_loss: 0.4201\n",
            "152/168, train_loss: 1.0113\n",
            "153/168, train_loss: 0.3075\n",
            "154/168, train_loss: 0.4613\n",
            "155/168, train_loss: 0.5062\n",
            "156/168, train_loss: 0.1285\n",
            "157/168, train_loss: 0.3377\n",
            "158/168, train_loss: 0.2461\n",
            "159/168, train_loss: 0.4307\n",
            "160/168, train_loss: 0.3687\n",
            "161/168, train_loss: 0.7235\n",
            "162/168, train_loss: 0.7221\n",
            "163/168, train_loss: 1.2567\n",
            "164/168, train_loss: 0.4143\n",
            "165/168, train_loss: 0.3877\n",
            "166/168, train_loss: 0.7165\n",
            "167/168, train_loss: 0.3679\n",
            "168/168, train_loss: 0.3122\n",
            "169/168, train_loss: 0.9152\n",
            "epoch 39 average loss: 0.4941\n",
            "----------\n",
            "epoch 40/5\n",
            "1/168, train_loss: 0.3816\n",
            "2/168, train_loss: 0.1490\n",
            "3/168, train_loss: 0.4460\n",
            "4/168, train_loss: 0.0910\n",
            "5/168, train_loss: 0.2592\n",
            "6/168, train_loss: 0.4572\n",
            "7/168, train_loss: 0.1429\n",
            "8/168, train_loss: 0.4161\n",
            "9/168, train_loss: 1.2529\n",
            "10/168, train_loss: 0.6467\n",
            "11/168, train_loss: 0.4593\n",
            "12/168, train_loss: 0.3187\n",
            "13/168, train_loss: 0.4536\n",
            "14/168, train_loss: 0.1548\n",
            "15/168, train_loss: 0.2926\n",
            "16/168, train_loss: 0.4299\n",
            "17/168, train_loss: 0.1699\n",
            "18/168, train_loss: 0.4961\n",
            "19/168, train_loss: 1.1830\n",
            "20/168, train_loss: 0.3851\n",
            "21/168, train_loss: 0.3833\n",
            "22/168, train_loss: 0.4200\n",
            "23/168, train_loss: 0.3868\n",
            "24/168, train_loss: 0.5729\n",
            "25/168, train_loss: 0.3655\n",
            "26/168, train_loss: 0.3769\n",
            "27/168, train_loss: 0.4865\n",
            "28/168, train_loss: 0.2890\n",
            "29/168, train_loss: 0.4491\n",
            "30/168, train_loss: 0.0915\n",
            "31/168, train_loss: 0.3702\n",
            "32/168, train_loss: 0.1635\n",
            "33/168, train_loss: 0.5659\n",
            "34/168, train_loss: 0.4686\n",
            "35/168, train_loss: 1.2187\n",
            "36/168, train_loss: 0.4179\n",
            "37/168, train_loss: 0.7645\n",
            "38/168, train_loss: 0.3405\n",
            "39/168, train_loss: 0.7581\n",
            "40/168, train_loss: 0.1610\n",
            "41/168, train_loss: 0.3489\n",
            "42/168, train_loss: 1.2209\n",
            "43/168, train_loss: 0.2738\n",
            "44/168, train_loss: 0.8744\n",
            "45/168, train_loss: 0.1827\n",
            "46/168, train_loss: 0.5843\n",
            "47/168, train_loss: 0.7551\n",
            "48/168, train_loss: 0.7522\n",
            "49/168, train_loss: 0.2515\n",
            "50/168, train_loss: 0.3204\n",
            "51/168, train_loss: 0.3845\n",
            "52/168, train_loss: 0.4290\n",
            "53/168, train_loss: 0.4738\n",
            "54/168, train_loss: 0.1523\n",
            "55/168, train_loss: 0.7479\n",
            "56/168, train_loss: 0.7474\n",
            "57/168, train_loss: 1.0691\n",
            "58/168, train_loss: 0.3226\n",
            "59/168, train_loss: 0.3903\n",
            "60/168, train_loss: 0.4213\n",
            "61/168, train_loss: 0.4278\n",
            "62/168, train_loss: 0.4911\n",
            "63/168, train_loss: 0.5124\n",
            "64/168, train_loss: 0.3745\n",
            "65/168, train_loss: 0.5121\n",
            "66/168, train_loss: 0.2131\n",
            "67/168, train_loss: 0.1662\n",
            "68/168, train_loss: 1.0386\n",
            "69/168, train_loss: 0.4517\n",
            "70/168, train_loss: 0.2162\n",
            "71/168, train_loss: 1.1053\n",
            "72/168, train_loss: 0.5159\n",
            "73/168, train_loss: 0.5786\n",
            "74/168, train_loss: 0.5152\n",
            "75/168, train_loss: 0.3722\n",
            "76/168, train_loss: 0.4486\n",
            "77/168, train_loss: 0.2828\n",
            "78/168, train_loss: 1.5932\n",
            "79/168, train_loss: 0.4300\n",
            "80/168, train_loss: 0.5407\n",
            "81/168, train_loss: 0.4431\n",
            "82/168, train_loss: 0.3693\n",
            "83/168, train_loss: 0.4384\n",
            "84/168, train_loss: 0.5516\n",
            "85/168, train_loss: 0.4611\n",
            "86/168, train_loss: 1.0877\n",
            "87/168, train_loss: 0.3973\n",
            "88/168, train_loss: 0.4987\n",
            "89/168, train_loss: 0.4016\n",
            "90/168, train_loss: 1.1246\n",
            "91/168, train_loss: 0.5214\n",
            "92/168, train_loss: 0.1008\n",
            "93/168, train_loss: 0.5803\n",
            "94/168, train_loss: 0.3033\n",
            "95/168, train_loss: 0.3755\n",
            "96/168, train_loss: 0.7415\n",
            "97/168, train_loss: 0.3883\n",
            "98/168, train_loss: 0.1256\n",
            "99/168, train_loss: 0.4318\n",
            "100/168, train_loss: 1.0981\n",
            "101/168, train_loss: 0.7362\n",
            "102/168, train_loss: 0.7340\n",
            "103/168, train_loss: 0.6590\n",
            "104/168, train_loss: 0.2671\n",
            "105/168, train_loss: 0.4312\n",
            "106/168, train_loss: 0.1019\n",
            "107/168, train_loss: 0.1600\n",
            "108/168, train_loss: 0.4102\n",
            "109/168, train_loss: 0.4734\n",
            "110/168, train_loss: 0.7284\n",
            "111/168, train_loss: 0.2339\n",
            "112/168, train_loss: 0.4402\n",
            "113/168, train_loss: 0.3738\n",
            "114/168, train_loss: 0.3323\n",
            "115/168, train_loss: 0.3859\n",
            "116/168, train_loss: 0.1101\n",
            "117/168, train_loss: 1.5687\n",
            "118/168, train_loss: 0.9306\n",
            "119/168, train_loss: 0.3725\n",
            "120/168, train_loss: 0.3558\n",
            "121/168, train_loss: 1.1403\n",
            "122/168, train_loss: 0.3708\n",
            "123/168, train_loss: 0.3679\n",
            "124/168, train_loss: 0.3077\n",
            "125/168, train_loss: 0.3618\n",
            "126/168, train_loss: 1.1956\n",
            "127/168, train_loss: 0.3923\n",
            "128/168, train_loss: 0.8545\n",
            "129/168, train_loss: 0.2033\n",
            "130/168, train_loss: 1.2623\n",
            "131/168, train_loss: 0.2694\n",
            "132/168, train_loss: 0.5672\n",
            "133/168, train_loss: 0.3515\n",
            "134/168, train_loss: 1.0319\n",
            "135/168, train_loss: 0.1129\n",
            "136/168, train_loss: 0.5311\n",
            "137/168, train_loss: 0.2642\n",
            "138/168, train_loss: 0.9903\n",
            "139/168, train_loss: 0.3514\n",
            "140/168, train_loss: 0.3765\n",
            "141/168, train_loss: 0.5094\n",
            "142/168, train_loss: 0.7188\n",
            "143/168, train_loss: 0.3831\n",
            "144/168, train_loss: 0.3749\n",
            "145/168, train_loss: 0.5170\n",
            "146/168, train_loss: 1.7954\n",
            "147/168, train_loss: 1.0581\n",
            "148/168, train_loss: 0.7161\n",
            "149/168, train_loss: 0.5047\n",
            "150/168, train_loss: 0.5978\n",
            "151/168, train_loss: 0.7200\n",
            "152/168, train_loss: 0.2688\n",
            "153/168, train_loss: 0.3638\n",
            "154/168, train_loss: 0.4678\n",
            "155/168, train_loss: 0.3089\n",
            "156/168, train_loss: 1.1120\n",
            "157/168, train_loss: 1.0941\n",
            "158/168, train_loss: 0.7242\n",
            "159/168, train_loss: 0.3513\n",
            "160/168, train_loss: 0.4665\n",
            "161/168, train_loss: 0.4312\n",
            "162/168, train_loss: 0.0742\n",
            "163/168, train_loss: 0.8140\n",
            "164/168, train_loss: 0.2346\n",
            "165/168, train_loss: 0.1173\n",
            "166/168, train_loss: 0.6226\n",
            "167/168, train_loss: 0.3850\n",
            "168/168, train_loss: 0.5925\n",
            "169/168, train_loss: 0.4893\n",
            "epoch 40 average loss: 0.5177\n",
            "current epoch: 40 current accuracy: 0.4235 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 41/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.5433\n",
            "2/168, train_loss: 1.0456\n",
            "3/168, train_loss: 0.4319\n",
            "4/168, train_loss: 0.6137\n",
            "5/168, train_loss: 0.2012\n",
            "6/168, train_loss: 0.5272\n",
            "7/168, train_loss: 0.5033\n",
            "8/168, train_loss: 1.6536\n",
            "9/168, train_loss: 0.4479\n",
            "10/168, train_loss: 0.1534\n",
            "11/168, train_loss: 0.5069\n",
            "12/168, train_loss: 0.3695\n",
            "13/168, train_loss: 0.3413\n",
            "14/168, train_loss: 0.1792\n",
            "15/168, train_loss: 0.1614\n",
            "16/168, train_loss: 0.1377\n",
            "17/168, train_loss: 1.0874\n",
            "18/168, train_loss: 0.4390\n",
            "19/168, train_loss: 0.4556\n",
            "20/168, train_loss: 0.4056\n",
            "21/168, train_loss: 1.1600\n",
            "22/168, train_loss: 1.0939\n",
            "23/168, train_loss: 0.1897\n",
            "24/168, train_loss: 0.4715\n",
            "25/168, train_loss: 0.7309\n",
            "26/168, train_loss: 0.4398\n",
            "27/168, train_loss: 0.1615\n",
            "28/168, train_loss: 1.1718\n",
            "29/168, train_loss: 0.3309\n",
            "30/168, train_loss: 0.4097\n",
            "31/168, train_loss: 0.4857\n",
            "32/168, train_loss: 0.9791\n",
            "33/168, train_loss: 0.5270\n",
            "34/168, train_loss: 0.4981\n",
            "35/168, train_loss: 0.3929\n",
            "36/168, train_loss: 0.9470\n",
            "37/168, train_loss: 0.0971\n",
            "38/168, train_loss: 0.5092\n",
            "39/168, train_loss: 0.7157\n",
            "40/168, train_loss: 0.0890\n",
            "41/168, train_loss: 0.7166\n",
            "42/168, train_loss: 0.4635\n",
            "43/168, train_loss: 0.3804\n",
            "44/168, train_loss: 1.0627\n",
            "45/168, train_loss: 0.3888\n",
            "46/168, train_loss: 0.4997\n",
            "47/168, train_loss: 0.3164\n",
            "48/168, train_loss: 0.3914\n",
            "49/168, train_loss: 0.1349\n",
            "50/168, train_loss: 0.5019\n",
            "51/168, train_loss: 0.4262\n",
            "52/168, train_loss: 0.4574\n",
            "53/168, train_loss: 0.4318\n",
            "54/168, train_loss: 0.4822\n",
            "55/168, train_loss: 0.5140\n",
            "56/168, train_loss: 0.7279\n",
            "57/168, train_loss: 0.4237\n",
            "58/168, train_loss: 0.3810\n",
            "59/168, train_loss: 0.4671\n",
            "60/168, train_loss: 0.4842\n",
            "61/168, train_loss: 0.4431\n",
            "62/168, train_loss: 0.4140\n",
            "63/168, train_loss: 0.3878\n",
            "64/168, train_loss: 0.2226\n",
            "65/168, train_loss: 0.7394\n",
            "66/168, train_loss: 0.4278\n",
            "67/168, train_loss: 0.4867\n",
            "68/168, train_loss: 0.4350\n",
            "69/168, train_loss: 0.5353\n",
            "70/168, train_loss: 0.4477\n",
            "71/168, train_loss: 0.4463\n",
            "72/168, train_loss: 0.2316\n",
            "73/168, train_loss: 0.4501\n",
            "74/168, train_loss: 1.0679\n",
            "75/168, train_loss: 0.5071\n",
            "76/168, train_loss: 0.1311\n",
            "77/168, train_loss: 0.3193\n",
            "78/168, train_loss: 0.3630\n",
            "79/168, train_loss: 0.4537\n",
            "80/168, train_loss: 0.4974\n",
            "81/168, train_loss: 0.3265\n",
            "82/168, train_loss: 1.4786\n",
            "83/168, train_loss: 1.1205\n",
            "84/168, train_loss: 1.3132\n",
            "85/168, train_loss: 0.2781\n",
            "86/168, train_loss: 0.5538\n",
            "87/168, train_loss: 0.1953\n",
            "88/168, train_loss: 0.2563\n",
            "89/168, train_loss: 0.2530\n",
            "90/168, train_loss: 0.9941\n",
            "91/168, train_loss: 0.3209\n",
            "92/168, train_loss: 0.5286\n",
            "93/168, train_loss: 0.3288\n",
            "94/168, train_loss: 1.1103\n",
            "95/168, train_loss: 0.5357\n",
            "96/168, train_loss: 0.4052\n",
            "97/168, train_loss: 0.5243\n",
            "98/168, train_loss: 0.1599\n",
            "99/168, train_loss: 1.1149\n",
            "100/168, train_loss: 0.3449\n",
            "101/168, train_loss: 0.3648\n",
            "102/168, train_loss: 0.1297\n",
            "103/168, train_loss: 0.2662\n",
            "104/168, train_loss: 0.3846\n",
            "105/168, train_loss: 0.4621\n",
            "106/168, train_loss: 0.5154\n",
            "107/168, train_loss: 1.0118\n",
            "108/168, train_loss: 0.7268\n",
            "109/168, train_loss: 0.3652\n",
            "110/168, train_loss: 0.7211\n",
            "111/168, train_loss: 0.7189\n",
            "112/168, train_loss: 1.1582\n",
            "113/168, train_loss: 0.3778\n",
            "114/168, train_loss: 0.3774\n",
            "115/168, train_loss: 0.5113\n",
            "116/168, train_loss: 0.3895\n",
            "117/168, train_loss: 0.1072\n",
            "118/168, train_loss: 0.5174\n",
            "119/168, train_loss: 0.6202\n",
            "120/168, train_loss: 0.2387\n",
            "121/168, train_loss: 0.4540\n",
            "122/168, train_loss: 0.3331\n",
            "123/168, train_loss: 0.3299\n",
            "124/168, train_loss: 0.7267\n",
            "125/168, train_loss: 0.4786\n",
            "126/168, train_loss: 0.4715\n",
            "127/168, train_loss: 0.4717\n",
            "128/168, train_loss: 0.8448\n",
            "129/168, train_loss: 0.1755\n",
            "130/168, train_loss: 0.3719\n",
            "131/168, train_loss: 1.1163\n",
            "132/168, train_loss: 0.1674\n",
            "133/168, train_loss: 0.4692\n",
            "134/168, train_loss: 0.4385\n",
            "135/168, train_loss: 0.2838\n",
            "136/168, train_loss: 0.2959\n",
            "137/168, train_loss: 0.4109\n",
            "138/168, train_loss: 0.1036\n",
            "139/168, train_loss: 0.3113\n",
            "140/168, train_loss: 0.6242\n",
            "141/168, train_loss: 0.7428\n",
            "142/168, train_loss: 1.0564\n",
            "143/168, train_loss: 0.3813\n",
            "144/168, train_loss: 0.2640\n",
            "145/168, train_loss: 0.4924\n",
            "146/168, train_loss: 0.7277\n",
            "147/168, train_loss: 1.3066\n",
            "148/168, train_loss: 0.3721\n",
            "149/168, train_loss: 0.1911\n",
            "150/168, train_loss: 0.1709\n",
            "151/168, train_loss: 1.1612\n",
            "152/168, train_loss: 0.3869\n",
            "153/168, train_loss: 0.1660\n",
            "154/168, train_loss: 1.2113\n",
            "155/168, train_loss: 0.1917\n",
            "156/168, train_loss: 0.3658\n",
            "157/168, train_loss: 0.4908\n",
            "158/168, train_loss: 0.3228\n",
            "159/168, train_loss: 0.2382\n",
            "160/168, train_loss: 0.5774\n",
            "161/168, train_loss: 0.3666\n",
            "162/168, train_loss: 0.5997\n",
            "163/168, train_loss: 0.1348\n",
            "164/168, train_loss: 0.3766\n",
            "165/168, train_loss: 0.1100\n",
            "166/168, train_loss: 0.3708\n",
            "167/168, train_loss: 0.4511\n",
            "168/168, train_loss: 0.9277\n",
            "169/168, train_loss: 1.0389\n",
            "epoch 41 average loss: 0.5115\n",
            "----------\n",
            "epoch 42/5\n",
            "1/168, train_loss: 0.2172\n",
            "2/168, train_loss: 1.0592\n",
            "3/168, train_loss: 0.1185\n",
            "4/168, train_loss: 0.5963\n",
            "5/168, train_loss: 0.3983\n",
            "6/168, train_loss: 0.3970\n",
            "7/168, train_loss: 1.0110\n",
            "8/168, train_loss: 0.4363\n",
            "9/168, train_loss: 1.5153\n",
            "10/168, train_loss: 0.4153\n",
            "11/168, train_loss: 0.7201\n",
            "12/168, train_loss: 0.3712\n",
            "13/168, train_loss: 0.3765\n",
            "14/168, train_loss: 0.4329\n",
            "15/168, train_loss: 0.7168\n",
            "16/168, train_loss: 0.4049\n",
            "17/168, train_loss: 0.3881\n",
            "18/168, train_loss: 0.4449\n",
            "19/168, train_loss: 0.7142\n",
            "20/168, train_loss: 0.3901\n",
            "21/168, train_loss: 0.5162\n",
            "22/168, train_loss: 0.5966\n",
            "23/168, train_loss: 0.3860\n",
            "24/168, train_loss: 0.2175\n",
            "25/168, train_loss: 1.0838\n",
            "26/168, train_loss: 0.3770\n",
            "27/168, train_loss: 0.4278\n",
            "28/168, train_loss: 1.1564\n",
            "29/168, train_loss: 0.5701\n",
            "30/168, train_loss: 0.4148\n",
            "31/168, train_loss: 1.0852\n",
            "32/168, train_loss: 0.4086\n",
            "33/168, train_loss: 0.3633\n",
            "34/168, train_loss: 0.3675\n",
            "35/168, train_loss: 0.3938\n",
            "36/168, train_loss: 0.3661\n",
            "37/168, train_loss: 0.5199\n",
            "38/168, train_loss: 1.5109\n",
            "39/168, train_loss: 0.7179\n",
            "40/168, train_loss: 0.9494\n",
            "41/168, train_loss: 0.3527\n",
            "42/168, train_loss: 0.2194\n",
            "43/168, train_loss: 1.1763\n",
            "44/168, train_loss: 0.3224\n",
            "45/168, train_loss: 0.1001\n",
            "46/168, train_loss: 0.5129\n",
            "47/168, train_loss: 0.3873\n",
            "48/168, train_loss: 0.7617\n",
            "49/168, train_loss: 0.7206\n",
            "50/168, train_loss: 0.5528\n",
            "51/168, train_loss: 0.2007\n",
            "52/168, train_loss: 0.4805\n",
            "53/168, train_loss: 1.0884\n",
            "54/168, train_loss: 0.1155\n",
            "55/168, train_loss: 1.0030\n",
            "56/168, train_loss: 0.4551\n",
            "57/168, train_loss: 0.1811\n",
            "58/168, train_loss: 0.5227\n",
            "59/168, train_loss: 0.3788\n",
            "60/168, train_loss: 0.3853\n",
            "61/168, train_loss: 0.4807\n",
            "62/168, train_loss: 0.3069\n",
            "63/168, train_loss: 0.4493\n",
            "64/168, train_loss: 0.3818\n",
            "65/168, train_loss: 1.0100\n",
            "66/168, train_loss: 0.4548\n",
            "67/168, train_loss: 0.3577\n",
            "68/168, train_loss: 0.3919\n",
            "69/168, train_loss: 0.4321\n",
            "70/168, train_loss: 0.3832\n",
            "71/168, train_loss: 1.3091\n",
            "72/168, train_loss: 0.4100\n",
            "73/168, train_loss: 0.1627\n",
            "74/168, train_loss: 0.4519\n",
            "75/168, train_loss: 0.4112\n",
            "76/168, train_loss: 0.2110\n",
            "77/168, train_loss: 0.4495\n",
            "78/168, train_loss: 0.3751\n",
            "79/168, train_loss: 0.5397\n",
            "80/168, train_loss: 0.3736\n",
            "81/168, train_loss: 1.1080\n",
            "82/168, train_loss: 1.2173\n",
            "83/168, train_loss: 0.3742\n",
            "84/168, train_loss: 1.1506\n",
            "85/168, train_loss: 0.2111\n",
            "86/168, train_loss: 0.2170\n",
            "87/168, train_loss: 0.4523\n",
            "88/168, train_loss: 0.3689\n",
            "89/168, train_loss: 0.1082\n",
            "90/168, train_loss: 0.7411\n",
            "91/168, train_loss: 0.5075\n",
            "92/168, train_loss: 1.1057\n",
            "93/168, train_loss: 0.3823\n",
            "94/168, train_loss: 0.4608\n",
            "95/168, train_loss: 0.4966\n",
            "96/168, train_loss: 0.5407\n",
            "97/168, train_loss: 0.1595\n",
            "98/168, train_loss: 0.3085\n",
            "99/168, train_loss: 0.4615\n",
            "100/168, train_loss: 1.0741\n",
            "101/168, train_loss: 1.1280\n",
            "102/168, train_loss: 0.4352\n",
            "103/168, train_loss: 0.3486\n",
            "104/168, train_loss: 0.2731\n",
            "105/168, train_loss: 0.3799\n",
            "106/168, train_loss: 0.2341\n",
            "107/168, train_loss: 0.7373\n",
            "108/168, train_loss: 0.1485\n",
            "109/168, train_loss: 0.4216\n",
            "110/168, train_loss: 1.0156\n",
            "111/168, train_loss: 0.3799\n",
            "112/168, train_loss: 0.6001\n",
            "113/168, train_loss: 0.5695\n",
            "114/168, train_loss: 0.1441\n",
            "115/168, train_loss: 1.2787\n",
            "116/168, train_loss: 0.4003\n",
            "117/168, train_loss: 0.4546\n",
            "118/168, train_loss: 0.1584\n",
            "119/168, train_loss: 1.1447\n",
            "120/168, train_loss: 0.7185\n",
            "121/168, train_loss: 0.1160\n",
            "122/168, train_loss: 0.4397\n",
            "123/168, train_loss: 0.4775\n",
            "124/168, train_loss: 0.3578\n",
            "125/168, train_loss: 0.4327\n",
            "126/168, train_loss: 0.4239\n",
            "127/168, train_loss: 0.5137\n",
            "128/168, train_loss: 0.7157\n",
            "129/168, train_loss: 0.2914\n",
            "130/168, train_loss: 0.2824\n",
            "131/168, train_loss: 0.2584\n",
            "132/168, train_loss: 0.0785\n",
            "133/168, train_loss: 0.3620\n",
            "134/168, train_loss: 1.1344\n",
            "135/168, train_loss: 0.3810\n",
            "136/168, train_loss: 0.3957\n",
            "137/168, train_loss: 0.3931\n",
            "138/168, train_loss: 1.2273\n",
            "139/168, train_loss: 0.4280\n",
            "140/168, train_loss: 0.5280\n",
            "141/168, train_loss: 0.4793\n",
            "142/168, train_loss: 0.4889\n",
            "143/168, train_loss: 0.3945\n",
            "144/168, train_loss: 0.1976\n",
            "145/168, train_loss: 1.0935\n",
            "146/168, train_loss: 0.4864\n",
            "147/168, train_loss: 0.1142\n",
            "148/168, train_loss: 0.4272\n",
            "149/168, train_loss: 0.3999\n",
            "150/168, train_loss: 0.1150\n",
            "151/168, train_loss: 0.5561\n",
            "152/168, train_loss: 0.1451\n",
            "153/168, train_loss: 0.1369\n",
            "154/168, train_loss: 0.7377\n",
            "155/168, train_loss: 0.3907\n",
            "156/168, train_loss: 0.4294\n",
            "157/168, train_loss: 0.5424\n",
            "158/168, train_loss: 0.0793\n",
            "159/168, train_loss: 0.4453\n",
            "160/168, train_loss: 0.5037\n",
            "161/168, train_loss: 0.2955\n",
            "162/168, train_loss: 0.4707\n",
            "163/168, train_loss: 0.4014\n",
            "164/168, train_loss: 0.7446\n",
            "165/168, train_loss: 0.2247\n",
            "166/168, train_loss: 0.1272\n",
            "167/168, train_loss: 0.4601\n",
            "168/168, train_loss: 1.0632\n",
            "169/168, train_loss: 0.3749\n",
            "epoch 42 average loss: 0.5136\n",
            "current epoch: 42 current accuracy: 0.4118 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 43/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.1090\n",
            "2/168, train_loss: 0.7321\n",
            "3/168, train_loss: 0.4575\n",
            "4/168, train_loss: 0.4329\n",
            "5/168, train_loss: 0.3979\n",
            "6/168, train_loss: 0.1497\n",
            "7/168, train_loss: 1.4256\n",
            "8/168, train_loss: 0.3700\n",
            "9/168, train_loss: 0.4383\n",
            "10/168, train_loss: 0.3984\n",
            "11/168, train_loss: 0.7442\n",
            "12/168, train_loss: 0.3506\n",
            "13/168, train_loss: 0.6324\n",
            "14/168, train_loss: 0.2950\n",
            "15/168, train_loss: 1.3933\n",
            "16/168, train_loss: 0.3801\n",
            "17/168, train_loss: 0.4176\n",
            "18/168, train_loss: 0.3658\n",
            "19/168, train_loss: 0.1664\n",
            "20/168, train_loss: 0.0962\n",
            "21/168, train_loss: 0.7587\n",
            "22/168, train_loss: 0.3649\n",
            "23/168, train_loss: 0.3892\n",
            "24/168, train_loss: 0.3745\n",
            "25/168, train_loss: 0.3786\n",
            "26/168, train_loss: 1.0612\n",
            "27/168, train_loss: 0.3663\n",
            "28/168, train_loss: 0.7574\n",
            "29/168, train_loss: 0.3710\n",
            "30/168, train_loss: 0.4105\n",
            "31/168, train_loss: 0.0896\n",
            "32/168, train_loss: 0.5199\n",
            "33/168, train_loss: 0.4184\n",
            "34/168, train_loss: 0.2068\n",
            "35/168, train_loss: 0.4337\n",
            "36/168, train_loss: 0.4119\n",
            "37/168, train_loss: 0.7530\n",
            "38/168, train_loss: 0.3580\n",
            "39/168, train_loss: 1.2510\n",
            "40/168, train_loss: 0.3925\n",
            "41/168, train_loss: 0.5866\n",
            "42/168, train_loss: 0.3966\n",
            "43/168, train_loss: 0.3532\n",
            "44/168, train_loss: 0.3518\n",
            "45/168, train_loss: 1.2438\n",
            "46/168, train_loss: 0.3840\n",
            "47/168, train_loss: 0.7698\n",
            "48/168, train_loss: 0.4311\n",
            "49/168, train_loss: 0.2618\n",
            "50/168, train_loss: 1.5112\n",
            "51/168, train_loss: 0.3532\n",
            "52/168, train_loss: 0.3780\n",
            "53/168, train_loss: 0.3959\n",
            "54/168, train_loss: 0.7627\n",
            "55/168, train_loss: 0.3011\n",
            "56/168, train_loss: 0.4077\n",
            "57/168, train_loss: 0.3861\n",
            "58/168, train_loss: 0.3391\n",
            "59/168, train_loss: 0.4024\n",
            "60/168, train_loss: 0.2197\n",
            "61/168, train_loss: 0.1030\n",
            "62/168, train_loss: 1.1086\n",
            "63/168, train_loss: 0.2334\n",
            "64/168, train_loss: 0.1635\n",
            "65/168, train_loss: 0.4741\n",
            "66/168, train_loss: 0.3732\n",
            "67/168, train_loss: 0.2842\n",
            "68/168, train_loss: 0.4752\n",
            "69/168, train_loss: 0.8506\n",
            "70/168, train_loss: 0.1539\n",
            "71/168, train_loss: 0.6524\n",
            "72/168, train_loss: 0.4402\n",
            "73/168, train_loss: 0.3672\n",
            "74/168, train_loss: 1.1334\n",
            "75/168, train_loss: 0.1955\n",
            "76/168, train_loss: 0.6565\n",
            "77/168, train_loss: 1.2754\n",
            "78/168, train_loss: 0.3693\n",
            "79/168, train_loss: 0.3769\n",
            "80/168, train_loss: 1.0852\n",
            "81/168, train_loss: 0.7649\n",
            "82/168, train_loss: 0.1837\n",
            "83/168, train_loss: 0.4137\n",
            "84/168, train_loss: 0.3484\n",
            "85/168, train_loss: 0.3701\n",
            "86/168, train_loss: 0.4567\n",
            "87/168, train_loss: 0.7545\n",
            "88/168, train_loss: 0.1945\n",
            "89/168, train_loss: 0.4117\n",
            "90/168, train_loss: 0.3321\n",
            "91/168, train_loss: 0.3818\n",
            "92/168, train_loss: 0.5118\n",
            "93/168, train_loss: 0.3440\n",
            "94/168, train_loss: 0.7588\n",
            "95/168, train_loss: 1.1197\n",
            "96/168, train_loss: 0.7194\n",
            "97/168, train_loss: 0.1166\n",
            "98/168, train_loss: 0.6504\n",
            "99/168, train_loss: 1.0544\n",
            "100/168, train_loss: 0.0777\n",
            "101/168, train_loss: 0.4151\n",
            "102/168, train_loss: 0.7288\n",
            "103/168, train_loss: 0.5129\n",
            "104/168, train_loss: 0.5600\n",
            "105/168, train_loss: 0.3544\n",
            "106/168, train_loss: 0.4383\n",
            "107/168, train_loss: 0.1435\n",
            "108/168, train_loss: 0.1644\n",
            "109/168, train_loss: 0.3558\n",
            "110/168, train_loss: 0.4439\n",
            "111/168, train_loss: 0.4933\n",
            "112/168, train_loss: 0.2151\n",
            "113/168, train_loss: 1.0273\n",
            "114/168, train_loss: 0.3890\n",
            "115/168, train_loss: 0.4392\n",
            "116/168, train_loss: 0.9688\n",
            "117/168, train_loss: 0.4433\n",
            "118/168, train_loss: 0.2318\n",
            "119/168, train_loss: 0.5059\n",
            "120/168, train_loss: 0.3370\n",
            "121/168, train_loss: 1.1979\n",
            "122/168, train_loss: 0.1606\n",
            "123/168, train_loss: 0.4585\n",
            "124/168, train_loss: 1.2288\n",
            "125/168, train_loss: 0.7210\n",
            "126/168, train_loss: 0.7194\n",
            "127/168, train_loss: 0.4296\n",
            "128/168, train_loss: 1.3465\n",
            "129/168, train_loss: 0.3586\n",
            "130/168, train_loss: 1.1694\n",
            "131/168, train_loss: 0.1465\n",
            "132/168, train_loss: 0.4036\n",
            "133/168, train_loss: 0.3530\n",
            "134/168, train_loss: 0.3769\n",
            "135/168, train_loss: 1.4414\n",
            "136/168, train_loss: 0.4104\n",
            "137/168, train_loss: 0.1099\n",
            "138/168, train_loss: 0.8892\n",
            "139/168, train_loss: 0.3891\n",
            "140/168, train_loss: 0.4145\n",
            "141/168, train_loss: 0.3678\n",
            "142/168, train_loss: 0.4866\n",
            "143/168, train_loss: 0.8632\n",
            "144/168, train_loss: 0.3767\n",
            "145/168, train_loss: 1.2489\n",
            "146/168, train_loss: 0.0901\n",
            "147/168, train_loss: 0.1020\n",
            "148/168, train_loss: 1.7016\n",
            "149/168, train_loss: 0.3699\n",
            "150/168, train_loss: 0.3652\n",
            "151/168, train_loss: 0.7114\n",
            "152/168, train_loss: 0.0747\n",
            "153/168, train_loss: 0.5420\n",
            "154/168, train_loss: 0.5727\n",
            "155/168, train_loss: 0.0686\n",
            "156/168, train_loss: 0.3547\n",
            "157/168, train_loss: 0.3925\n",
            "158/168, train_loss: 0.3864\n",
            "159/168, train_loss: 0.4193\n",
            "160/168, train_loss: 0.1158\n",
            "161/168, train_loss: 0.7116\n",
            "162/168, train_loss: 0.7676\n",
            "163/168, train_loss: 1.1334\n",
            "164/168, train_loss: 0.6140\n",
            "165/168, train_loss: 0.4102\n",
            "166/168, train_loss: 0.3803\n",
            "167/168, train_loss: 0.7072\n",
            "168/168, train_loss: 0.5421\n",
            "169/168, train_loss: 0.4092\n",
            "epoch 43 average loss: 0.5205\n",
            "----------\n",
            "epoch 44/5\n",
            "1/168, train_loss: 0.1939\n",
            "2/168, train_loss: 0.7047\n",
            "3/168, train_loss: 0.4194\n",
            "4/168, train_loss: 0.1884\n",
            "5/168, train_loss: 0.6614\n",
            "6/168, train_loss: 0.3656\n",
            "7/168, train_loss: 0.2854\n",
            "8/168, train_loss: 1.0952\n",
            "9/168, train_loss: 0.6612\n",
            "10/168, train_loss: 0.1497\n",
            "11/168, train_loss: 0.4370\n",
            "12/168, train_loss: 0.4456\n",
            "13/168, train_loss: 1.0152\n",
            "14/168, train_loss: 1.4601\n",
            "15/168, train_loss: 0.5638\n",
            "16/168, train_loss: 0.4678\n",
            "17/168, train_loss: 0.7025\n",
            "18/168, train_loss: 0.3874\n",
            "19/168, train_loss: 1.0898\n",
            "20/168, train_loss: 0.3864\n",
            "21/168, train_loss: 0.3920\n",
            "22/168, train_loss: 0.5721\n",
            "23/168, train_loss: 0.3840\n",
            "24/168, train_loss: 1.1418\n",
            "25/168, train_loss: 0.4312\n",
            "26/168, train_loss: 0.8853\n",
            "27/168, train_loss: 0.2129\n",
            "28/168, train_loss: 0.4733\n",
            "29/168, train_loss: 0.4880\n",
            "30/168, train_loss: 0.4282\n",
            "31/168, train_loss: 0.3796\n",
            "32/168, train_loss: 1.1230\n",
            "33/168, train_loss: 0.4978\n",
            "34/168, train_loss: 0.1044\n",
            "35/168, train_loss: 0.4779\n",
            "36/168, train_loss: 0.3811\n",
            "37/168, train_loss: 0.1913\n",
            "38/168, train_loss: 1.1077\n",
            "39/168, train_loss: 0.5606\n",
            "40/168, train_loss: 0.1368\n",
            "41/168, train_loss: 0.0835\n",
            "42/168, train_loss: 0.6788\n",
            "43/168, train_loss: 0.4184\n",
            "44/168, train_loss: 0.4829\n",
            "45/168, train_loss: 0.1983\n",
            "46/168, train_loss: 0.3837\n",
            "47/168, train_loss: 0.7196\n",
            "48/168, train_loss: 0.7281\n",
            "49/168, train_loss: 1.0149\n",
            "50/168, train_loss: 0.1983\n",
            "51/168, train_loss: 0.6255\n",
            "52/168, train_loss: 0.3988\n",
            "53/168, train_loss: 0.3352\n",
            "54/168, train_loss: 1.1584\n",
            "55/168, train_loss: 1.0158\n",
            "56/168, train_loss: 0.4625\n",
            "57/168, train_loss: 0.4544\n",
            "58/168, train_loss: 0.7186\n",
            "59/168, train_loss: 0.4957\n",
            "60/168, train_loss: 0.3380\n",
            "61/168, train_loss: 0.1673\n",
            "62/168, train_loss: 0.4411\n",
            "63/168, train_loss: 0.8133\n",
            "64/168, train_loss: 0.2842\n",
            "65/168, train_loss: 0.4639\n",
            "66/168, train_loss: 0.3284\n",
            "67/168, train_loss: 0.7350\n",
            "68/168, train_loss: 1.5776\n",
            "69/168, train_loss: 0.4393\n",
            "70/168, train_loss: 0.4579\n",
            "71/168, train_loss: 0.4225\n",
            "72/168, train_loss: 0.7493\n",
            "73/168, train_loss: 0.4038\n",
            "74/168, train_loss: 0.4363\n",
            "75/168, train_loss: 0.7539\n",
            "76/168, train_loss: 0.0993\n",
            "77/168, train_loss: 0.7501\n",
            "78/168, train_loss: 0.3885\n",
            "79/168, train_loss: 1.4029\n",
            "80/168, train_loss: 0.2980\n",
            "81/168, train_loss: 0.4881\n",
            "82/168, train_loss: 0.4239\n",
            "83/168, train_loss: 0.3092\n",
            "84/168, train_loss: 0.4367\n",
            "85/168, train_loss: 0.3873\n",
            "86/168, train_loss: 0.6724\n",
            "87/168, train_loss: 0.3920\n",
            "88/168, train_loss: 0.4034\n",
            "89/168, train_loss: 0.3993\n",
            "90/168, train_loss: 0.5448\n",
            "91/168, train_loss: 0.4000\n",
            "92/168, train_loss: 0.5784\n",
            "93/168, train_loss: 1.1430\n",
            "94/168, train_loss: 0.0761\n",
            "95/168, train_loss: 0.3728\n",
            "96/168, train_loss: 1.1341\n",
            "97/168, train_loss: 1.0944\n",
            "98/168, train_loss: 0.3762\n",
            "99/168, train_loss: 0.3879\n",
            "100/168, train_loss: 0.3711\n",
            "101/168, train_loss: 0.3770\n",
            "102/168, train_loss: 0.7495\n",
            "103/168, train_loss: 0.3819\n",
            "104/168, train_loss: 0.5503\n",
            "105/168, train_loss: 0.4570\n",
            "106/168, train_loss: 0.0942\n",
            "107/168, train_loss: 0.4115\n",
            "108/168, train_loss: 1.2066\n",
            "109/168, train_loss: 0.4661\n",
            "110/168, train_loss: 0.3642\n",
            "111/168, train_loss: 0.4529\n",
            "112/168, train_loss: 0.4110\n",
            "113/168, train_loss: 0.1731\n",
            "114/168, train_loss: 0.7409\n",
            "115/168, train_loss: 1.1204\n",
            "116/168, train_loss: 0.3779\n",
            "117/168, train_loss: 0.4242\n",
            "118/168, train_loss: 0.3771\n",
            "119/168, train_loss: 0.4424\n",
            "120/168, train_loss: 0.5779\n",
            "121/168, train_loss: 0.3571\n",
            "122/168, train_loss: 0.7422\n",
            "123/168, train_loss: 0.4214\n",
            "124/168, train_loss: 0.2797\n",
            "125/168, train_loss: 0.4263\n",
            "126/168, train_loss: 0.3719\n",
            "127/168, train_loss: 0.4119\n",
            "128/168, train_loss: 0.5126\n",
            "129/168, train_loss: 0.5819\n",
            "130/168, train_loss: 0.7616\n",
            "131/168, train_loss: 1.1826\n",
            "132/168, train_loss: 0.1560\n",
            "133/168, train_loss: 0.3939\n",
            "134/168, train_loss: 0.1630\n",
            "135/168, train_loss: 0.4841\n",
            "136/168, train_loss: 0.1830\n",
            "137/168, train_loss: 0.3642\n",
            "138/168, train_loss: 1.1389\n",
            "139/168, train_loss: 0.3550\n",
            "140/168, train_loss: 1.1027\n",
            "141/168, train_loss: 0.3771\n",
            "142/168, train_loss: 0.4906\n",
            "143/168, train_loss: 0.4145\n",
            "144/168, train_loss: 0.4851\n",
            "145/168, train_loss: 0.1676\n",
            "146/168, train_loss: 0.5289\n",
            "147/168, train_loss: 0.9009\n",
            "148/168, train_loss: 0.7458\n",
            "149/168, train_loss: 0.3613\n",
            "150/168, train_loss: 0.4502\n",
            "151/168, train_loss: 1.0497\n",
            "152/168, train_loss: 0.1118\n",
            "153/168, train_loss: 0.7391\n",
            "154/168, train_loss: 1.0334\n",
            "155/168, train_loss: 0.7352\n",
            "156/168, train_loss: 0.7324\n",
            "157/168, train_loss: 0.3333\n",
            "158/168, train_loss: 0.3055\n",
            "159/168, train_loss: 0.1582\n",
            "160/168, train_loss: 0.3701\n",
            "161/168, train_loss: 0.7230\n",
            "162/168, train_loss: 0.3715\n",
            "163/168, train_loss: 0.7199\n",
            "164/168, train_loss: 0.1631\n",
            "165/168, train_loss: 1.1526\n",
            "166/168, train_loss: 0.4000\n",
            "167/168, train_loss: 1.0577\n",
            "168/168, train_loss: 0.2330\n",
            "169/168, train_loss: 0.5136\n",
            "epoch 44 average loss: 0.5393\n",
            "current epoch: 44 current accuracy: 0.4118 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 45/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.4434\n",
            "2/168, train_loss: 1.0788\n",
            "3/168, train_loss: 0.4112\n",
            "4/168, train_loss: 0.4796\n",
            "5/168, train_loss: 0.3719\n",
            "6/168, train_loss: 0.5986\n",
            "7/168, train_loss: 0.4496\n",
            "8/168, train_loss: 0.2627\n",
            "9/168, train_loss: 0.9736\n",
            "10/168, train_loss: 0.3644\n",
            "11/168, train_loss: 0.1440\n",
            "12/168, train_loss: 0.5081\n",
            "13/168, train_loss: 0.5031\n",
            "14/168, train_loss: 0.7183\n",
            "15/168, train_loss: 1.0370\n",
            "16/168, train_loss: 0.4456\n",
            "17/168, train_loss: 0.4845\n",
            "18/168, train_loss: 0.4464\n",
            "19/168, train_loss: 0.4016\n",
            "20/168, train_loss: 0.4746\n",
            "21/168, train_loss: 0.3336\n",
            "22/168, train_loss: 0.4704\n",
            "23/168, train_loss: 0.4384\n",
            "24/168, train_loss: 0.6192\n",
            "25/168, train_loss: 1.0365\n",
            "26/168, train_loss: 0.3889\n",
            "27/168, train_loss: 0.4549\n",
            "28/168, train_loss: 0.5725\n",
            "29/168, train_loss: 1.2087\n",
            "30/168, train_loss: 0.4417\n",
            "31/168, train_loss: 0.4310\n",
            "32/168, train_loss: 0.4170\n",
            "33/168, train_loss: 0.1717\n",
            "34/168, train_loss: 0.2870\n",
            "35/168, train_loss: 0.7603\n",
            "36/168, train_loss: 0.4492\n",
            "37/168, train_loss: 0.3846\n",
            "38/168, train_loss: 0.1364\n",
            "39/168, train_loss: 0.5260\n",
            "40/168, train_loss: 0.6195\n",
            "41/168, train_loss: 0.4257\n",
            "42/168, train_loss: 0.3774\n",
            "43/168, train_loss: 1.3401\n",
            "44/168, train_loss: 0.1236\n",
            "45/168, train_loss: 0.4435\n",
            "46/168, train_loss: 0.3955\n",
            "47/168, train_loss: 0.3943\n",
            "48/168, train_loss: 0.3516\n",
            "49/168, train_loss: 1.1527\n",
            "50/168, train_loss: 0.6822\n",
            "51/168, train_loss: 0.5743\n",
            "52/168, train_loss: 0.2869\n",
            "53/168, train_loss: 0.4062\n",
            "54/168, train_loss: 0.2287\n",
            "55/168, train_loss: 0.3910\n",
            "56/168, train_loss: 0.3706\n",
            "57/168, train_loss: 0.1143\n",
            "58/168, train_loss: 0.4081\n",
            "59/168, train_loss: 0.2487\n",
            "60/168, train_loss: 0.2589\n",
            "61/168, train_loss: 0.3929\n",
            "62/168, train_loss: 0.1682\n",
            "63/168, train_loss: 0.1056\n",
            "64/168, train_loss: 0.5487\n",
            "65/168, train_loss: 0.3858\n",
            "66/168, train_loss: 1.3789\n",
            "67/168, train_loss: 0.3522\n",
            "68/168, train_loss: 0.3807\n",
            "69/168, train_loss: 0.7705\n",
            "70/168, train_loss: 0.7720\n",
            "71/168, train_loss: 0.1574\n",
            "72/168, train_loss: 0.2472\n",
            "73/168, train_loss: 0.5030\n",
            "74/168, train_loss: 0.1227\n",
            "75/168, train_loss: 1.1633\n",
            "76/168, train_loss: 0.3937\n",
            "77/168, train_loss: 0.3502\n",
            "78/168, train_loss: 0.1009\n",
            "79/168, train_loss: 0.3975\n",
            "80/168, train_loss: 0.4775\n",
            "81/168, train_loss: 0.4203\n",
            "82/168, train_loss: 0.7638\n",
            "83/168, train_loss: 0.6567\n",
            "84/168, train_loss: 0.3592\n",
            "85/168, train_loss: 0.1486\n",
            "86/168, train_loss: 0.4362\n",
            "87/168, train_loss: 0.7557\n",
            "88/168, train_loss: 0.3419\n",
            "89/168, train_loss: 0.5766\n",
            "90/168, train_loss: 0.2152\n",
            "91/168, train_loss: 0.3493\n",
            "92/168, train_loss: 0.3422\n",
            "93/168, train_loss: 0.2348\n",
            "94/168, train_loss: 0.4360\n",
            "95/168, train_loss: 0.4749\n",
            "96/168, train_loss: 0.4423\n",
            "97/168, train_loss: 0.7449\n",
            "98/168, train_loss: 0.2923\n",
            "99/168, train_loss: 0.3814\n",
            "100/168, train_loss: 0.4832\n",
            "101/168, train_loss: 0.7401\n",
            "102/168, train_loss: 0.3706\n",
            "103/168, train_loss: 0.2494\n",
            "104/168, train_loss: 0.3533\n",
            "105/168, train_loss: 1.2323\n",
            "106/168, train_loss: 0.4323\n",
            "107/168, train_loss: 0.4229\n",
            "108/168, train_loss: 0.1168\n",
            "109/168, train_loss: 0.1057\n",
            "110/168, train_loss: 0.1492\n",
            "111/168, train_loss: 0.1434\n",
            "112/168, train_loss: 0.1755\n",
            "113/168, train_loss: 0.0679\n",
            "114/168, train_loss: 1.1484\n",
            "115/168, train_loss: 0.4887\n",
            "116/168, train_loss: 0.7328\n",
            "117/168, train_loss: 0.4077\n",
            "118/168, train_loss: 0.3774\n",
            "119/168, train_loss: 0.4073\n",
            "120/168, train_loss: 0.0732\n",
            "121/168, train_loss: 0.3621\n",
            "122/168, train_loss: 1.2383\n",
            "123/168, train_loss: 1.4243\n",
            "124/168, train_loss: 0.3009\n",
            "125/168, train_loss: 0.7271\n",
            "126/168, train_loss: 0.3950\n",
            "127/168, train_loss: 0.4874\n",
            "128/168, train_loss: 0.9719\n",
            "129/168, train_loss: 0.3904\n",
            "130/168, train_loss: 0.7188\n",
            "131/168, train_loss: 0.4105\n",
            "132/168, train_loss: 0.4669\n",
            "133/168, train_loss: 0.4488\n",
            "134/168, train_loss: 0.5187\n",
            "135/168, train_loss: 0.1254\n",
            "136/168, train_loss: 0.4588\n",
            "137/168, train_loss: 1.2322\n",
            "138/168, train_loss: 0.3177\n",
            "139/168, train_loss: 0.9323\n",
            "140/168, train_loss: 0.3027\n",
            "141/168, train_loss: 1.1694\n",
            "142/168, train_loss: 0.3640\n",
            "143/168, train_loss: 0.1199\n",
            "144/168, train_loss: 0.4108\n",
            "145/168, train_loss: 0.3541\n",
            "146/168, train_loss: 0.4128\n",
            "147/168, train_loss: 0.4899\n",
            "148/168, train_loss: 0.1510\n",
            "149/168, train_loss: 0.4528\n",
            "150/168, train_loss: 0.7393\n",
            "151/168, train_loss: 1.6567\n",
            "152/168, train_loss: 0.7442\n",
            "153/168, train_loss: 1.1062\n",
            "154/168, train_loss: 0.1877\n",
            "155/168, train_loss: 0.5115\n",
            "156/168, train_loss: 0.2613\n",
            "157/168, train_loss: 0.9668\n",
            "158/168, train_loss: 0.4051\n",
            "159/168, train_loss: 0.4672\n",
            "160/168, train_loss: 0.2307\n",
            "161/168, train_loss: 0.4315\n",
            "162/168, train_loss: 0.5235\n",
            "163/168, train_loss: 1.0792\n",
            "164/168, train_loss: 0.3469\n",
            "165/168, train_loss: 0.3777\n",
            "166/168, train_loss: 0.2978\n",
            "167/168, train_loss: 0.5007\n",
            "168/168, train_loss: 1.1740\n",
            "169/168, train_loss: 0.3842\n",
            "epoch 45 average loss: 0.4999\n",
            "----------\n",
            "epoch 46/5\n",
            "1/168, train_loss: 0.5421\n",
            "2/168, train_loss: 1.0997\n",
            "3/168, train_loss: 0.4977\n",
            "4/168, train_loss: 0.1307\n",
            "5/168, train_loss: 1.0541\n",
            "6/168, train_loss: 1.0775\n",
            "7/168, train_loss: 0.1497\n",
            "8/168, train_loss: 0.4419\n",
            "9/168, train_loss: 0.4404\n",
            "10/168, train_loss: 0.4296\n",
            "11/168, train_loss: 0.3819\n",
            "12/168, train_loss: 0.7443\n",
            "13/168, train_loss: 0.2413\n",
            "14/168, train_loss: 0.0670\n",
            "15/168, train_loss: 0.4326\n",
            "16/168, train_loss: 0.2167\n",
            "17/168, train_loss: 0.3855\n",
            "18/168, train_loss: 0.2736\n",
            "19/168, train_loss: 0.4247\n",
            "20/168, train_loss: 1.0557\n",
            "21/168, train_loss: 0.3920\n",
            "22/168, train_loss: 0.7314\n",
            "23/168, train_loss: 0.6379\n",
            "24/168, train_loss: 0.4631\n",
            "25/168, train_loss: 0.7278\n",
            "26/168, train_loss: 0.4661\n",
            "27/168, train_loss: 0.5163\n",
            "28/168, train_loss: 0.3996\n",
            "29/168, train_loss: 0.6453\n",
            "30/168, train_loss: 0.4164\n",
            "31/168, train_loss: 0.7270\n",
            "32/168, train_loss: 1.1666\n",
            "33/168, train_loss: 0.7267\n",
            "34/168, train_loss: 0.5097\n",
            "35/168, train_loss: 0.7243\n",
            "36/168, train_loss: 1.0733\n",
            "37/168, train_loss: 0.7208\n",
            "38/168, train_loss: 1.2304\n",
            "39/168, train_loss: 0.4168\n",
            "40/168, train_loss: 0.9297\n",
            "41/168, train_loss: 0.5167\n",
            "42/168, train_loss: 0.3747\n",
            "43/168, train_loss: 0.3643\n",
            "44/168, train_loss: 0.3577\n",
            "45/168, train_loss: 0.7561\n",
            "46/168, train_loss: 0.3738\n",
            "47/168, train_loss: 0.3551\n",
            "48/168, train_loss: 0.3714\n",
            "49/168, train_loss: 0.4063\n",
            "50/168, train_loss: 0.3737\n",
            "51/168, train_loss: 0.4552\n",
            "52/168, train_loss: 0.4535\n",
            "53/168, train_loss: 0.3806\n",
            "54/168, train_loss: 0.4231\n",
            "55/168, train_loss: 0.3695\n",
            "56/168, train_loss: 0.4615\n",
            "57/168, train_loss: 0.7318\n",
            "58/168, train_loss: 0.1487\n",
            "59/168, train_loss: 0.1171\n",
            "60/168, train_loss: 1.1020\n",
            "61/168, train_loss: 0.3689\n",
            "62/168, train_loss: 1.0687\n",
            "63/168, train_loss: 0.6343\n",
            "64/168, train_loss: 1.3516\n",
            "65/168, train_loss: 0.7274\n",
            "66/168, train_loss: 0.5914\n",
            "67/168, train_loss: 0.2603\n",
            "68/168, train_loss: 1.1135\n",
            "69/168, train_loss: 0.3584\n",
            "70/168, train_loss: 0.7218\n",
            "71/168, train_loss: 0.4433\n",
            "72/168, train_loss: 0.4081\n",
            "73/168, train_loss: 1.0657\n",
            "74/168, train_loss: 0.1962\n",
            "75/168, train_loss: 0.2466\n",
            "76/168, train_loss: 0.3722\n",
            "77/168, train_loss: 0.3720\n",
            "78/168, train_loss: 0.7141\n",
            "79/168, train_loss: 0.4657\n",
            "80/168, train_loss: 0.3727\n",
            "81/168, train_loss: 0.4611\n",
            "82/168, train_loss: 0.4478\n",
            "83/168, train_loss: 0.4413\n",
            "84/168, train_loss: 0.1172\n",
            "85/168, train_loss: 0.5266\n",
            "86/168, train_loss: 1.0904\n",
            "87/168, train_loss: 0.5579\n",
            "88/168, train_loss: 0.4217\n",
            "89/168, train_loss: 0.7174\n",
            "90/168, train_loss: 0.3886\n",
            "91/168, train_loss: 0.4449\n",
            "92/168, train_loss: 0.7188\n",
            "93/168, train_loss: 0.1516\n",
            "94/168, train_loss: 0.4064\n",
            "95/168, train_loss: 0.7169\n",
            "96/168, train_loss: 0.2181\n",
            "97/168, train_loss: 0.3766\n",
            "98/168, train_loss: 0.0916\n",
            "99/168, train_loss: 1.0042\n",
            "100/168, train_loss: 0.3466\n",
            "101/168, train_loss: 0.1333\n",
            "102/168, train_loss: 0.4092\n",
            "103/168, train_loss: 0.1337\n",
            "104/168, train_loss: 0.7116\n",
            "105/168, train_loss: 0.4216\n",
            "106/168, train_loss: 1.3206\n",
            "107/168, train_loss: 0.2119\n",
            "108/168, train_loss: 0.1329\n",
            "109/168, train_loss: 0.5452\n",
            "110/168, train_loss: 0.5135\n",
            "111/168, train_loss: 0.5441\n",
            "112/168, train_loss: 0.7073\n",
            "113/168, train_loss: 1.1061\n",
            "114/168, train_loss: 0.5080\n",
            "115/168, train_loss: 0.2622\n",
            "116/168, train_loss: 0.4660\n",
            "117/168, train_loss: 0.4902\n",
            "118/168, train_loss: 0.4078\n",
            "119/168, train_loss: 1.0404\n",
            "120/168, train_loss: 0.4218\n",
            "121/168, train_loss: 0.0984\n",
            "122/168, train_loss: 0.4175\n",
            "123/168, train_loss: 0.5746\n",
            "124/168, train_loss: 0.4477\n",
            "125/168, train_loss: 0.2956\n",
            "126/168, train_loss: 1.0348\n",
            "127/168, train_loss: 0.5498\n",
            "128/168, train_loss: 0.3444\n",
            "129/168, train_loss: 0.3968\n",
            "130/168, train_loss: 0.3558\n",
            "131/168, train_loss: 0.2056\n",
            "132/168, train_loss: 0.4549\n",
            "133/168, train_loss: 1.1345\n",
            "134/168, train_loss: 0.5145\n",
            "135/168, train_loss: 0.3898\n",
            "136/168, train_loss: 0.4380\n",
            "137/168, train_loss: 0.7200\n",
            "138/168, train_loss: 0.2009\n",
            "139/168, train_loss: 0.4305\n",
            "140/168, train_loss: 0.7193\n",
            "141/168, train_loss: 0.4612\n",
            "142/168, train_loss: 0.4950\n",
            "143/168, train_loss: 0.1710\n",
            "144/168, train_loss: 0.7222\n",
            "145/168, train_loss: 0.4477\n",
            "146/168, train_loss: 0.0870\n",
            "147/168, train_loss: 0.4109\n",
            "148/168, train_loss: 0.4750\n",
            "149/168, train_loss: 0.5292\n",
            "150/168, train_loss: 0.4498\n",
            "151/168, train_loss: 0.9655\n",
            "152/168, train_loss: 0.3734\n",
            "153/168, train_loss: 0.4427\n",
            "154/168, train_loss: 0.7268\n",
            "155/168, train_loss: 0.5223\n",
            "156/168, train_loss: 0.4426\n",
            "157/168, train_loss: 0.3064\n",
            "158/168, train_loss: 0.4724\n",
            "159/168, train_loss: 0.4648\n",
            "160/168, train_loss: 0.5688\n",
            "161/168, train_loss: 0.1490\n",
            "162/168, train_loss: 1.1776\n",
            "163/168, train_loss: 0.5537\n",
            "164/168, train_loss: 0.0990\n",
            "165/168, train_loss: 0.4019\n",
            "166/168, train_loss: 0.4215\n",
            "167/168, train_loss: 1.0889\n",
            "168/168, train_loss: 0.1390\n",
            "169/168, train_loss: 0.3803\n",
            "epoch 46 average loss: 0.5222\n",
            "current epoch: 46 current accuracy: 0.4118 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 47/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.5106\n",
            "2/168, train_loss: 0.4298\n",
            "3/168, train_loss: 1.1449\n",
            "4/168, train_loss: 0.7450\n",
            "5/168, train_loss: 0.7447\n",
            "6/168, train_loss: 0.1663\n",
            "7/168, train_loss: 0.3821\n",
            "8/168, train_loss: 0.4284\n",
            "9/168, train_loss: 0.1318\n",
            "10/168, train_loss: 0.4400\n",
            "11/168, train_loss: 0.4308\n",
            "12/168, train_loss: 0.0922\n",
            "13/168, train_loss: 0.4236\n",
            "14/168, train_loss: 0.4000\n",
            "15/168, train_loss: 0.4173\n",
            "16/168, train_loss: 0.6259\n",
            "17/168, train_loss: 0.3489\n",
            "18/168, train_loss: 0.7031\n",
            "19/168, train_loss: 0.3829\n",
            "20/168, train_loss: 0.7219\n",
            "21/168, train_loss: 0.2853\n",
            "22/168, train_loss: 0.2549\n",
            "23/168, train_loss: 0.4310\n",
            "24/168, train_loss: 0.7586\n",
            "25/168, train_loss: 0.1315\n",
            "26/168, train_loss: 0.3062\n",
            "27/168, train_loss: 0.7533\n",
            "28/168, train_loss: 0.4166\n",
            "29/168, train_loss: 0.1492\n",
            "30/168, train_loss: 0.2340\n",
            "31/168, train_loss: 0.7450\n",
            "32/168, train_loss: 0.7423\n",
            "33/168, train_loss: 0.4341\n",
            "34/168, train_loss: 0.4087\n",
            "35/168, train_loss: 0.4060\n",
            "36/168, train_loss: 0.7401\n",
            "37/168, train_loss: 0.1019\n",
            "38/168, train_loss: 0.2884\n",
            "39/168, train_loss: 0.7373\n",
            "40/168, train_loss: 1.6001\n",
            "41/168, train_loss: 0.4640\n",
            "42/168, train_loss: 0.7323\n",
            "43/168, train_loss: 0.3895\n",
            "44/168, train_loss: 0.1576\n",
            "45/168, train_loss: 0.5682\n",
            "46/168, train_loss: 0.7258\n",
            "47/168, train_loss: 0.2382\n",
            "48/168, train_loss: 1.3035\n",
            "49/168, train_loss: 1.2877\n",
            "50/168, train_loss: 0.9555\n",
            "51/168, train_loss: 0.2101\n",
            "52/168, train_loss: 0.4621\n",
            "53/168, train_loss: 0.1353\n",
            "54/168, train_loss: 0.7130\n",
            "55/168, train_loss: 0.3888\n",
            "56/168, train_loss: 0.2204\n",
            "57/168, train_loss: 0.4767\n",
            "58/168, train_loss: 0.1339\n",
            "59/168, train_loss: 0.3658\n",
            "60/168, train_loss: 0.7352\n",
            "61/168, train_loss: 0.4519\n",
            "62/168, train_loss: 0.9220\n",
            "63/168, train_loss: 0.3770\n",
            "64/168, train_loss: 0.2297\n",
            "65/168, train_loss: 1.0276\n",
            "66/168, train_loss: 0.5327\n",
            "67/168, train_loss: 0.3639\n",
            "68/168, train_loss: 0.3550\n",
            "69/168, train_loss: 0.3432\n",
            "70/168, train_loss: 0.3885\n",
            "71/168, train_loss: 0.1220\n",
            "72/168, train_loss: 0.4948\n",
            "73/168, train_loss: 0.2834\n",
            "74/168, train_loss: 0.1292\n",
            "75/168, train_loss: 0.7296\n",
            "76/168, train_loss: 0.1402\n",
            "77/168, train_loss: 0.3714\n",
            "78/168, train_loss: 0.3723\n",
            "79/168, train_loss: 0.7276\n",
            "80/168, train_loss: 0.4192\n",
            "81/168, train_loss: 0.3412\n",
            "82/168, train_loss: 0.1309\n",
            "83/168, train_loss: 0.0664\n",
            "84/168, train_loss: 0.5184\n",
            "85/168, train_loss: 0.2036\n",
            "86/168, train_loss: 0.1012\n",
            "87/168, train_loss: 1.0206\n",
            "88/168, train_loss: 0.1498\n",
            "89/168, train_loss: 0.0934\n",
            "90/168, train_loss: 0.3683\n",
            "91/168, train_loss: 0.4618\n",
            "92/168, train_loss: 0.4765\n",
            "93/168, train_loss: 0.6318\n",
            "94/168, train_loss: 0.5355\n",
            "95/168, train_loss: 1.3378\n",
            "96/168, train_loss: 0.4797\n",
            "97/168, train_loss: 0.7263\n",
            "98/168, train_loss: 0.3904\n",
            "99/168, train_loss: 0.4585\n",
            "100/168, train_loss: 0.4720\n",
            "101/168, train_loss: 0.9326\n",
            "102/168, train_loss: 0.4000\n",
            "103/168, train_loss: 0.2386\n",
            "104/168, train_loss: 0.1485\n",
            "105/168, train_loss: 0.3511\n",
            "106/168, train_loss: 0.2638\n",
            "107/168, train_loss: 1.0652\n",
            "108/168, train_loss: 0.2938\n",
            "109/168, train_loss: 0.4810\n",
            "110/168, train_loss: 0.4165\n",
            "111/168, train_loss: 1.2701\n",
            "112/168, train_loss: 0.3681\n",
            "113/168, train_loss: 0.3645\n",
            "114/168, train_loss: 0.2430\n",
            "115/168, train_loss: 0.7370\n",
            "116/168, train_loss: 0.5062\n",
            "117/168, train_loss: 0.4749\n",
            "118/168, train_loss: 1.0623\n",
            "119/168, train_loss: 0.5500\n",
            "120/168, train_loss: 0.3970\n",
            "121/168, train_loss: 0.0772\n",
            "122/168, train_loss: 0.4574\n",
            "123/168, train_loss: 0.4542\n",
            "124/168, train_loss: 0.1953\n",
            "125/168, train_loss: 0.5152\n",
            "126/168, train_loss: 0.4580\n",
            "127/168, train_loss: 0.0975\n",
            "128/168, train_loss: 0.3896\n",
            "129/168, train_loss: 0.1536\n",
            "130/168, train_loss: 0.1526\n",
            "131/168, train_loss: 1.0391\n",
            "132/168, train_loss: 0.3361\n",
            "133/168, train_loss: 0.2063\n",
            "134/168, train_loss: 0.4095\n",
            "135/168, train_loss: 0.4360\n",
            "136/168, train_loss: 0.0788\n",
            "137/168, train_loss: 0.0817\n",
            "138/168, train_loss: 1.1225\n",
            "139/168, train_loss: 0.5416\n",
            "140/168, train_loss: 0.4122\n",
            "141/168, train_loss: 0.2889\n",
            "142/168, train_loss: 0.4006\n",
            "143/168, train_loss: 0.5774\n",
            "144/168, train_loss: 0.5934\n",
            "145/168, train_loss: 0.3759\n",
            "146/168, train_loss: 1.1776\n",
            "147/168, train_loss: 0.1022\n",
            "148/168, train_loss: 0.3609\n",
            "149/168, train_loss: 0.3540\n",
            "150/168, train_loss: 0.1023\n",
            "151/168, train_loss: 0.1914\n",
            "152/168, train_loss: 0.3381\n",
            "153/168, train_loss: 0.0851\n",
            "154/168, train_loss: 0.1827\n",
            "155/168, train_loss: 0.3357\n",
            "156/168, train_loss: 0.2159\n",
            "157/168, train_loss: 0.5404\n",
            "158/168, train_loss: 0.3528\n",
            "159/168, train_loss: 0.5133\n",
            "160/168, train_loss: 0.3720\n",
            "161/168, train_loss: 0.1727\n",
            "162/168, train_loss: 0.3807\n",
            "163/168, train_loss: 1.1996\n",
            "164/168, train_loss: 0.4087\n",
            "165/168, train_loss: 0.2371\n",
            "166/168, train_loss: 0.3338\n",
            "167/168, train_loss: 0.9116\n",
            "168/168, train_loss: 0.1479\n",
            "169/168, train_loss: 0.3654\n",
            "epoch 47 average loss: 0.4604\n",
            "----------\n",
            "epoch 48/5\n",
            "1/168, train_loss: 0.1055\n",
            "2/168, train_loss: 0.0878\n",
            "3/168, train_loss: 0.4663\n",
            "4/168, train_loss: 0.3484\n",
            "5/168, train_loss: 1.1396\n",
            "6/168, train_loss: 0.0846\n",
            "7/168, train_loss: 1.3452\n",
            "8/168, train_loss: 0.4499\n",
            "9/168, train_loss: 0.4344\n",
            "10/168, train_loss: 0.3769\n",
            "11/168, train_loss: 0.3551\n",
            "12/168, train_loss: 1.2315\n",
            "13/168, train_loss: 0.3927\n",
            "14/168, train_loss: 0.4268\n",
            "15/168, train_loss: 0.4020\n",
            "16/168, train_loss: 0.4432\n",
            "17/168, train_loss: 0.1601\n",
            "18/168, train_loss: 0.7557\n",
            "19/168, train_loss: 0.1951\n",
            "20/168, train_loss: 0.7515\n",
            "21/168, train_loss: 0.4115\n",
            "22/168, train_loss: 0.4166\n",
            "23/168, train_loss: 0.7067\n",
            "24/168, train_loss: 0.7522\n",
            "25/168, train_loss: 0.3581\n",
            "26/168, train_loss: 0.1967\n",
            "27/168, train_loss: 0.1939\n",
            "28/168, train_loss: 1.0878\n",
            "29/168, train_loss: 0.0511\n",
            "30/168, train_loss: 0.6164\n",
            "31/168, train_loss: 1.7980\n",
            "32/168, train_loss: 0.3437\n",
            "33/168, train_loss: 0.7377\n",
            "34/168, train_loss: 0.2110\n",
            "35/168, train_loss: 0.3734\n",
            "36/168, train_loss: 0.3298\n",
            "37/168, train_loss: 0.1065\n",
            "38/168, train_loss: 0.3249\n",
            "39/168, train_loss: 0.1390\n",
            "40/168, train_loss: 0.3036\n",
            "41/168, train_loss: 0.2785\n",
            "42/168, train_loss: 0.4017\n",
            "43/168, train_loss: 0.4160\n",
            "44/168, train_loss: 0.1583\n",
            "45/168, train_loss: 0.4509\n",
            "46/168, train_loss: 0.4441\n",
            "47/168, train_loss: 0.3011\n",
            "48/168, train_loss: 0.4580\n",
            "49/168, train_loss: 0.4764\n",
            "50/168, train_loss: 0.6305\n",
            "51/168, train_loss: 0.3934\n",
            "52/168, train_loss: 0.1799\n",
            "53/168, train_loss: 0.4867\n",
            "54/168, train_loss: 0.7503\n",
            "55/168, train_loss: 0.5249\n",
            "56/168, train_loss: 0.4874\n",
            "57/168, train_loss: 0.1159\n",
            "58/168, train_loss: 0.4139\n",
            "59/168, train_loss: 0.3771\n",
            "60/168, train_loss: 0.0772\n",
            "61/168, train_loss: 0.5736\n",
            "62/168, train_loss: 0.1125\n",
            "63/168, train_loss: 1.2202\n",
            "64/168, train_loss: 0.9635\n",
            "65/168, train_loss: 0.1035\n",
            "66/168, train_loss: 0.7639\n",
            "67/168, train_loss: 0.3792\n",
            "68/168, train_loss: 0.0858\n",
            "69/168, train_loss: 0.6147\n",
            "70/168, train_loss: 0.6108\n",
            "71/168, train_loss: 1.1209\n",
            "72/168, train_loss: 1.2166\n",
            "73/168, train_loss: 0.3868\n",
            "74/168, train_loss: 1.0732\n",
            "75/168, train_loss: 0.0820\n",
            "76/168, train_loss: 1.2769\n",
            "77/168, train_loss: 0.3718\n",
            "78/168, train_loss: 0.3572\n",
            "79/168, train_loss: 0.5069\n",
            "80/168, train_loss: 0.7410\n",
            "81/168, train_loss: 0.8084\n",
            "82/168, train_loss: 0.3445\n",
            "83/168, train_loss: 0.3941\n",
            "84/168, train_loss: 0.3255\n",
            "85/168, train_loss: 0.3489\n",
            "86/168, train_loss: 0.4507\n",
            "87/168, train_loss: 0.3564\n",
            "88/168, train_loss: 0.4039\n",
            "89/168, train_loss: 1.1300\n",
            "90/168, train_loss: 0.3999\n",
            "91/168, train_loss: 0.4314\n",
            "92/168, train_loss: 1.3349\n",
            "93/168, train_loss: 0.1391\n",
            "94/168, train_loss: 0.6244\n",
            "95/168, train_loss: 0.7333\n",
            "96/168, train_loss: 0.4490\n",
            "97/168, train_loss: 0.1347\n",
            "98/168, train_loss: 0.5925\n",
            "99/168, train_loss: 0.1291\n",
            "100/168, train_loss: 0.1672\n",
            "101/168, train_loss: 0.3896\n",
            "102/168, train_loss: 0.4678\n",
            "103/168, train_loss: 0.7291\n",
            "104/168, train_loss: 0.3849\n",
            "105/168, train_loss: 0.4826\n",
            "106/168, train_loss: 0.0826\n",
            "107/168, train_loss: 0.4476\n",
            "108/168, train_loss: 0.4936\n",
            "109/168, train_loss: 0.4927\n",
            "110/168, train_loss: 0.1495\n",
            "111/168, train_loss: 1.2088\n",
            "112/168, train_loss: 0.4757\n",
            "113/168, train_loss: 0.0801\n",
            "114/168, train_loss: 0.4105\n",
            "115/168, train_loss: 0.4801\n",
            "116/168, train_loss: 0.0799\n",
            "117/168, train_loss: 0.7520\n",
            "118/168, train_loss: 1.0935\n",
            "119/168, train_loss: 0.7439\n",
            "120/168, train_loss: 0.1479\n",
            "121/168, train_loss: 0.4610\n",
            "122/168, train_loss: 0.7340\n",
            "123/168, train_loss: 0.3806\n",
            "124/168, train_loss: 0.0847\n",
            "125/168, train_loss: 0.4283\n",
            "126/168, train_loss: 0.3663\n",
            "127/168, train_loss: 0.1012\n",
            "128/168, train_loss: 0.5785\n",
            "129/168, train_loss: 0.4557\n",
            "130/168, train_loss: 0.4283\n",
            "131/168, train_loss: 0.3707\n",
            "132/168, train_loss: 0.6696\n",
            "133/168, train_loss: 1.0049\n",
            "134/168, train_loss: 0.7232\n",
            "135/168, train_loss: 0.7199\n",
            "136/168, train_loss: 0.6131\n",
            "137/168, train_loss: 0.4947\n",
            "138/168, train_loss: 0.1176\n",
            "139/168, train_loss: 0.3426\n",
            "140/168, train_loss: 0.1874\n",
            "141/168, train_loss: 0.3696\n",
            "142/168, train_loss: 0.0706\n",
            "143/168, train_loss: 0.4291\n",
            "144/168, train_loss: 0.6325\n",
            "145/168, train_loss: 0.4120\n",
            "146/168, train_loss: 0.3193\n",
            "147/168, train_loss: 0.7393\n",
            "148/168, train_loss: 0.4334\n",
            "149/168, train_loss: 0.4516\n",
            "150/168, train_loss: 0.4152\n",
            "151/168, train_loss: 0.4387\n",
            "152/168, train_loss: 1.1131\n",
            "153/168, train_loss: 1.0690\n",
            "154/168, train_loss: 1.1962\n",
            "155/168, train_loss: 0.7361\n",
            "156/168, train_loss: 0.2476\n",
            "157/168, train_loss: 1.2987\n",
            "158/168, train_loss: 1.1345\n",
            "159/168, train_loss: 0.7271\n",
            "160/168, train_loss: 0.9656\n",
            "161/168, train_loss: 0.4804\n",
            "162/168, train_loss: 0.4834\n",
            "163/168, train_loss: 0.0995\n",
            "164/168, train_loss: 0.4858\n",
            "165/168, train_loss: 0.1338\n",
            "166/168, train_loss: 0.7269\n",
            "167/168, train_loss: 0.4664\n",
            "168/168, train_loss: 0.7296\n",
            "169/168, train_loss: 0.3486\n",
            "epoch 48 average loss: 0.5059\n",
            "current epoch: 48 current accuracy: 0.4235 best accuracy: 0.5412 at epoch 4\n",
            "----------\n",
            "epoch 49/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.2641\n",
            "2/168, train_loss: 0.2760\n",
            "3/168, train_loss: 0.3674\n",
            "4/168, train_loss: 0.6672\n",
            "5/168, train_loss: 0.7346\n",
            "6/168, train_loss: 0.8262\n",
            "7/168, train_loss: 0.0841\n",
            "8/168, train_loss: 0.0813\n",
            "9/168, train_loss: 0.3594\n",
            "10/168, train_loss: 0.4519\n",
            "11/168, train_loss: 0.0955\n",
            "12/168, train_loss: 1.3074\n",
            "13/168, train_loss: 0.3192\n",
            "14/168, train_loss: 1.0485\n",
            "15/168, train_loss: 0.3757\n",
            "16/168, train_loss: 0.4394\n",
            "17/168, train_loss: 0.4209\n",
            "18/168, train_loss: 0.7492\n",
            "19/168, train_loss: 0.5089\n",
            "20/168, train_loss: 0.3509\n",
            "21/168, train_loss: 0.6438\n",
            "22/168, train_loss: 1.1632\n",
            "23/168, train_loss: 0.4496\n",
            "24/168, train_loss: 0.4704\n",
            "25/168, train_loss: 1.1347\n",
            "26/168, train_loss: 1.5948\n",
            "27/168, train_loss: 0.2146\n",
            "28/168, train_loss: 0.2858\n",
            "29/168, train_loss: 0.6343\n",
            "30/168, train_loss: 0.3975\n",
            "31/168, train_loss: 1.1474\n",
            "32/168, train_loss: 0.7624\n",
            "33/168, train_loss: 0.4534\n",
            "34/168, train_loss: 0.5310\n",
            "35/168, train_loss: 0.0920\n",
            "36/168, train_loss: 0.3876\n",
            "37/168, train_loss: 0.1167\n",
            "38/168, train_loss: 1.2228\n",
            "39/168, train_loss: 0.3199\n",
            "40/168, train_loss: 0.3744\n",
            "41/168, train_loss: 0.3623\n",
            "42/168, train_loss: 0.7605\n",
            "43/168, train_loss: 0.6897\n",
            "44/168, train_loss: 0.3962\n",
            "45/168, train_loss: 0.1938\n",
            "46/168, train_loss: 0.3628\n",
            "47/168, train_loss: 0.3139\n",
            "48/168, train_loss: 0.1612\n",
            "49/168, train_loss: 0.5571\n",
            "50/168, train_loss: 0.4110\n",
            "51/168, train_loss: 0.1204\n",
            "52/168, train_loss: 0.5384\n",
            "53/168, train_loss: 0.1344\n",
            "54/168, train_loss: 1.0077\n",
            "55/168, train_loss: 0.5377\n",
            "56/168, train_loss: 0.0714\n",
            "57/168, train_loss: 0.7882\n",
            "58/168, train_loss: 0.2584\n",
            "59/168, train_loss: 0.3865\n",
            "60/168, train_loss: 0.0846\n",
            "61/168, train_loss: 0.3322\n",
            "62/168, train_loss: 0.4003\n",
            "63/168, train_loss: 1.0834\n",
            "64/168, train_loss: 0.2982\n",
            "65/168, train_loss: 0.7742\n",
            "66/168, train_loss: 0.4527\n",
            "67/168, train_loss: 0.4017\n",
            "68/168, train_loss: 1.1428\n",
            "69/168, train_loss: 0.5116\n",
            "70/168, train_loss: 0.3952\n",
            "71/168, train_loss: 0.5291\n",
            "72/168, train_loss: 0.3949\n",
            "73/168, train_loss: 1.4277\n",
            "74/168, train_loss: 0.4290\n",
            "75/168, train_loss: 0.4695\n",
            "76/168, train_loss: 0.3880\n",
            "77/168, train_loss: 0.3810\n",
            "78/168, train_loss: 0.4159\n",
            "79/168, train_loss: 1.1863\n",
            "80/168, train_loss: 0.5308\n",
            "81/168, train_loss: 0.3947\n",
            "82/168, train_loss: 0.1785\n",
            "83/168, train_loss: 0.4789\n",
            "84/168, train_loss: 0.3423\n",
            "85/168, train_loss: 0.3756\n",
            "86/168, train_loss: 0.5278\n",
            "87/168, train_loss: 0.1361\n",
            "88/168, train_loss: 0.7585\n",
            "89/168, train_loss: 1.4297\n",
            "90/168, train_loss: 0.1813\n",
            "91/168, train_loss: 0.4677\n",
            "92/168, train_loss: 0.1076\n",
            "93/168, train_loss: 0.4135\n",
            "94/168, train_loss: 0.4620\n",
            "95/168, train_loss: 0.7537\n",
            "96/168, train_loss: 0.3623\n",
            "97/168, train_loss: 0.7534\n",
            "98/168, train_loss: 0.4066\n",
            "99/168, train_loss: 1.1061\n",
            "100/168, train_loss: 0.3670\n",
            "101/168, train_loss: 0.5748\n",
            "102/168, train_loss: 0.7432\n",
            "103/168, train_loss: 0.6393\n",
            "104/168, train_loss: 1.0372\n",
            "105/168, train_loss: 0.4138\n",
            "106/168, train_loss: 0.4245\n",
            "107/168, train_loss: 0.5183\n",
            "108/168, train_loss: 0.4192\n",
            "109/168, train_loss: 0.4758\n",
            "110/168, train_loss: 0.2589\n",
            "111/168, train_loss: 0.3615\n",
            "112/168, train_loss: 0.1619\n",
            "113/168, train_loss: 0.9927\n",
            "114/168, train_loss: 0.3137\n",
            "115/168, train_loss: 0.3820\n",
            "116/168, train_loss: 0.4750\n",
            "117/168, train_loss: 1.7932\n",
            "118/168, train_loss: 0.2366\n",
            "119/168, train_loss: 0.3692\n",
            "120/168, train_loss: 0.5839\n",
            "121/168, train_loss: 0.4254\n",
            "122/168, train_loss: 0.6109\n",
            "123/168, train_loss: 0.4425\n",
            "124/168, train_loss: 0.3436\n",
            "125/168, train_loss: 0.8438\n",
            "126/168, train_loss: 0.1742\n",
            "127/168, train_loss: 0.6060\n",
            "128/168, train_loss: 0.2878\n",
            "129/168, train_loss: 0.1495\n",
            "130/168, train_loss: 1.1344\n",
            "131/168, train_loss: 0.5612\n",
            "132/168, train_loss: 0.1877\n",
            "133/168, train_loss: 0.2995\n",
            "134/168, train_loss: 0.2401\n",
            "135/168, train_loss: 0.4205\n",
            "136/168, train_loss: 0.2231\n",
            "137/168, train_loss: 0.7330\n",
            "138/168, train_loss: 0.7311\n",
            "139/168, train_loss: 0.4555\n",
            "140/168, train_loss: 0.4937\n",
            "141/168, train_loss: 0.4732\n",
            "142/168, train_loss: 1.0985\n",
            "143/168, train_loss: 0.3571\n",
            "144/168, train_loss: 0.5774\n",
            "145/168, train_loss: 0.3873\n",
            "146/168, train_loss: 0.4536\n",
            "147/168, train_loss: 0.4051\n",
            "148/168, train_loss: 0.3728\n",
            "149/168, train_loss: 0.3872\n",
            "150/168, train_loss: 0.2324\n",
            "151/168, train_loss: 0.3660\n",
            "152/168, train_loss: 0.5134\n",
            "153/168, train_loss: 0.6457\n",
            "154/168, train_loss: 0.7199\n",
            "155/168, train_loss: 0.4221\n",
            "156/168, train_loss: 0.2187\n",
            "157/168, train_loss: 0.4172\n",
            "158/168, train_loss: 0.2013\n",
            "159/168, train_loss: 1.1688\n",
            "160/168, train_loss: 0.6409\n",
            "161/168, train_loss: 1.0919\n",
            "162/168, train_loss: 0.4481\n",
            "163/168, train_loss: 1.1021\n",
            "164/168, train_loss: 0.7549\n",
            "165/168, train_loss: 0.7524\n",
            "166/168, train_loss: 0.2099\n",
            "167/168, train_loss: 0.1414\n",
            "168/168, train_loss: 1.3444\n",
            "169/168, train_loss: 0.4291\n",
            "epoch 49 average loss: 0.5271\n",
            "----------\n",
            "epoch 50/5\n",
            "1/168, train_loss: 0.4247\n",
            "2/168, train_loss: 0.2098\n",
            "3/168, train_loss: 0.2525\n",
            "4/168, train_loss: 0.4049\n",
            "5/168, train_loss: 0.3581\n",
            "6/168, train_loss: 0.3891\n",
            "7/168, train_loss: 0.2064\n",
            "8/168, train_loss: 0.6161\n",
            "9/168, train_loss: 0.5121\n",
            "10/168, train_loss: 0.3696\n",
            "11/168, train_loss: 0.3927\n",
            "12/168, train_loss: 0.3469\n",
            "13/168, train_loss: 0.7810\n",
            "14/168, train_loss: 0.3582\n",
            "15/168, train_loss: 0.6335\n",
            "16/168, train_loss: 1.0370\n",
            "17/168, train_loss: 0.5133\n",
            "18/168, train_loss: 1.0911\n",
            "19/168, train_loss: 0.2818\n",
            "20/168, train_loss: 0.4067\n",
            "21/168, train_loss: 0.3505\n",
            "22/168, train_loss: 0.5142\n",
            "23/168, train_loss: 0.3172\n",
            "24/168, train_loss: 0.1268\n",
            "25/168, train_loss: 0.4518\n",
            "26/168, train_loss: 0.3310\n",
            "27/168, train_loss: 1.2392\n",
            "28/168, train_loss: 0.7803\n",
            "29/168, train_loss: 0.3710\n",
            "30/168, train_loss: 0.0892\n",
            "31/168, train_loss: 0.5703\n",
            "32/168, train_loss: 0.4804\n",
            "33/168, train_loss: 0.8576\n",
            "34/168, train_loss: 0.4500\n",
            "35/168, train_loss: 0.6738\n",
            "36/168, train_loss: 0.3658\n",
            "37/168, train_loss: 1.2112\n",
            "38/168, train_loss: 0.4435\n",
            "39/168, train_loss: 0.4023\n",
            "40/168, train_loss: 1.1338\n",
            "41/168, train_loss: 1.1722\n",
            "42/168, train_loss: 0.0788\n",
            "43/168, train_loss: 0.5080\n",
            "44/168, train_loss: 0.4014\n",
            "45/168, train_loss: 1.1480\n",
            "46/168, train_loss: 0.4130\n",
            "47/168, train_loss: 0.7248\n",
            "48/168, train_loss: 0.1092\n",
            "49/168, train_loss: 0.4659\n",
            "50/168, train_loss: 0.7476\n",
            "51/168, train_loss: 0.1014\n",
            "52/168, train_loss: 0.4138\n",
            "53/168, train_loss: 0.4026\n",
            "54/168, train_loss: 0.4792\n",
            "55/168, train_loss: 0.4504\n",
            "56/168, train_loss: 0.5011\n",
            "57/168, train_loss: 0.4042\n",
            "58/168, train_loss: 0.4333\n",
            "59/168, train_loss: 0.2208\n",
            "60/168, train_loss: 0.4509\n",
            "61/168, train_loss: 0.6479\n",
            "62/168, train_loss: 0.4724\n",
            "63/168, train_loss: 0.4738\n",
            "64/168, train_loss: 0.5910\n",
            "65/168, train_loss: 0.2398\n",
            "66/168, train_loss: 0.3620\n",
            "67/168, train_loss: 0.4062\n",
            "68/168, train_loss: 1.0947\n",
            "69/168, train_loss: 0.5260\n",
            "70/168, train_loss: 0.1960\n",
            "71/168, train_loss: 0.3360\n",
            "72/168, train_loss: 0.4246\n",
            "73/168, train_loss: 0.4018\n",
            "74/168, train_loss: 0.4070\n",
            "75/168, train_loss: 0.3769\n",
            "76/168, train_loss: 0.3466\n",
            "77/168, train_loss: 0.3659\n",
            "78/168, train_loss: 0.7793\n",
            "79/168, train_loss: 0.1723\n",
            "80/168, train_loss: 0.1693\n",
            "81/168, train_loss: 0.3554\n",
            "82/168, train_loss: 0.3931\n",
            "83/168, train_loss: 1.1712\n",
            "84/168, train_loss: 0.7787\n",
            "85/168, train_loss: 0.4875\n",
            "86/168, train_loss: 0.3680\n",
            "87/168, train_loss: 1.0864\n",
            "88/168, train_loss: 0.3626\n",
            "89/168, train_loss: 1.0342\n",
            "90/168, train_loss: 0.3392\n",
            "91/168, train_loss: 0.1515\n",
            "92/168, train_loss: 0.3632\n",
            "93/168, train_loss: 1.2463\n",
            "94/168, train_loss: 0.1237\n",
            "95/168, train_loss: 0.4840\n",
            "96/168, train_loss: 0.3866\n",
            "97/168, train_loss: 1.1494\n",
            "98/168, train_loss: 0.2584\n",
            "99/168, train_loss: 0.4689\n",
            "100/168, train_loss: 0.3409\n",
            "101/168, train_loss: 0.7947\n",
            "102/168, train_loss: 0.0803\n",
            "103/168, train_loss: 0.5454\n",
            "104/168, train_loss: 0.1574\n",
            "105/168, train_loss: 0.2830\n",
            "106/168, train_loss: 0.4317\n",
            "107/168, train_loss: 0.3298\n",
            "108/168, train_loss: 0.3413\n",
            "109/168, train_loss: 1.1993\n",
            "110/168, train_loss: 0.3486\n",
            "111/168, train_loss: 1.1975\n",
            "112/168, train_loss: 0.1977\n",
            "113/168, train_loss: 0.1952\n",
            "114/168, train_loss: 0.3862\n",
            "115/168, train_loss: 0.1292\n",
            "116/168, train_loss: 1.2810\n",
            "117/168, train_loss: 0.3892\n",
            "118/168, train_loss: 0.3516\n",
            "119/168, train_loss: 0.4860\n",
            "120/168, train_loss: 0.3195\n",
            "121/168, train_loss: 0.7528\n",
            "122/168, train_loss: 1.1656\n",
            "123/168, train_loss: 0.3366\n",
            "124/168, train_loss: 0.3542\n",
            "125/168, train_loss: 1.1378\n",
            "126/168, train_loss: 0.4309\n",
            "127/168, train_loss: 1.6810\n",
            "128/168, train_loss: 0.3631\n",
            "129/168, train_loss: 1.1019\n",
            "130/168, train_loss: 0.3689\n",
            "131/168, train_loss: 0.7403\n",
            "132/168, train_loss: 1.1372\n",
            "133/168, train_loss: 0.7361\n",
            "134/168, train_loss: 0.1189\n",
            "135/168, train_loss: 0.4453\n",
            "136/168, train_loss: 0.3482\n",
            "137/168, train_loss: 1.1552\n",
            "138/168, train_loss: 0.5144\n",
            "139/168, train_loss: 0.3645\n",
            "140/168, train_loss: 0.0950\n",
            "141/168, train_loss: 0.7262\n",
            "142/168, train_loss: 0.4721\n",
            "143/168, train_loss: 0.9866\n",
            "144/168, train_loss: 0.7234\n",
            "145/168, train_loss: 0.3673\n",
            "146/168, train_loss: 0.5164\n",
            "147/168, train_loss: 0.0812\n",
            "148/168, train_loss: 0.7186\n",
            "149/168, train_loss: 0.3903\n",
            "150/168, train_loss: 0.3775\n",
            "151/168, train_loss: 1.0628\n",
            "152/168, train_loss: 0.9226\n",
            "153/168, train_loss: 0.3506\n",
            "154/168, train_loss: 0.0843\n",
            "155/168, train_loss: 0.3540\n",
            "156/168, train_loss: 0.5166\n",
            "157/168, train_loss: 0.1878\n",
            "158/168, train_loss: 0.1346\n",
            "159/168, train_loss: 0.1262\n",
            "160/168, train_loss: 0.5270\n",
            "161/168, train_loss: 0.1884\n",
            "162/168, train_loss: 0.5347\n",
            "163/168, train_loss: 1.0271\n",
            "164/168, train_loss: 0.1087\n",
            "165/168, train_loss: 0.7376\n",
            "166/168, train_loss: 0.9273\n",
            "167/168, train_loss: 0.7359\n",
            "168/168, train_loss: 0.4764\n",
            "169/168, train_loss: 0.3682\n",
            "epoch 50 average loss: 0.5209\n",
            "current epoch: 50 current accuracy: 0.4353 best accuracy: 0.5412 at epoch 4\n",
            "Fold 2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 1 is out of bounds for dimension 0 with size 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-6cb8b99e1d07>\u001b[0m in \u001b[0;36m<cell line: 194>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-6cb8b99e1d07>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mtrain_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mval_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing Code - Saving IMG"
      ],
      "metadata": {
        "id": "ppq0dQPkWw5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/Praktikum/3d_dataset_2/3d_patches_nii/'  # Directory containing the image files\n",
        "labels_csv = '/content/drive/MyDrive/Praktikum/labels.csv'  # Path to the CSV file containing labels"
      ],
      "metadata": {
        "id": "WWr7djXpd5Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import monai\n",
        "from monai.data import ImageDataset, DataLoader\n",
        "from monai.transforms import EnsureChannelFirst, Compose, RandRotate90, Resize, ScaleIntensity\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, f1_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_image_paths_and_labels(data_dir, labels_csv):\n",
        "    \"\"\"\n",
        "    Load image paths and corresponding labels from the directory and CSV file.\n",
        "\n",
        "    Args:\n",
        "    - data_dir (str): Directory containing the image files.\n",
        "    - labels_csv (str): Path to the CSV file containing labels.\n",
        "\n",
        "    Returns:\n",
        "    - image_paths (List[str]): List of image file paths.\n",
        "    - labels (List[int]): List of labels corresponding to the image file paths.\n",
        "    \"\"\"\n",
        "    # Read labels CSV\n",
        "    labels_df = pd.read_csv(labels_csv)\n",
        "\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    for _, row in labels_df.iterrows():\n",
        "        patient_id = row['PatientID']\n",
        "        label = row['Cancer']\n",
        "        # Find all image files for the patient\n",
        "        patient_images = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.startswith(patient_id)]\n",
        "        image_paths.extend(patient_images)\n",
        "        labels.extend([label] * len(patient_images))\n",
        "\n",
        "    return image_paths, labels\n",
        "\n",
        "def plot_and_save_metrics(epochs, train_metrics, val_metrics, metric_name, save_path):\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, train_metrics, label=f'Train {metric_name}')\n",
        "    plt.plot(epochs, val_metrics, label=f'Validation {metric_name}')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.legend()\n",
        "    plt.title(f'Train and Validation {metric_name}')\n",
        "    plt.savefig(os.path.join(save_path, f'{metric_name}.png'))\n",
        "\n",
        "def main():\n",
        "    monai.config.print_config()\n",
        "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "\n",
        "    # Define data paths\n",
        "    data_dir = '/content/drive/MyDrive/Praktikum/3d_dataset_2/3d_patches_nii/'  # Directory containing the image files\n",
        "    labels_csv = '/content/drive/MyDrive/Praktikum/labels.csv'  # Path to the CSV file containing labels\n",
        "\n",
        "    # Load image paths and labels\n",
        "    images, labels = load_image_paths_and_labels(data_dir, labels_csv)\n",
        "    labels = np.array(labels, dtype=np.int64)\n",
        "\n",
        "    # Debugging: Print number of images and labels\n",
        "    print(f\"Number of images: {len(images)}, Number of labels: {len(labels)}\")\n",
        "    print(f\"Labels distribution: {np.bincount(labels)}\")\n",
        "\n",
        "    # Define transforms\n",
        "    train_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(), Resize((96, 96, 96)), RandRotate90()])\n",
        "    val_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(), Resize((96, 96, 96))])\n",
        "\n",
        "    # Split dataset into training and validation sets\n",
        "    train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Debugging: Print train and validation label distributions\n",
        "    print(f\"Train labels distribution: {np.bincount(train_labels)}\")\n",
        "    print(f\"Validation labels distribution: {np.bincount(val_labels)}\")\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_ds = ImageDataset(image_files=train_images, labels=train_labels, transform=train_transforms)\n",
        "    val_ds = ImageDataset(image_files=val_images, labels=val_labels, transform=val_transforms)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "    val_loader = DataLoader(val_ds, batch_size=2, num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "    # Create DenseNet121, CrossEntropyLoss and Adam optimizer\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = monai.networks.nets.DenseNet121(spatial_dims=3, in_channels=1, out_channels=2).to(device)\n",
        "    loss_function = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), 1e-5)\n",
        "\n",
        "    val_interval = 2\n",
        "    best_metric = -1\n",
        "    epoch_loss_values = list()\n",
        "    epoch_accuracy_values = list()\n",
        "    epoch_recall_values = list()\n",
        "    epoch_auc_values = list()\n",
        "    epoch_f1_values = list()\n",
        "    epoch_mcc_values = list()\n",
        "    writer = SummaryWriter()\n",
        "\n",
        "    for epoch in range(5):\n",
        "        print(\"-\" * 10)\n",
        "        print(f\"epoch {epoch + 1}/{5}\")\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        step = 0\n",
        "        for batch_data in train_loader:\n",
        "            step += 1\n",
        "            inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_len = len(train_ds) // train_loader.batch_size\n",
        "            print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
        "            writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
        "        epoch_loss /= step\n",
        "        epoch_loss_values.append(epoch_loss)\n",
        "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        if (epoch + 1) % val_interval == 0:\n",
        "            model.eval()\n",
        "            val_preds = []\n",
        "            val_true = []\n",
        "            with torch.no_grad():\n",
        "                num_correct = 0.0\n",
        "                metric_count = 0\n",
        "                for val_data in val_loader:\n",
        "                    val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
        "                    val_outputs = model(val_images)\n",
        "                    val_preds.extend(val_outputs.argmax(dim=1).cpu().numpy())\n",
        "                    val_true.extend(val_labels.cpu().numpy())\n",
        "                    value = torch.eq(val_outputs.argmax(dim=1), val_labels)\n",
        "                    metric_count += len(value)\n",
        "                    num_correct += value.sum().item()\n",
        "            accuracy = accuracy_score(val_true, val_preds)\n",
        "            recall = recall_score(val_true, val_preds)\n",
        "            auc = roc_auc_score(val_true, val_preds)\n",
        "            f1 = f1_score(val_true, val_preds)\n",
        "            mcc = matthews_corrcoef(val_true, val_preds)\n",
        "\n",
        "            epoch_accuracy_values.append(accuracy)\n",
        "            epoch_recall_values.append(recall)\n",
        "            epoch_auc_values.append(auc)\n",
        "            epoch_f1_values.append(f1)\n",
        "            epoch_mcc_values.append(mcc)\n",
        "\n",
        "            if accuracy > best_metric:\n",
        "                best_metric = accuracy\n",
        "                best_metric_epoch = epoch + 1\n",
        "                torch.save(model.state_dict(), \"best_metric_model_classification3d.pth\")\n",
        "                print(\"saved new best metric model\")\n",
        "            print(\n",
        "                f\"current epoch: {epoch + 1} current accuracy: {accuracy:.4f} best accuracy: {best_metric:.4f} at epoch {best_metric_epoch}\"\n",
        "            )\n",
        "            writer.add_scalar(\"val_accuracy\", accuracy, epoch + 1)\n",
        "\n",
        "    # Save metrics as PNG\n",
        "    metrics = {\n",
        "        'accuracy': epoch_accuracy_values,\n",
        "        'recall': epoch_recall_values,\n",
        "        'auc': epoch_auc_values,\n",
        "        'f1': epoch_f1_values,\n",
        "        'mcc': epoch_mcc_values\n",
        "    }\n",
        "    for metric_name, metric_values in metrics.items():\n",
        "        plot_and_save_metrics(list(range(1, 6)), epoch_loss_values, metric_values, metric_name, '/content/')\n",
        "\n",
        "    print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
        "    writer.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pAVNYsZGWwD0",
        "outputId": "188919af-8350-410f-b425-d6b75746387c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MONAI version: 1.3.1\n",
            "Numpy version: 1.25.2\n",
            "Pytorch version: 2.3.0+cpu\n",
            "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
            "MONAI rev id: 96bfda00c6bd290297f5e3514ea227c6be4d08b4\n",
            "MONAI __file__: /usr/local/lib/python3.10/dist-packages/monai/__init__.py\n",
            "\n",
            "Optional dependencies:\n",
            "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "Nibabel version: 5.2.1\n",
            "scikit-image version: 0.19.3\n",
            "scipy version: 1.11.4\n",
            "Pillow version: 10.3.0\n",
            "Tensorboard version: 2.15.2\n",
            "gdown version: 4.7.3\n",
            "TorchVision version: 0.18.0+cpu\n",
            "tqdm version: 4.66.4\n",
            "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "psutil version: 5.9.5\n",
            "pandas version: 2.0.3\n",
            "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "transformers version: 4.41.2\n",
            "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "\n",
            "For details about installing the optional dependencies, please visit:\n",
            "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
            "\n",
            "Number of images: 422, Number of labels: 422\n",
            "Labels distribution: [264 158]\n",
            "Train labels distribution: [215 122]\n",
            "Validation labels distribution: [49 36]\n",
            "----------\n",
            "epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.6854\n",
            "2/168, train_loss: 0.7662\n",
            "3/168, train_loss: 0.7646\n",
            "4/168, train_loss: 0.7054\n",
            "5/168, train_loss: 0.7622\n",
            "6/168, train_loss: 0.7613\n",
            "7/168, train_loss: 0.7018\n",
            "8/168, train_loss: 0.6936\n",
            "9/168, train_loss: 0.7406\n",
            "10/168, train_loss: 0.7533\n",
            "11/168, train_loss: 0.7320\n",
            "12/168, train_loss: 0.7382\n",
            "13/168, train_loss: 0.7267\n",
            "14/168, train_loss: 0.7429\n",
            "15/168, train_loss: 0.6766\n",
            "16/168, train_loss: 0.7104\n",
            "17/168, train_loss: 0.7131\n",
            "18/168, train_loss: 0.6788\n",
            "19/168, train_loss: 0.7150\n",
            "20/168, train_loss: 0.5748\n",
            "21/168, train_loss: 0.7044\n",
            "22/168, train_loss: 0.6675\n",
            "23/168, train_loss: 0.7121\n",
            "24/168, train_loss: 0.9428\n",
            "25/168, train_loss: 0.6142\n",
            "26/168, train_loss: 0.7081\n",
            "27/168, train_loss: 0.6057\n",
            "28/168, train_loss: 0.5489\n",
            "29/168, train_loss: 0.8280\n",
            "30/168, train_loss: 0.7222\n",
            "31/168, train_loss: 0.6933\n",
            "32/168, train_loss: 0.6672\n",
            "33/168, train_loss: 0.5588\n",
            "34/168, train_loss: 0.6912\n",
            "35/168, train_loss: 0.7696\n",
            "36/168, train_loss: 0.8624\n",
            "37/168, train_loss: 0.6376\n",
            "38/168, train_loss: 0.6897\n",
            "39/168, train_loss: 0.6936\n",
            "40/168, train_loss: 0.7006\n",
            "41/168, train_loss: 0.6931\n",
            "42/168, train_loss: 0.6014\n",
            "43/168, train_loss: 0.6954\n",
            "44/168, train_loss: 0.7423\n",
            "45/168, train_loss: 0.6863\n",
            "46/168, train_loss: 0.6897\n",
            "47/168, train_loss: 0.7321\n",
            "48/168, train_loss: 0.5523\n",
            "49/168, train_loss: 0.7283\n",
            "50/168, train_loss: 0.6697\n",
            "51/168, train_loss: 0.6935\n",
            "52/168, train_loss: 0.6934\n",
            "53/168, train_loss: 0.6932\n",
            "54/168, train_loss: 0.6932\n",
            "55/168, train_loss: 0.9403\n",
            "56/168, train_loss: 0.6778\n",
            "57/168, train_loss: 0.6957\n",
            "58/168, train_loss: 0.6937\n",
            "59/168, train_loss: 0.6827\n",
            "60/168, train_loss: 0.6932\n",
            "61/168, train_loss: 0.5901\n",
            "62/168, train_loss: 0.6723\n",
            "63/168, train_loss: 0.8097\n",
            "64/168, train_loss: 0.6619\n",
            "65/168, train_loss: 0.6935\n",
            "66/168, train_loss: 0.6599\n",
            "67/168, train_loss: 0.6484\n",
            "68/168, train_loss: 0.6621\n",
            "69/168, train_loss: 0.6660\n",
            "70/168, train_loss: 0.6526\n",
            "71/168, train_loss: 0.6393\n",
            "72/168, train_loss: 0.7170\n",
            "73/168, train_loss: 0.7019\n",
            "74/168, train_loss: 0.6459\n",
            "75/168, train_loss: 0.7526\n",
            "76/168, train_loss: 0.6332\n",
            "77/168, train_loss: 0.6321\n",
            "78/168, train_loss: 0.7747\n",
            "79/168, train_loss: 0.6265\n",
            "80/168, train_loss: 0.6426\n",
            "81/168, train_loss: 0.6941\n",
            "82/168, train_loss: 0.7601\n",
            "83/168, train_loss: 0.7122\n",
            "84/168, train_loss: 0.7710\n",
            "85/168, train_loss: 0.6600\n",
            "86/168, train_loss: 0.7395\n",
            "87/168, train_loss: 0.6950\n",
            "88/168, train_loss: 0.7232\n",
            "89/168, train_loss: 0.6980\n",
            "90/168, train_loss: 0.7749\n",
            "91/168, train_loss: 0.6955\n",
            "92/168, train_loss: 0.7047\n",
            "93/168, train_loss: 0.6165\n",
            "94/168, train_loss: 0.7595\n",
            "95/168, train_loss: 0.7654\n",
            "96/168, train_loss: 0.6953\n",
            "97/168, train_loss: 0.7698\n",
            "98/168, train_loss: 0.7471\n",
            "99/168, train_loss: 0.6514\n",
            "100/168, train_loss: 0.6320\n",
            "101/168, train_loss: 0.6245\n",
            "102/168, train_loss: 0.7476\n",
            "103/168, train_loss: 0.7473\n",
            "104/168, train_loss: 0.6465\n",
            "105/168, train_loss: 0.7367\n",
            "106/168, train_loss: 0.6946\n",
            "107/168, train_loss: 0.6419\n",
            "108/168, train_loss: 0.6447\n",
            "109/168, train_loss: 0.7573\n",
            "110/168, train_loss: 0.6652\n",
            "111/168, train_loss: 0.6944\n",
            "112/168, train_loss: 0.6783\n",
            "113/168, train_loss: 0.6469\n",
            "114/168, train_loss: 0.6390\n",
            "115/168, train_loss: 0.6554\n",
            "116/168, train_loss: 0.7737\n",
            "117/168, train_loss: 0.6295\n",
            "118/168, train_loss: 0.6287\n",
            "119/168, train_loss: 0.6129\n",
            "120/168, train_loss: 0.6816\n",
            "121/168, train_loss: 0.6951\n",
            "122/168, train_loss: 0.6261\n",
            "123/168, train_loss: 0.6951\n",
            "124/168, train_loss: 0.7678\n",
            "125/168, train_loss: 0.8997\n",
            "126/168, train_loss: 0.6260\n",
            "127/168, train_loss: 0.6215\n",
            "128/168, train_loss: 0.6324\n",
            "129/168, train_loss: 0.8630\n",
            "130/168, train_loss: 0.6885\n",
            "131/168, train_loss: 0.6179\n",
            "132/168, train_loss: 0.6178\n",
            "133/168, train_loss: 0.6910\n",
            "134/168, train_loss: 0.7274\n",
            "135/168, train_loss: 0.6152\n",
            "136/168, train_loss: 0.7733\n",
            "137/168, train_loss: 0.5996\n",
            "138/168, train_loss: 0.6072\n",
            "139/168, train_loss: 0.6384\n",
            "140/168, train_loss: 0.5975\n",
            "141/168, train_loss: 0.6883\n",
            "142/168, train_loss: 0.6966\n",
            "143/168, train_loss: 0.8071\n",
            "144/168, train_loss: 0.5879\n",
            "145/168, train_loss: 0.7074\n",
            "146/168, train_loss: 0.5988\n",
            "147/168, train_loss: 0.6962\n",
            "148/168, train_loss: 0.5931\n",
            "149/168, train_loss: 0.5755\n",
            "150/168, train_loss: 0.6566\n",
            "151/168, train_loss: 0.7298\n",
            "152/168, train_loss: 0.5795\n",
            "153/168, train_loss: 0.7155\n",
            "154/168, train_loss: 0.6016\n",
            "155/168, train_loss: 0.5846\n",
            "156/168, train_loss: 0.5587\n",
            "157/168, train_loss: 0.8127\n",
            "158/168, train_loss: 0.5804\n",
            "159/168, train_loss: 0.5775\n",
            "160/168, train_loss: 0.5689\n",
            "161/168, train_loss: 0.6388\n",
            "162/168, train_loss: 0.5557\n",
            "163/168, train_loss: 0.5725\n",
            "164/168, train_loss: 0.8540\n",
            "165/168, train_loss: 0.7920\n",
            "166/168, train_loss: 0.5696\n",
            "167/168, train_loss: 0.5480\n",
            "168/168, train_loss: 0.5714\n",
            "169/168, train_loss: 0.5567\n",
            "epoch 1 average loss: 0.6851\n",
            "----------\n",
            "epoch 2/5\n",
            "1/168, train_loss: 0.8483\n",
            "2/168, train_loss: 0.8640\n",
            "3/168, train_loss: 0.7035\n",
            "4/168, train_loss: 0.5480\n",
            "5/168, train_loss: 0.6166\n",
            "6/168, train_loss: 0.5669\n",
            "7/168, train_loss: 0.5278\n",
            "8/168, train_loss: 0.6967\n",
            "9/168, train_loss: 0.8534\n",
            "10/168, train_loss: 0.7635\n",
            "11/168, train_loss: 0.6277\n",
            "12/168, train_loss: 0.8991\n",
            "13/168, train_loss: 0.6372\n",
            "14/168, train_loss: 0.5695\n",
            "15/168, train_loss: 0.5325\n",
            "16/168, train_loss: 0.5544\n",
            "17/168, train_loss: 0.7270\n",
            "18/168, train_loss: 0.6277\n",
            "19/168, train_loss: 0.6956\n",
            "20/168, train_loss: 0.5322\n",
            "21/168, train_loss: 0.5239\n",
            "22/168, train_loss: 0.5487\n",
            "23/168, train_loss: 0.7697\n",
            "24/168, train_loss: 0.7055\n",
            "25/168, train_loss: 0.5403\n",
            "26/168, train_loss: 0.7044\n",
            "27/168, train_loss: 0.9062\n",
            "28/168, train_loss: 0.6343\n",
            "29/168, train_loss: 0.5165\n",
            "30/168, train_loss: 0.6912\n",
            "31/168, train_loss: 0.9089\n",
            "32/168, train_loss: 0.7047\n",
            "33/168, train_loss: 0.5261\n",
            "34/168, train_loss: 0.6128\n",
            "35/168, train_loss: 0.5600\n",
            "36/168, train_loss: 0.6764\n",
            "37/168, train_loss: 0.6961\n",
            "38/168, train_loss: 0.7080\n",
            "39/168, train_loss: 0.8729\n",
            "40/168, train_loss: 0.7403\n",
            "41/168, train_loss: 0.7196\n",
            "42/168, train_loss: 0.5555\n",
            "43/168, train_loss: 0.5479\n",
            "44/168, train_loss: 0.5578\n",
            "45/168, train_loss: 0.8698\n",
            "46/168, train_loss: 0.5449\n",
            "47/168, train_loss: 0.5461\n",
            "48/168, train_loss: 0.8799\n",
            "49/168, train_loss: 0.5091\n",
            "50/168, train_loss: 0.6772\n",
            "51/168, train_loss: 0.7654\n",
            "52/168, train_loss: 0.7327\n",
            "53/168, train_loss: 0.5398\n",
            "54/168, train_loss: 0.6578\n",
            "55/168, train_loss: 0.6746\n",
            "56/168, train_loss: 0.5330\n",
            "57/168, train_loss: 0.7108\n",
            "58/168, train_loss: 0.5584\n",
            "59/168, train_loss: 0.5154\n",
            "60/168, train_loss: 0.7084\n",
            "61/168, train_loss: 0.7609\n",
            "62/168, train_loss: 0.5373\n",
            "63/168, train_loss: 0.5209\n",
            "64/168, train_loss: 0.8112\n",
            "65/168, train_loss: 0.6566\n",
            "66/168, train_loss: 0.5349\n",
            "67/168, train_loss: 0.5367\n",
            "68/168, train_loss: 0.6794\n",
            "69/168, train_loss: 0.7301\n",
            "70/168, train_loss: 0.9064\n",
            "71/168, train_loss: 0.5875\n",
            "72/168, train_loss: 0.5382\n",
            "73/168, train_loss: 0.5143\n",
            "74/168, train_loss: 0.9071\n",
            "75/168, train_loss: 0.7013\n",
            "76/168, train_loss: 0.7249\n",
            "77/168, train_loss: 0.5336\n",
            "78/168, train_loss: 0.5435\n",
            "79/168, train_loss: 0.7415\n",
            "80/168, train_loss: 0.5108\n",
            "81/168, train_loss: 0.5262\n",
            "82/168, train_loss: 0.6061\n",
            "83/168, train_loss: 0.5229\n",
            "84/168, train_loss: 0.6242\n",
            "85/168, train_loss: 0.5248\n",
            "86/168, train_loss: 0.6018\n",
            "87/168, train_loss: 0.8934\n",
            "88/168, train_loss: 0.5708\n",
            "89/168, train_loss: 0.5234\n",
            "90/168, train_loss: 0.4982\n",
            "91/168, train_loss: 0.6915\n",
            "92/168, train_loss: 0.5146\n",
            "93/168, train_loss: 0.5145\n",
            "94/168, train_loss: 0.5167\n",
            "95/168, train_loss: 0.5231\n",
            "96/168, train_loss: 0.9326\n",
            "97/168, train_loss: 0.5001\n",
            "98/168, train_loss: 0.8146\n",
            "99/168, train_loss: 0.5254\n",
            "100/168, train_loss: 0.7150\n",
            "101/168, train_loss: 0.6893\n",
            "102/168, train_loss: 0.7211\n",
            "103/168, train_loss: 0.5031\n",
            "104/168, train_loss: 0.7115\n",
            "105/168, train_loss: 0.4818\n",
            "106/168, train_loss: 0.5160\n",
            "107/168, train_loss: 0.4861\n",
            "108/168, train_loss: 1.0207\n",
            "109/168, train_loss: 0.5277\n",
            "110/168, train_loss: 0.5080\n",
            "111/168, train_loss: 0.5169\n",
            "112/168, train_loss: 0.5030\n",
            "113/168, train_loss: 0.4896\n",
            "114/168, train_loss: 0.9496\n",
            "115/168, train_loss: 0.9663\n",
            "116/168, train_loss: 0.5367\n",
            "117/168, train_loss: 0.7398\n",
            "118/168, train_loss: 0.5758\n",
            "119/168, train_loss: 0.8261\n",
            "120/168, train_loss: 0.7178\n",
            "121/168, train_loss: 0.4695\n",
            "122/168, train_loss: 0.5030\n",
            "123/168, train_loss: 0.4966\n",
            "124/168, train_loss: 1.0053\n",
            "125/168, train_loss: 0.4735\n",
            "126/168, train_loss: 0.7794\n",
            "127/168, train_loss: 0.4977\n",
            "128/168, train_loss: 0.5041\n",
            "129/168, train_loss: 0.6998\n",
            "130/168, train_loss: 0.9343\n",
            "131/168, train_loss: 0.7350\n",
            "132/168, train_loss: 0.9234\n",
            "133/168, train_loss: 0.4940\n",
            "134/168, train_loss: 0.5016\n",
            "135/168, train_loss: 0.6475\n",
            "136/168, train_loss: 0.7392\n",
            "137/168, train_loss: 0.4963\n",
            "138/168, train_loss: 0.6367\n",
            "139/168, train_loss: 0.6794\n",
            "140/168, train_loss: 0.4848\n",
            "141/168, train_loss: 0.7015\n",
            "142/168, train_loss: 0.8176\n",
            "143/168, train_loss: 0.7069\n",
            "144/168, train_loss: 0.8696\n",
            "145/168, train_loss: 0.6034\n",
            "146/168, train_loss: 0.9694\n",
            "147/168, train_loss: 0.4848\n",
            "148/168, train_loss: 0.4910\n",
            "149/168, train_loss: 0.7648\n",
            "150/168, train_loss: 0.5364\n",
            "151/168, train_loss: 0.5013\n",
            "152/168, train_loss: 0.6068\n",
            "153/168, train_loss: 0.4862\n",
            "154/168, train_loss: 0.9571\n",
            "155/168, train_loss: 0.6940\n",
            "156/168, train_loss: 0.4958\n",
            "157/168, train_loss: 0.4791\n",
            "158/168, train_loss: 0.9870\n",
            "159/168, train_loss: 0.5471\n",
            "160/168, train_loss: 0.4958\n",
            "161/168, train_loss: 0.8493\n",
            "162/168, train_loss: 0.4989\n",
            "163/168, train_loss: 0.5906\n",
            "164/168, train_loss: 0.6177\n",
            "165/168, train_loss: 0.9492\n",
            "166/168, train_loss: 0.7532\n",
            "167/168, train_loss: 0.4973\n",
            "168/168, train_loss: 0.4922\n",
            "169/168, train_loss: 0.4988\n",
            "epoch 2 average loss: 0.6479\n",
            "saved new best metric model\n",
            "current epoch: 2 current accuracy: 0.5294 best accuracy: 0.5294 at epoch 2\n",
            "----------\n",
            "epoch 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.4693\n",
            "2/168, train_loss: 0.5011\n",
            "3/168, train_loss: 0.9380\n",
            "4/168, train_loss: 0.4982\n",
            "5/168, train_loss: 0.4069\n",
            "6/168, train_loss: 0.7200\n",
            "7/168, train_loss: 0.6748\n",
            "8/168, train_loss: 0.6796\n",
            "9/168, train_loss: 0.4962\n",
            "10/168, train_loss: 0.4650\n",
            "11/168, train_loss: 0.9710\n",
            "12/168, train_loss: 0.5198\n",
            "13/168, train_loss: 0.5107\n",
            "14/168, train_loss: 0.5519\n",
            "15/168, train_loss: 0.4847\n",
            "16/168, train_loss: 0.9558\n",
            "17/168, train_loss: 0.5127\n",
            "18/168, train_loss: 0.5191\n",
            "19/168, train_loss: 0.6243\n",
            "20/168, train_loss: 0.9844\n",
            "21/168, train_loss: 0.5019\n",
            "22/168, train_loss: 0.4802\n",
            "23/168, train_loss: 0.5293\n",
            "24/168, train_loss: 0.8803\n",
            "25/168, train_loss: 1.0052\n",
            "26/168, train_loss: 0.5172\n",
            "27/168, train_loss: 0.4530\n",
            "28/168, train_loss: 0.7180\n",
            "29/168, train_loss: 0.4931\n",
            "30/168, train_loss: 0.6177\n",
            "31/168, train_loss: 0.8467\n",
            "32/168, train_loss: 0.9414\n",
            "33/168, train_loss: 0.8498\n",
            "34/168, train_loss: 0.9827\n",
            "35/168, train_loss: 0.6734\n",
            "36/168, train_loss: 0.4938\n",
            "37/168, train_loss: 0.4931\n",
            "38/168, train_loss: 0.7256\n",
            "39/168, train_loss: 0.5501\n",
            "40/168, train_loss: 0.5534\n",
            "41/168, train_loss: 0.5853\n",
            "42/168, train_loss: 0.9823\n",
            "43/168, train_loss: 0.5009\n",
            "44/168, train_loss: 0.4028\n",
            "45/168, train_loss: 0.4996\n",
            "46/168, train_loss: 0.5088\n",
            "47/168, train_loss: 0.7232\n",
            "48/168, train_loss: 0.9339\n",
            "49/168, train_loss: 0.5124\n",
            "50/168, train_loss: 0.5531\n",
            "51/168, train_loss: 0.7746\n",
            "52/168, train_loss: 0.5019\n",
            "53/168, train_loss: 0.4985\n",
            "54/168, train_loss: 0.8719\n",
            "55/168, train_loss: 0.9130\n",
            "56/168, train_loss: 0.6027\n",
            "57/168, train_loss: 0.5135\n",
            "58/168, train_loss: 0.9262\n",
            "59/168, train_loss: 0.4867\n",
            "60/168, train_loss: 0.9503\n",
            "61/168, train_loss: 0.4826\n",
            "62/168, train_loss: 0.5067\n",
            "63/168, train_loss: 0.4866\n",
            "64/168, train_loss: 0.5078\n",
            "65/168, train_loss: 0.4798\n",
            "66/168, train_loss: 0.4939\n",
            "67/168, train_loss: 0.8174\n",
            "68/168, train_loss: 1.0034\n",
            "69/168, train_loss: 0.4875\n",
            "70/168, train_loss: 0.6168\n",
            "71/168, train_loss: 0.4583\n",
            "72/168, train_loss: 0.4840\n",
            "73/168, train_loss: 0.5237\n",
            "74/168, train_loss: 0.4774\n",
            "75/168, train_loss: 0.4719\n",
            "76/168, train_loss: 0.4883\n",
            "77/168, train_loss: 0.7379\n",
            "78/168, train_loss: 0.4755\n",
            "79/168, train_loss: 0.4766\n",
            "80/168, train_loss: 1.0265\n",
            "81/168, train_loss: 0.4834\n",
            "82/168, train_loss: 0.6825\n",
            "83/168, train_loss: 0.5963\n",
            "84/168, train_loss: 0.5572\n",
            "85/168, train_loss: 0.5664\n",
            "86/168, train_loss: 0.4649\n",
            "87/168, train_loss: 0.7336\n",
            "88/168, train_loss: 0.8198\n",
            "89/168, train_loss: 0.8326\n",
            "90/168, train_loss: 0.8943\n",
            "91/168, train_loss: 0.4416\n",
            "92/168, train_loss: 0.5976\n",
            "93/168, train_loss: 0.7174\n",
            "94/168, train_loss: 0.6660\n",
            "95/168, train_loss: 0.5958\n",
            "96/168, train_loss: 0.4694\n",
            "97/168, train_loss: 0.5700\n",
            "98/168, train_loss: 0.6985\n",
            "99/168, train_loss: 0.6360\n",
            "100/168, train_loss: 0.4763\n",
            "101/168, train_loss: 0.9943\n",
            "102/168, train_loss: 0.8053\n",
            "103/168, train_loss: 0.4527\n",
            "104/168, train_loss: 0.7345\n",
            "105/168, train_loss: 0.4709\n",
            "106/168, train_loss: 0.4847\n",
            "107/168, train_loss: 0.7270\n",
            "108/168, train_loss: 0.7649\n",
            "109/168, train_loss: 0.9698\n",
            "110/168, train_loss: 0.3987\n",
            "111/168, train_loss: 0.7708\n",
            "112/168, train_loss: 0.7315\n",
            "113/168, train_loss: 0.4717\n",
            "114/168, train_loss: 0.5681\n",
            "115/168, train_loss: 0.6055\n",
            "116/168, train_loss: 0.7921\n",
            "117/168, train_loss: 0.4941\n",
            "118/168, train_loss: 0.4820\n",
            "119/168, train_loss: 0.6192\n",
            "120/168, train_loss: 0.8883\n",
            "121/168, train_loss: 0.6833\n",
            "122/168, train_loss: 0.6358\n",
            "123/168, train_loss: 0.5605\n",
            "124/168, train_loss: 0.5378\n",
            "125/168, train_loss: 0.5117\n",
            "126/168, train_loss: 0.5807\n",
            "127/168, train_loss: 0.6251\n",
            "128/168, train_loss: 0.9825\n",
            "129/168, train_loss: 0.4682\n",
            "130/168, train_loss: 0.4927\n",
            "131/168, train_loss: 0.9785\n",
            "132/168, train_loss: 0.5365\n",
            "133/168, train_loss: 0.7294\n",
            "134/168, train_loss: 1.0308\n",
            "135/168, train_loss: 0.5158\n",
            "136/168, train_loss: 0.6091\n",
            "137/168, train_loss: 0.5048\n",
            "138/168, train_loss: 0.4870\n",
            "139/168, train_loss: 0.5254\n",
            "140/168, train_loss: 0.9887\n",
            "141/168, train_loss: 0.4719\n",
            "142/168, train_loss: 0.7248\n",
            "143/168, train_loss: 0.5086\n",
            "144/168, train_loss: 0.7238\n",
            "145/168, train_loss: 0.4987\n",
            "146/168, train_loss: 0.7208\n",
            "147/168, train_loss: 0.5178\n",
            "148/168, train_loss: 0.4999\n",
            "149/168, train_loss: 0.4938\n",
            "150/168, train_loss: 0.7256\n",
            "151/168, train_loss: 0.4850\n",
            "152/168, train_loss: 0.4630\n",
            "153/168, train_loss: 0.9909\n",
            "154/168, train_loss: 1.0141\n",
            "155/168, train_loss: 0.4598\n",
            "156/168, train_loss: 0.7819\n",
            "157/168, train_loss: 0.5122\n",
            "158/168, train_loss: 0.6538\n",
            "159/168, train_loss: 0.4570\n",
            "160/168, train_loss: 0.5891\n",
            "161/168, train_loss: 0.7283\n",
            "162/168, train_loss: 0.9862\n",
            "163/168, train_loss: 0.4689\n",
            "164/168, train_loss: 0.5197\n",
            "165/168, train_loss: 0.6276\n",
            "166/168, train_loss: 1.0093\n",
            "167/168, train_loss: 0.4660\n",
            "168/168, train_loss: 0.4577\n",
            "169/168, train_loss: 0.4814\n",
            "epoch 3 average loss: 0.6348\n",
            "----------\n",
            "epoch 4/5\n",
            "1/168, train_loss: 0.5954\n",
            "2/168, train_loss: 0.5591\n",
            "3/168, train_loss: 0.7315\n",
            "4/168, train_loss: 0.5105\n",
            "5/168, train_loss: 0.5699\n",
            "6/168, train_loss: 0.6055\n",
            "7/168, train_loss: 0.4737\n",
            "8/168, train_loss: 0.4557\n",
            "9/168, train_loss: 0.4561\n",
            "10/168, train_loss: 0.7073\n",
            "11/168, train_loss: 0.4768\n",
            "12/168, train_loss: 0.5484\n",
            "13/168, train_loss: 0.4825\n",
            "14/168, train_loss: 0.4572\n",
            "15/168, train_loss: 1.0594\n",
            "16/168, train_loss: 0.4666\n",
            "17/168, train_loss: 1.0134\n",
            "18/168, train_loss: 0.4582\n",
            "19/168, train_loss: 0.5017\n",
            "20/168, train_loss: 0.5019\n",
            "21/168, train_loss: 0.7256\n",
            "22/168, train_loss: 0.5074\n",
            "23/168, train_loss: 0.4975\n",
            "24/168, train_loss: 0.4567\n",
            "25/168, train_loss: 0.7219\n",
            "26/168, train_loss: 0.4460\n",
            "27/168, train_loss: 1.0211\n",
            "28/168, train_loss: 0.4221\n",
            "29/168, train_loss: 0.4956\n",
            "30/168, train_loss: 0.4358\n",
            "31/168, train_loss: 0.4996\n",
            "32/168, train_loss: 0.4820\n",
            "33/168, train_loss: 1.0247\n",
            "34/168, train_loss: 0.7210\n",
            "35/168, train_loss: 0.4906\n",
            "36/168, train_loss: 0.4385\n",
            "37/168, train_loss: 0.4683\n",
            "38/168, train_loss: 0.4999\n",
            "39/168, train_loss: 1.0496\n",
            "40/168, train_loss: 0.5022\n",
            "41/168, train_loss: 0.4987\n",
            "42/168, train_loss: 0.7320\n",
            "43/168, train_loss: 0.4610\n",
            "44/168, train_loss: 0.5721\n",
            "45/168, train_loss: 0.4331\n",
            "46/168, train_loss: 0.5551\n",
            "47/168, train_loss: 0.4943\n",
            "48/168, train_loss: 0.3332\n",
            "49/168, train_loss: 1.0546\n",
            "50/168, train_loss: 0.4583\n",
            "51/168, train_loss: 0.7353\n",
            "52/168, train_loss: 0.4235\n",
            "53/168, train_loss: 0.5031\n",
            "54/168, train_loss: 0.7360\n",
            "55/168, train_loss: 0.4861\n",
            "56/168, train_loss: 0.4716\n",
            "57/168, train_loss: 0.4645\n",
            "58/168, train_loss: 1.0421\n",
            "59/168, train_loss: 0.4449\n",
            "60/168, train_loss: 1.0900\n",
            "61/168, train_loss: 1.0691\n",
            "62/168, train_loss: 0.4416\n",
            "63/168, train_loss: 0.4269\n",
            "64/168, train_loss: 0.7131\n",
            "65/168, train_loss: 1.0418\n",
            "66/168, train_loss: 0.5344\n",
            "67/168, train_loss: 0.8188\n",
            "68/168, train_loss: 0.4274\n",
            "69/168, train_loss: 0.4455\n",
            "70/168, train_loss: 1.0487\n",
            "71/168, train_loss: 0.7469\n",
            "72/168, train_loss: 0.6122\n",
            "73/168, train_loss: 0.7845\n",
            "74/168, train_loss: 0.4576\n",
            "75/168, train_loss: 1.0537\n",
            "76/168, train_loss: 1.0571\n",
            "77/168, train_loss: 0.7371\n",
            "78/168, train_loss: 0.5189\n",
            "79/168, train_loss: 0.4527\n",
            "80/168, train_loss: 0.4599\n",
            "81/168, train_loss: 1.0414\n",
            "82/168, train_loss: 0.4327\n",
            "83/168, train_loss: 0.5205\n",
            "84/168, train_loss: 0.6813\n",
            "85/168, train_loss: 0.4427\n",
            "86/168, train_loss: 0.4480\n",
            "87/168, train_loss: 0.4783\n",
            "88/168, train_loss: 0.4614\n",
            "89/168, train_loss: 0.4475\n",
            "90/168, train_loss: 0.7713\n",
            "91/168, train_loss: 1.0557\n",
            "92/168, train_loss: 0.5052\n",
            "93/168, train_loss: 0.5310\n",
            "94/168, train_loss: 0.3442\n",
            "95/168, train_loss: 0.4488\n",
            "96/168, train_loss: 0.4482\n",
            "97/168, train_loss: 0.4499\n",
            "98/168, train_loss: 0.5376\n",
            "99/168, train_loss: 0.8751\n",
            "100/168, train_loss: 0.4328\n",
            "101/168, train_loss: 0.4730\n",
            "102/168, train_loss: 1.0775\n",
            "103/168, train_loss: 1.0614\n",
            "104/168, train_loss: 0.5212\n",
            "105/168, train_loss: 0.4130\n",
            "106/168, train_loss: 0.4727\n",
            "107/168, train_loss: 0.7560\n",
            "108/168, train_loss: 0.7917\n",
            "109/168, train_loss: 0.4107\n",
            "110/168, train_loss: 0.3521\n",
            "111/168, train_loss: 1.0450\n",
            "112/168, train_loss: 0.4727\n",
            "113/168, train_loss: 0.6113\n",
            "114/168, train_loss: 0.9802\n",
            "115/168, train_loss: 0.4348\n",
            "116/168, train_loss: 0.7856\n",
            "117/168, train_loss: 0.4414\n",
            "118/168, train_loss: 0.5010\n",
            "119/168, train_loss: 0.6467\n",
            "120/168, train_loss: 1.0369\n",
            "121/168, train_loss: 0.4381\n",
            "122/168, train_loss: 0.4368\n",
            "123/168, train_loss: 0.6220\n",
            "124/168, train_loss: 0.7539\n",
            "125/168, train_loss: 1.0974\n",
            "126/168, train_loss: 1.0298\n",
            "127/168, train_loss: 0.4496\n",
            "128/168, train_loss: 0.4495\n",
            "129/168, train_loss: 0.8403\n",
            "130/168, train_loss: 0.4534\n",
            "131/168, train_loss: 0.6233\n",
            "132/168, train_loss: 0.6325\n",
            "133/168, train_loss: 0.4382\n",
            "134/168, train_loss: 0.4387\n",
            "135/168, train_loss: 0.4541\n",
            "136/168, train_loss: 0.4432\n",
            "137/168, train_loss: 0.6503\n",
            "138/168, train_loss: 0.7349\n",
            "139/168, train_loss: 0.7324\n",
            "140/168, train_loss: 0.5889\n",
            "141/168, train_loss: 0.4645\n",
            "142/168, train_loss: 0.4494\n",
            "143/168, train_loss: 1.0407\n",
            "144/168, train_loss: 0.5268\n",
            "145/168, train_loss: 0.6510\n",
            "146/168, train_loss: 1.0682\n",
            "147/168, train_loss: 0.4873\n",
            "148/168, train_loss: 0.4416\n",
            "149/168, train_loss: 0.7374\n",
            "150/168, train_loss: 0.4516\n",
            "151/168, train_loss: 1.0134\n",
            "152/168, train_loss: 0.5017\n",
            "153/168, train_loss: 0.4580\n",
            "154/168, train_loss: 0.7297\n",
            "155/168, train_loss: 0.5399\n",
            "156/168, train_loss: 0.4482\n",
            "157/168, train_loss: 0.4763\n",
            "158/168, train_loss: 0.6194\n",
            "159/168, train_loss: 0.4233\n",
            "160/168, train_loss: 0.4514\n",
            "161/168, train_loss: 1.1012\n",
            "162/168, train_loss: 1.0278\n",
            "163/168, train_loss: 0.7809\n",
            "164/168, train_loss: 0.6260\n",
            "165/168, train_loss: 0.6887\n",
            "166/168, train_loss: 0.4493\n",
            "167/168, train_loss: 0.4694\n",
            "168/168, train_loss: 1.0333\n",
            "169/168, train_loss: 1.0214\n",
            "epoch 4 average loss: 0.6242\n",
            "current epoch: 4 current accuracy: 0.4824 best accuracy: 0.5294 at epoch 2\n",
            "----------\n",
            "epoch 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.4518\n",
            "2/168, train_loss: 0.4567\n",
            "3/168, train_loss: 0.4644\n",
            "4/168, train_loss: 0.8719\n",
            "5/168, train_loss: 0.4472\n",
            "6/168, train_loss: 0.4724\n",
            "7/168, train_loss: 0.4606\n",
            "8/168, train_loss: 1.0386\n",
            "9/168, train_loss: 0.3805\n",
            "10/168, train_loss: 0.4833\n",
            "11/168, train_loss: 0.5945\n",
            "12/168, train_loss: 1.0809\n",
            "13/168, train_loss: 0.4688\n",
            "14/168, train_loss: 0.9093\n",
            "15/168, train_loss: 0.4465\n",
            "16/168, train_loss: 0.4238\n",
            "17/168, train_loss: 0.4720\n",
            "18/168, train_loss: 0.4549\n",
            "19/168, train_loss: 0.4434\n",
            "20/168, train_loss: 0.9750\n",
            "21/168, train_loss: 0.3800\n",
            "22/168, train_loss: 0.4766\n",
            "23/168, train_loss: 0.6472\n",
            "24/168, train_loss: 0.4403\n",
            "25/168, train_loss: 0.4570\n",
            "26/168, train_loss: 0.4261\n",
            "27/168, train_loss: 0.4541\n",
            "28/168, train_loss: 0.4459\n",
            "29/168, train_loss: 0.7508\n",
            "30/168, train_loss: 0.4488\n",
            "31/168, train_loss: 0.4821\n",
            "32/168, train_loss: 1.3033\n",
            "33/168, train_loss: 0.7384\n",
            "34/168, train_loss: 0.4344\n",
            "35/168, train_loss: 0.3833\n",
            "36/168, train_loss: 0.7335\n",
            "37/168, train_loss: 1.0476\n",
            "38/168, train_loss: 1.0760\n",
            "39/168, train_loss: 0.3953\n",
            "40/168, train_loss: 0.4590\n",
            "41/168, train_loss: 0.4482\n",
            "42/168, train_loss: 0.7360\n",
            "43/168, train_loss: 0.4492\n",
            "44/168, train_loss: 0.4333\n",
            "45/168, train_loss: 0.5988\n",
            "46/168, train_loss: 0.7155\n",
            "47/168, train_loss: 1.0269\n",
            "48/168, train_loss: 0.4527\n",
            "49/168, train_loss: 0.4508\n",
            "50/168, train_loss: 0.4793\n",
            "51/168, train_loss: 0.7108\n",
            "52/168, train_loss: 0.4935\n",
            "53/168, train_loss: 1.0647\n",
            "54/168, train_loss: 0.7366\n",
            "55/168, train_loss: 0.7953\n",
            "56/168, train_loss: 1.0427\n",
            "57/168, train_loss: 0.4373\n",
            "58/168, train_loss: 0.7362\n",
            "59/168, train_loss: 0.5064\n",
            "60/168, train_loss: 0.4672\n",
            "61/168, train_loss: 0.4618\n",
            "62/168, train_loss: 0.4362\n",
            "63/168, train_loss: 0.4792\n",
            "64/168, train_loss: 1.0280\n",
            "65/168, train_loss: 0.6871\n",
            "66/168, train_loss: 1.0476\n",
            "67/168, train_loss: 0.4396\n",
            "68/168, train_loss: 0.4380\n",
            "69/168, train_loss: 0.7346\n",
            "70/168, train_loss: 1.0277\n",
            "71/168, train_loss: 0.5426\n",
            "72/168, train_loss: 0.4696\n",
            "73/168, train_loss: 0.5771\n",
            "74/168, train_loss: 0.4459\n",
            "75/168, train_loss: 0.2913\n",
            "76/168, train_loss: 1.0748\n",
            "77/168, train_loss: 0.4513\n",
            "78/168, train_loss: 0.4498\n",
            "79/168, train_loss: 0.9714\n",
            "80/168, train_loss: 0.7325\n",
            "81/168, train_loss: 0.7309\n",
            "82/168, train_loss: 0.7268\n",
            "83/168, train_loss: 0.4701\n",
            "84/168, train_loss: 0.6367\n",
            "85/168, train_loss: 0.4831\n",
            "86/168, train_loss: 0.4816\n",
            "87/168, train_loss: 0.4683\n",
            "88/168, train_loss: 0.9754\n",
            "89/168, train_loss: 0.4604\n",
            "90/168, train_loss: 0.4722\n",
            "91/168, train_loss: 0.4556\n",
            "92/168, train_loss: 0.7281\n",
            "93/168, train_loss: 0.4451\n",
            "94/168, train_loss: 0.5398\n",
            "95/168, train_loss: 0.4261\n",
            "96/168, train_loss: 1.0046\n",
            "97/168, train_loss: 0.6366\n",
            "98/168, train_loss: 0.4399\n",
            "99/168, train_loss: 0.7181\n",
            "100/168, train_loss: 0.7358\n",
            "101/168, train_loss: 0.5772\n",
            "102/168, train_loss: 0.6348\n",
            "103/168, train_loss: 0.4442\n",
            "104/168, train_loss: 0.4419\n",
            "105/168, train_loss: 1.0455\n",
            "106/168, train_loss: 0.4387\n",
            "107/168, train_loss: 0.7369\n",
            "108/168, train_loss: 0.4461\n",
            "109/168, train_loss: 0.4326\n",
            "110/168, train_loss: 0.4257\n",
            "111/168, train_loss: 0.7922\n",
            "112/168, train_loss: 0.4269\n",
            "113/168, train_loss: 0.5547\n",
            "114/168, train_loss: 0.7398\n",
            "115/168, train_loss: 0.4426\n",
            "116/168, train_loss: 0.4328\n",
            "117/168, train_loss: 0.4290\n",
            "118/168, train_loss: 0.4353\n",
            "119/168, train_loss: 0.5880\n",
            "120/168, train_loss: 1.0383\n",
            "121/168, train_loss: 0.4373\n",
            "122/168, train_loss: 0.6942\n",
            "123/168, train_loss: 0.4333\n",
            "124/168, train_loss: 1.0527\n",
            "125/168, train_loss: 0.4358\n",
            "126/168, train_loss: 0.4288\n",
            "127/168, train_loss: 0.4422\n",
            "128/168, train_loss: 0.4496\n",
            "129/168, train_loss: 0.7370\n",
            "130/168, train_loss: 0.4226\n",
            "131/168, train_loss: 1.0158\n",
            "132/168, train_loss: 0.6524\n",
            "133/168, train_loss: 0.4199\n",
            "134/168, train_loss: 1.0585\n",
            "135/168, train_loss: 0.6998\n",
            "136/168, train_loss: 0.4140\n",
            "137/168, train_loss: 0.4406\n",
            "138/168, train_loss: 1.0375\n",
            "139/168, train_loss: 0.8671\n",
            "140/168, train_loss: 0.5878\n",
            "141/168, train_loss: 0.4413\n",
            "142/168, train_loss: 0.7419\n",
            "143/168, train_loss: 0.4142\n",
            "144/168, train_loss: 0.4269\n",
            "145/168, train_loss: 0.8900\n",
            "146/168, train_loss: 0.6833\n",
            "147/168, train_loss: 0.7407\n",
            "148/168, train_loss: 0.7267\n",
            "149/168, train_loss: 0.6729\n",
            "150/168, train_loss: 1.0626\n",
            "151/168, train_loss: 0.7420\n",
            "152/168, train_loss: 0.7393\n",
            "153/168, train_loss: 1.0288\n",
            "154/168, train_loss: 0.4191\n",
            "155/168, train_loss: 1.0165\n",
            "156/168, train_loss: 1.0512\n",
            "157/168, train_loss: 0.7819\n",
            "158/168, train_loss: 0.7378\n",
            "159/168, train_loss: 0.7334\n",
            "160/168, train_loss: 0.5898\n",
            "161/168, train_loss: 1.0024\n",
            "162/168, train_loss: 0.7166\n",
            "163/168, train_loss: 0.6368\n",
            "164/168, train_loss: 0.4763\n",
            "165/168, train_loss: 1.0458\n",
            "166/168, train_loss: 0.6428\n",
            "167/168, train_loss: 0.4902\n",
            "168/168, train_loss: 0.4536\n",
            "169/168, train_loss: 0.4625\n",
            "epoch 5 average loss: 0.6283\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "x and y must have same first dimension, but have shapes (5,) and (2,)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-63917e8913f6>\u001b[0m in \u001b[0;36m<cell line: 176>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-63917e8913f6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m     }\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmetric_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_values\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mplot_and_save_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-63917e8913f6>\u001b[0m in \u001b[0;36mplot_and_save_metrics\u001b[0;34m(epochs, train_metrics, val_metrics, metric_name, save_path)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Train {metric_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Validation {metric_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2812\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2813\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \"\"\"\n\u001b[1;32m   1687\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    312\u001b[0m                 this, kwargs, ambiguous_fmt_datakey=ambiguous_fmt_datakey)\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    505\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (5,) and (2,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ResNext50"
      ],
      "metadata": {
        "id": "svl4GgtQgqf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import monai\n",
        "from monai.data import ImageDataset, DataLoader\n",
        "from monai.transforms import EnsureChannelFirst, Compose, RandRotate90, Resize, ScaleIntensity, RepeatChannel\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, f1_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.models.video import r3d_18\n",
        "\n",
        "def load_image_paths_and_labels(data_dir, labels_csv):\n",
        "    \"\"\"\n",
        "    Load image paths and corresponding labels from the directory and CSV file.\n",
        "\n",
        "    Args:\n",
        "    - data_dir (str): Directory containing the image files.\n",
        "    - labels_csv (str): Path to the CSV file containing labels.\n",
        "\n",
        "    Returns:\n",
        "    - image_paths (List[str]): List of image file paths.\n",
        "    - labels (List[int]): List of labels corresponding to the image file paths.\n",
        "    \"\"\"\n",
        "    # Read labels CSV\n",
        "    labels_df = pd.read_csv(labels_csv)\n",
        "\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    for _, row in labels_df.iterrows():\n",
        "        patient_id = row['PatientID']\n",
        "        label = row['Cancer']\n",
        "        # Find all image files for the patient\n",
        "        patient_images = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.startswith(patient_id)]\n",
        "        image_paths.extend(patient_images)\n",
        "        labels.extend([label] * len(patient_images))\n",
        "\n",
        "    return image_paths, labels\n",
        "\n",
        "def plot_and_save_metrics(epochs, train_metrics, val_metrics, metric_name, save_path):\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, train_metrics, label=f'Train {metric_name}')\n",
        "    plt.plot(epochs, val_metrics, label=f'Validation {metric_name}')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.legend()\n",
        "    plt.title(f'Train and Validation {metric_name}')\n",
        "    plt.savefig(os.path.join(save_path, f'{metric_name}.png'))\n",
        "\n",
        "class ResNeXt3D(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(ResNeXt3D, self).__init__()\n",
        "        self.model = r3d_18(pretrained=True)\n",
        "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "def main():\n",
        "    monai.config.print_config()\n",
        "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "\n",
        "    # Define data paths\n",
        "    data_dir = '/content/drive/MyDrive/Praktikum/3d_dataset_2/3d_patches_nii/'  # Directory containing the image files\n",
        "    labels_csv = '/content/drive/MyDrive/Praktikum/labels.csv'  # Path to the CSV file containing labels\n",
        "\n",
        "    # Number of epochs\n",
        "    num_epochs = 10  # Change this value to set the number of epochs\n",
        "\n",
        "    # Load image paths and labels\n",
        "    images, labels = load_image_paths_and_labels(data_dir, labels_csv)\n",
        "    labels = np.array(labels, dtype=np.int64)\n",
        "\n",
        "    # Debugging: Print number of images and labels\n",
        "    print(f\"Number of images: {len(images)}, Number of labels: {len(labels)}\")\n",
        "    print(f\"Labels distribution: {np.bincount(labels)}\")\n",
        "\n",
        "    # Define transforms\n",
        "    train_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(), RepeatChannel(repeats=3), Resize((96, 96, 96)), RandRotate90()])\n",
        "    val_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(), RepeatChannel(repeats=3), Resize((96, 96, 96))])\n",
        "\n",
        "    # Split dataset into training and validation sets\n",
        "    train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Debugging: Print train and validation label distributions\n",
        "    print(f\"Train labels distribution: {np.bincount(train_labels)}\")\n",
        "    print(f\"Validation labels distribution: {np.bincount(val_labels)}\")\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_ds = ImageDataset(image_files=train_images, labels=train_labels, transform=train_transforms)\n",
        "    val_ds = ImageDataset(image_files=val_images, labels=val_labels, transform=val_transforms)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "    val_loader = DataLoader(val_ds, batch_size=2, num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "    # Create ResNeXt50 3D, CrossEntropyLoss and Adam optimizer\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = ResNeXt3D(num_classes=2).to(device)\n",
        "    loss_function = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), 1e-5)\n",
        "\n",
        "    val_interval = 2\n",
        "    best_metric = -1\n",
        "    best_metric_epoch = -1\n",
        "    epoch_loss_values = list()\n",
        "    epoch_accuracy_values = list()\n",
        "    epoch_recall_values = list()\n",
        "    epoch_auc_values = list()\n",
        "    epoch_f1_values = list()\n",
        "    epoch_mcc_values = list()\n",
        "    writer = SummaryWriter()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"-\" * 10)\n",
        "        print(f\"epoch {epoch + 1}/{num_epochs}\")\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        step = 0\n",
        "        for batch_data in train_loader:\n",
        "            step += 1\n",
        "            inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_len = len(train_ds) // train_loader.batch_size\n",
        "            print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
        "            writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
        "        epoch_loss /= step\n",
        "        epoch_loss_values.append(epoch_loss)\n",
        "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        if (epoch + 1) % val_interval == 0:\n",
        "            model.eval()\n",
        "            val_preds = []\n",
        "            val_true = []\n",
        "            with torch.no_grad():\n",
        "                num_correct = 0.0\n",
        "                metric_count = 0\n",
        "                for val_data in val_loader:\n",
        "                    val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
        "                    val_outputs = model(val_images)\n",
        "                    val_preds.extend(val_outputs.argmax(dim=1).cpu().numpy())\n",
        "                    val_true.extend(val_labels.cpu().numpy())\n",
        "                    value = torch.eq(val_outputs.argmax(dim=1), val_labels)\n",
        "                    metric_count += len(value)\n",
        "                    num_correct += value.sum().item()\n",
        "            accuracy = accuracy_score(val_true, val_preds)\n",
        "            recall = recall_score(val_true, val_preds)\n",
        "            auc = roc_auc_score(val_true, val_preds)\n",
        "            f1 = f1_score(val_true, val_preds)\n",
        "            mcc = matthews_corrcoef(val_true, val_preds)\n",
        "\n",
        "            epoch_accuracy_values.append(accuracy)\n",
        "            epoch_recall_values.append(recall)\n",
        "            epoch_auc_values.append(auc)\n",
        "            epoch_f1_values.append(f1)\n",
        "            epoch_mcc_values.append(mcc)\n",
        "\n",
        "            if accuracy > best_metric:\n",
        "                best_metric = accuracy\n",
        "                best_metric_epoch = epoch + 1\n",
        "                torch.save(model.state_dict(), \"best_metric_model_classification3d_5E_RNext50.pth\")\n",
        "                print(\"saved new best metric model\")\n",
        "            print(\n",
        "                f\"current epoch: {epoch + 1} current accuracy: {accuracy:.4f} best accuracy: {best_metric:.4f} at epoch {best_metric_epoch}\"\n",
        "            )\n",
        "            writer.add_scalar(\"val_accuracy\", accuracy, epoch + 1)\n",
        "\n",
        "    # Save metrics as PNG\n",
        "    epochs = list(range(1, num_epochs + 1))\n",
        "    metrics = {\n",
        "        'accuracy': epoch_accuracy_values,\n",
        "        'recall': epoch_recall_values,\n",
        "        'auc': epoch_auc_values,\n",
        "        'f1': epoch_f1_values,\n",
        "        'mcc': epoch_mcc_values\n",
        "    }\n",
        "    for metric_name, metric_values in metrics.items():\n",
        "        plot_and_save_metrics(epochs[:len(metric_values)], epoch_loss_values[:len(metric_values)], metric_values, metric_name, '/content/')\n",
        "\n",
        "    print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
        "    writer.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qdXOq49_goQl",
        "outputId": "d2ffd2e0-ee82-4ee3-94f5-b2ddd817e0bc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MONAI version: 1.3.1\n",
            "Numpy version: 1.25.2\n",
            "Pytorch version: 2.3.0+cpu\n",
            "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
            "MONAI rev id: 96bfda00c6bd290297f5e3514ea227c6be4d08b4\n",
            "MONAI __file__: /usr/local/lib/python3.10/dist-packages/monai/__init__.py\n",
            "\n",
            "Optional dependencies:\n",
            "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "Nibabel version: 5.2.1\n",
            "scikit-image version: 0.19.3\n",
            "scipy version: 1.11.4\n",
            "Pillow version: 10.3.0\n",
            "Tensorboard version: 2.15.2\n",
            "gdown version: 4.7.3\n",
            "TorchVision version: 0.18.0+cpu\n",
            "tqdm version: 4.66.4\n",
            "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "psutil version: 5.9.5\n",
            "pandas version: 2.0.3\n",
            "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "transformers version: 4.41.2\n",
            "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "\n",
            "For details about installing the optional dependencies, please visit:\n",
            "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
            "\n",
            "Number of images: 422, Number of labels: 422\n",
            "Labels distribution: [264 158]\n",
            "Train labels distribution: [215 122]\n",
            "Validation labels distribution: [49 36]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------\n",
            "epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.7054\n",
            "2/168, train_loss: 0.7182\n",
            "3/168, train_loss: 0.8881\n",
            "4/168, train_loss: 0.6956\n",
            "5/168, train_loss: 0.6929\n",
            "6/168, train_loss: 0.7401\n",
            "7/168, train_loss: 0.5131\n",
            "8/168, train_loss: 0.7248\n",
            "9/168, train_loss: 0.8122\n",
            "10/168, train_loss: 0.7185\n",
            "11/168, train_loss: 0.6764\n",
            "12/168, train_loss: 0.8342\n",
            "13/168, train_loss: 0.3584\n",
            "14/168, train_loss: 0.7054\n",
            "15/168, train_loss: 1.6734\n",
            "16/168, train_loss: 0.6311\n",
            "17/168, train_loss: 0.2255\n",
            "18/168, train_loss: 0.3089\n",
            "19/168, train_loss: 0.6935\n",
            "20/168, train_loss: 0.9154\n",
            "21/168, train_loss: 1.2989\n",
            "22/168, train_loss: 0.6521\n",
            "23/168, train_loss: 0.7883\n",
            "24/168, train_loss: 0.3293\n",
            "25/168, train_loss: 0.7019\n",
            "26/168, train_loss: 0.7290\n",
            "27/168, train_loss: 0.7897\n",
            "28/168, train_loss: 0.7243\n",
            "29/168, train_loss: 0.6939\n",
            "30/168, train_loss: 0.6707\n",
            "31/168, train_loss: 0.5109\n",
            "32/168, train_loss: 0.8829\n",
            "33/168, train_loss: 0.6907\n",
            "34/168, train_loss: 0.5907\n",
            "35/168, train_loss: 0.7202\n",
            "36/168, train_loss: 0.6678\n",
            "37/168, train_loss: 0.6915\n",
            "38/168, train_loss: 0.7040\n",
            "39/168, train_loss: 0.5968\n",
            "40/168, train_loss: 0.6667\n",
            "41/168, train_loss: 0.7020\n",
            "42/168, train_loss: 0.6736\n",
            "43/168, train_loss: 0.7971\n",
            "44/168, train_loss: 0.6339\n",
            "45/168, train_loss: 0.7849\n",
            "46/168, train_loss: 0.5389\n",
            "47/168, train_loss: 0.6736\n",
            "48/168, train_loss: 0.5820\n",
            "49/168, train_loss: 0.7623\n",
            "50/168, train_loss: 0.6354\n",
            "51/168, train_loss: 0.6264\n",
            "52/168, train_loss: 0.6614\n",
            "53/168, train_loss: 0.4381\n",
            "54/168, train_loss: 0.5389\n",
            "55/168, train_loss: 0.7500\n",
            "56/168, train_loss: 0.7551\n",
            "57/168, train_loss: 0.6939\n",
            "58/168, train_loss: 0.6301\n",
            "59/168, train_loss: 0.6428\n",
            "60/168, train_loss: 0.7538\n",
            "61/168, train_loss: 0.6666\n",
            "62/168, train_loss: 0.9670\n",
            "63/168, train_loss: 0.6932\n",
            "64/168, train_loss: 0.4332\n",
            "65/168, train_loss: 0.8340\n",
            "66/168, train_loss: 0.6129\n",
            "67/168, train_loss: 0.5256\n",
            "68/168, train_loss: 0.5987\n",
            "69/168, train_loss: 0.5634\n",
            "70/168, train_loss: 0.6342\n",
            "71/168, train_loss: 0.8414\n",
            "72/168, train_loss: 0.7837\n",
            "73/168, train_loss: 0.6311\n",
            "74/168, train_loss: 0.7618\n",
            "75/168, train_loss: 0.6069\n",
            "76/168, train_loss: 0.6635\n",
            "77/168, train_loss: 0.5681\n",
            "78/168, train_loss: 0.5876\n",
            "79/168, train_loss: 0.6024\n",
            "80/168, train_loss: 0.6514\n",
            "81/168, train_loss: 0.6100\n",
            "82/168, train_loss: 0.8334\n",
            "83/168, train_loss: 0.6937\n",
            "84/168, train_loss: 0.6076\n",
            "85/168, train_loss: 0.8475\n",
            "86/168, train_loss: 0.6148\n",
            "87/168, train_loss: 0.5953\n",
            "88/168, train_loss: 0.5491\n",
            "89/168, train_loss: 0.8037\n",
            "90/168, train_loss: 0.6125\n",
            "91/168, train_loss: 0.6536\n",
            "92/168, train_loss: 0.7297\n",
            "93/168, train_loss: 0.5776\n",
            "94/168, train_loss: 0.4409\n",
            "95/168, train_loss: 0.6962\n",
            "96/168, train_loss: 0.6538\n",
            "97/168, train_loss: 0.7891\n",
            "98/168, train_loss: 0.6023\n",
            "99/168, train_loss: 0.9511\n",
            "100/168, train_loss: 0.5941\n",
            "101/168, train_loss: 0.6983\n",
            "102/168, train_loss: 0.8176\n",
            "103/168, train_loss: 0.6476\n",
            "104/168, train_loss: 0.5966\n",
            "105/168, train_loss: 0.8842\n",
            "106/168, train_loss: 0.8238\n",
            "107/168, train_loss: 0.5881\n",
            "108/168, train_loss: 0.4312\n",
            "109/168, train_loss: 0.7014\n",
            "110/168, train_loss: 0.6295\n",
            "111/168, train_loss: 0.5173\n",
            "112/168, train_loss: 0.6333\n",
            "113/168, train_loss: 0.5596\n",
            "114/168, train_loss: 0.8552\n",
            "115/168, train_loss: 0.5978\n",
            "116/168, train_loss: 0.5714\n",
            "117/168, train_loss: 0.5309\n",
            "118/168, train_loss: 0.8034\n",
            "119/168, train_loss: 0.3469\n",
            "120/168, train_loss: 0.5863\n",
            "121/168, train_loss: 0.5120\n",
            "122/168, train_loss: 0.8398\n",
            "123/168, train_loss: 0.5876\n",
            "124/168, train_loss: 0.4935\n",
            "125/168, train_loss: 1.0079\n",
            "126/168, train_loss: 0.6822\n",
            "127/168, train_loss: 0.5946\n",
            "128/168, train_loss: 0.5589\n",
            "129/168, train_loss: 0.6027\n",
            "130/168, train_loss: 1.0269\n",
            "131/168, train_loss: 0.6223\n",
            "132/168, train_loss: 0.9995\n",
            "133/168, train_loss: 0.8947\n",
            "134/168, train_loss: 0.8372\n",
            "135/168, train_loss: 0.4880\n",
            "136/168, train_loss: 0.5802\n",
            "137/168, train_loss: 0.5676\n",
            "138/168, train_loss: 0.7216\n",
            "139/168, train_loss: 0.5392\n",
            "140/168, train_loss: 0.8891\n",
            "141/168, train_loss: 0.4833\n",
            "142/168, train_loss: 0.5879\n",
            "143/168, train_loss: 0.4849\n",
            "144/168, train_loss: 0.5672\n",
            "145/168, train_loss: 0.7245\n",
            "146/168, train_loss: 1.0166\n",
            "147/168, train_loss: 0.9898\n",
            "148/168, train_loss: 0.4739\n",
            "149/168, train_loss: 0.5896\n",
            "150/168, train_loss: 1.1366\n",
            "151/168, train_loss: 0.4624\n",
            "152/168, train_loss: 0.6484\n",
            "153/168, train_loss: 0.9188\n",
            "154/168, train_loss: 0.7577\n",
            "155/168, train_loss: 0.5437\n",
            "156/168, train_loss: 0.7407\n",
            "157/168, train_loss: 0.7025\n",
            "158/168, train_loss: 0.5398\n",
            "159/168, train_loss: 0.5141\n",
            "160/168, train_loss: 0.4177\n",
            "161/168, train_loss: 0.9707\n",
            "162/168, train_loss: 1.0379\n",
            "163/168, train_loss: 0.4437\n",
            "164/168, train_loss: 0.6261\n",
            "165/168, train_loss: 0.7876\n",
            "166/168, train_loss: 0.5390\n",
            "167/168, train_loss: 0.5628\n",
            "168/168, train_loss: 0.7330\n",
            "169/168, train_loss: 0.5507\n",
            "epoch 1 average loss: 0.6787\n",
            "----------\n",
            "epoch 2/10\n",
            "1/168, train_loss: 0.4325\n",
            "2/168, train_loss: 0.4529\n",
            "3/168, train_loss: 0.7635\n",
            "4/168, train_loss: 0.5406\n",
            "5/168, train_loss: 0.1913\n",
            "6/168, train_loss: 0.5064\n",
            "7/168, train_loss: 0.8893\n",
            "8/168, train_loss: 1.0828\n",
            "9/168, train_loss: 0.7828\n",
            "10/168, train_loss: 0.4336\n",
            "11/168, train_loss: 0.4288\n",
            "12/168, train_loss: 0.6556\n",
            "13/168, train_loss: 0.5474\n",
            "14/168, train_loss: 0.5340\n",
            "15/168, train_loss: 0.6183\n",
            "16/168, train_loss: 0.3836\n",
            "17/168, train_loss: 0.9268\n",
            "18/168, train_loss: 0.4925\n",
            "19/168, train_loss: 0.5525\n",
            "20/168, train_loss: 0.9580\n",
            "21/168, train_loss: 0.9117\n",
            "22/168, train_loss: 0.9052\n",
            "23/168, train_loss: 0.5268\n",
            "24/168, train_loss: 1.1193\n",
            "25/168, train_loss: 0.5872\n",
            "26/168, train_loss: 0.5109\n",
            "27/168, train_loss: 0.8837\n",
            "28/168, train_loss: 0.5250\n",
            "29/168, train_loss: 0.5353\n",
            "30/168, train_loss: 0.7772\n",
            "31/168, train_loss: 0.8253\n",
            "32/168, train_loss: 0.9369\n",
            "33/168, train_loss: 0.9578\n",
            "34/168, train_loss: 0.5462\n",
            "35/168, train_loss: 0.8726\n",
            "36/168, train_loss: 0.6326\n",
            "37/168, train_loss: 0.8844\n",
            "38/168, train_loss: 0.7619\n",
            "39/168, train_loss: 0.5372\n",
            "40/168, train_loss: 0.5310\n",
            "41/168, train_loss: 0.7141\n",
            "42/168, train_loss: 0.5470\n",
            "43/168, train_loss: 0.5313\n",
            "44/168, train_loss: 0.5029\n",
            "45/168, train_loss: 0.5243\n",
            "46/168, train_loss: 0.4766\n",
            "47/168, train_loss: 0.6527\n",
            "48/168, train_loss: 0.4814\n",
            "49/168, train_loss: 0.9755\n",
            "50/168, train_loss: 0.6012\n",
            "51/168, train_loss: 0.5268\n",
            "52/168, train_loss: 0.3479\n",
            "53/168, train_loss: 0.6882\n",
            "54/168, train_loss: 0.6320\n",
            "55/168, train_loss: 0.6696\n",
            "56/168, train_loss: 0.4917\n",
            "57/168, train_loss: 0.6459\n",
            "58/168, train_loss: 0.6217\n",
            "59/168, train_loss: 0.7229\n",
            "60/168, train_loss: 0.6095\n",
            "61/168, train_loss: 0.6212\n",
            "62/168, train_loss: 0.4718\n",
            "63/168, train_loss: 0.9898\n",
            "64/168, train_loss: 1.0122\n",
            "65/168, train_loss: 0.4308\n",
            "66/168, train_loss: 0.4948\n",
            "67/168, train_loss: 0.5032\n",
            "68/168, train_loss: 0.9679\n",
            "69/168, train_loss: 0.7233\n",
            "70/168, train_loss: 0.4200\n",
            "71/168, train_loss: 0.5035\n",
            "72/168, train_loss: 0.5543\n",
            "73/168, train_loss: 0.4809\n",
            "74/168, train_loss: 0.4030\n",
            "75/168, train_loss: 0.7204\n",
            "76/168, train_loss: 0.4879\n",
            "77/168, train_loss: 0.7726\n",
            "78/168, train_loss: 0.5117\n",
            "79/168, train_loss: 0.4785\n",
            "80/168, train_loss: 0.7251\n",
            "81/168, train_loss: 0.7268\n",
            "82/168, train_loss: 0.5356\n",
            "83/168, train_loss: 0.9833\n",
            "84/168, train_loss: 0.4684\n",
            "85/168, train_loss: 0.6090\n",
            "86/168, train_loss: 0.7302\n",
            "87/168, train_loss: 0.4878\n",
            "88/168, train_loss: 0.5691\n",
            "89/168, train_loss: 0.4989\n",
            "90/168, train_loss: 0.9538\n",
            "91/168, train_loss: 1.0283\n",
            "92/168, train_loss: 1.0275\n",
            "93/168, train_loss: 0.5183\n",
            "94/168, train_loss: 0.6133\n",
            "95/168, train_loss: 0.8476\n",
            "96/168, train_loss: 0.5345\n",
            "97/168, train_loss: 1.0611\n",
            "98/168, train_loss: 0.5016\n",
            "99/168, train_loss: 0.4561\n",
            "100/168, train_loss: 0.5109\n",
            "101/168, train_loss: 0.9744\n",
            "102/168, train_loss: 0.6618\n",
            "103/168, train_loss: 0.4834\n",
            "104/168, train_loss: 0.7068\n",
            "105/168, train_loss: 0.4772\n",
            "106/168, train_loss: 0.5042\n",
            "107/168, train_loss: 0.5500\n",
            "108/168, train_loss: 0.5003\n",
            "109/168, train_loss: 0.5077\n",
            "110/168, train_loss: 0.4872\n",
            "111/168, train_loss: 0.7271\n",
            "112/168, train_loss: 1.0250\n",
            "113/168, train_loss: 0.3675\n",
            "114/168, train_loss: 0.4881\n",
            "115/168, train_loss: 0.4957\n",
            "116/168, train_loss: 0.5415\n",
            "117/168, train_loss: 0.4994\n",
            "118/168, train_loss: 0.5119\n",
            "119/168, train_loss: 0.4974\n",
            "120/168, train_loss: 0.8452\n",
            "121/168, train_loss: 0.9623\n",
            "122/168, train_loss: 0.6847\n",
            "123/168, train_loss: 0.7431\n",
            "124/168, train_loss: 0.4694\n",
            "125/168, train_loss: 0.8666\n",
            "126/168, train_loss: 0.9777\n",
            "127/168, train_loss: 0.4872\n",
            "128/168, train_loss: 0.5372\n",
            "129/168, train_loss: 0.6429\n",
            "130/168, train_loss: 0.2554\n",
            "131/168, train_loss: 0.4983\n",
            "132/168, train_loss: 0.4896\n",
            "133/168, train_loss: 0.4285\n",
            "134/168, train_loss: 0.7189\n",
            "135/168, train_loss: 0.7191\n",
            "136/168, train_loss: 1.1391\n",
            "137/168, train_loss: 0.7183\n",
            "138/168, train_loss: 0.6543\n",
            "139/168, train_loss: 0.5359\n",
            "140/168, train_loss: 1.1150\n",
            "141/168, train_loss: 0.6444\n",
            "142/168, train_loss: 1.1185\n",
            "143/168, train_loss: 1.1886\n",
            "144/168, train_loss: 0.5469\n",
            "145/168, train_loss: 1.0550\n",
            "146/168, train_loss: 0.5096\n",
            "147/168, train_loss: 0.8484\n",
            "148/168, train_loss: 0.8792\n",
            "149/168, train_loss: 0.8731\n",
            "150/168, train_loss: 0.3966\n",
            "151/168, train_loss: 0.4847\n",
            "152/168, train_loss: 1.1281\n",
            "153/168, train_loss: 0.4820\n",
            "154/168, train_loss: 1.1784\n",
            "155/168, train_loss: 0.3804\n",
            "156/168, train_loss: 0.4978\n",
            "157/168, train_loss: 0.4888\n",
            "158/168, train_loss: 0.5415\n",
            "159/168, train_loss: 1.3074\n",
            "160/168, train_loss: 0.4995\n",
            "161/168, train_loss: 1.1805\n",
            "162/168, train_loss: 0.9558\n",
            "163/168, train_loss: 0.2916\n",
            "164/168, train_loss: 0.9793\n",
            "165/168, train_loss: 0.9794\n",
            "166/168, train_loss: 0.4867\n",
            "167/168, train_loss: 0.7134\n",
            "168/168, train_loss: 0.8755\n",
            "169/168, train_loss: 0.5184\n",
            "epoch 2 average loss: 0.6649\n",
            "saved new best metric model\n",
            "current epoch: 2 current accuracy: 0.6235 best accuracy: 0.6235 at epoch 2\n",
            "----------\n",
            "epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.4840\n",
            "2/168, train_loss: 0.4825\n",
            "3/168, train_loss: 0.4628\n",
            "4/168, train_loss: 0.4017\n",
            "5/168, train_loss: 0.4266\n",
            "6/168, train_loss: 0.3547\n",
            "7/168, train_loss: 0.4992\n",
            "8/168, train_loss: 1.5144\n",
            "9/168, train_loss: 0.7932\n",
            "10/168, train_loss: 0.8164\n",
            "11/168, train_loss: 0.8891\n",
            "12/168, train_loss: 0.6942\n",
            "13/168, train_loss: 0.9788\n",
            "14/168, train_loss: 0.5128\n",
            "15/168, train_loss: 1.3139\n",
            "16/168, train_loss: 1.1469\n",
            "17/168, train_loss: 0.7190\n",
            "18/168, train_loss: 0.4743\n",
            "19/168, train_loss: 0.7497\n",
            "20/168, train_loss: 1.0286\n",
            "21/168, train_loss: 0.5519\n",
            "22/168, train_loss: 0.4614\n",
            "23/168, train_loss: 0.4938\n",
            "24/168, train_loss: 0.9721\n",
            "25/168, train_loss: 0.3673\n",
            "26/168, train_loss: 0.5620\n",
            "27/168, train_loss: 0.5385\n",
            "28/168, train_loss: 0.6391\n",
            "29/168, train_loss: 0.9850\n",
            "30/168, train_loss: 0.9410\n",
            "31/168, train_loss: 0.9378\n",
            "32/168, train_loss: 0.4953\n",
            "33/168, train_loss: 0.4986\n",
            "34/168, train_loss: 0.7092\n",
            "35/168, train_loss: 0.4246\n",
            "36/168, train_loss: 0.4862\n",
            "37/168, train_loss: 0.9292\n",
            "38/168, train_loss: 0.5059\n",
            "39/168, train_loss: 0.4999\n",
            "40/168, train_loss: 0.5219\n",
            "41/168, train_loss: 0.5540\n",
            "42/168, train_loss: 0.9492\n",
            "43/168, train_loss: 0.8698\n",
            "44/168, train_loss: 0.5026\n",
            "45/168, train_loss: 0.8277\n",
            "46/168, train_loss: 0.4905\n",
            "47/168, train_loss: 0.4975\n",
            "48/168, train_loss: 0.7261\n",
            "49/168, train_loss: 0.5279\n",
            "50/168, train_loss: 0.5469\n",
            "51/168, train_loss: 0.9545\n",
            "52/168, train_loss: 0.9309\n",
            "53/168, train_loss: 0.5771\n",
            "54/168, train_loss: 0.4904\n",
            "55/168, train_loss: 0.5057\n",
            "56/168, train_loss: 0.4988\n",
            "57/168, train_loss: 0.4469\n",
            "58/168, train_loss: 0.5259\n",
            "59/168, train_loss: 0.4845\n",
            "60/168, train_loss: 0.9217\n",
            "61/168, train_loss: 0.6900\n",
            "62/168, train_loss: 0.5624\n",
            "63/168, train_loss: 0.3833\n",
            "64/168, train_loss: 0.7128\n",
            "65/168, train_loss: 0.5091\n",
            "66/168, train_loss: 0.3817\n",
            "67/168, train_loss: 0.4737\n",
            "68/168, train_loss: 0.9137\n",
            "69/168, train_loss: 0.9614\n",
            "70/168, train_loss: 0.8979\n",
            "71/168, train_loss: 0.4914\n",
            "72/168, train_loss: 0.4648\n",
            "73/168, train_loss: 0.9800\n",
            "74/168, train_loss: 0.8794\n",
            "75/168, train_loss: 0.5010\n",
            "76/168, train_loss: 0.4722\n",
            "77/168, train_loss: 0.4982\n",
            "78/168, train_loss: 0.4679\n",
            "79/168, train_loss: 0.5592\n",
            "80/168, train_loss: 0.9542\n",
            "81/168, train_loss: 0.5919\n",
            "82/168, train_loss: 0.4688\n",
            "83/168, train_loss: 0.6160\n",
            "84/168, train_loss: 0.4690\n",
            "85/168, train_loss: 1.0023\n",
            "86/168, train_loss: 0.4605\n",
            "87/168, train_loss: 0.5038\n",
            "88/168, train_loss: 0.5005\n",
            "89/168, train_loss: 0.4986\n",
            "90/168, train_loss: 0.9590\n",
            "91/168, train_loss: 0.5669\n",
            "92/168, train_loss: 0.5264\n",
            "93/168, train_loss: 0.8073\n",
            "94/168, train_loss: 0.4952\n",
            "95/168, train_loss: 0.6301\n",
            "96/168, train_loss: 0.6840\n",
            "97/168, train_loss: 0.4787\n",
            "98/168, train_loss: 0.9498\n",
            "99/168, train_loss: 0.7603\n",
            "100/168, train_loss: 0.7139\n",
            "101/168, train_loss: 0.3755\n",
            "102/168, train_loss: 0.4656\n",
            "103/168, train_loss: 1.0722\n",
            "104/168, train_loss: 0.5114\n",
            "105/168, train_loss: 0.5064\n",
            "106/168, train_loss: 0.4387\n",
            "107/168, train_loss: 0.9901\n",
            "108/168, train_loss: 1.0492\n",
            "109/168, train_loss: 0.4919\n",
            "110/168, train_loss: 0.4634\n",
            "111/168, train_loss: 0.4416\n",
            "112/168, train_loss: 0.3912\n",
            "113/168, train_loss: 0.5258\n",
            "114/168, train_loss: 0.4753\n",
            "115/168, train_loss: 0.4586\n",
            "116/168, train_loss: 0.9640\n",
            "117/168, train_loss: 0.7338\n",
            "118/168, train_loss: 0.7064\n",
            "119/168, train_loss: 1.0304\n",
            "120/168, train_loss: 0.7070\n",
            "121/168, train_loss: 0.4303\n",
            "122/168, train_loss: 0.5674\n",
            "123/168, train_loss: 0.5020\n",
            "124/168, train_loss: 0.6611\n",
            "125/168, train_loss: 0.7281\n",
            "126/168, train_loss: 0.4657\n",
            "127/168, train_loss: 0.9602\n",
            "128/168, train_loss: 0.6738\n",
            "129/168, train_loss: 0.5531\n",
            "130/168, train_loss: 0.4627\n",
            "131/168, train_loss: 0.9660\n",
            "132/168, train_loss: 0.5122\n",
            "133/168, train_loss: 0.6366\n",
            "134/168, train_loss: 0.7324\n",
            "135/168, train_loss: 0.4643\n",
            "136/168, train_loss: 0.3440\n",
            "137/168, train_loss: 0.5489\n",
            "138/168, train_loss: 0.9880\n",
            "139/168, train_loss: 0.9488\n",
            "140/168, train_loss: 0.4893\n",
            "141/168, train_loss: 0.5129\n",
            "142/168, train_loss: 1.0210\n",
            "143/168, train_loss: 0.7314\n",
            "144/168, train_loss: 0.5823\n",
            "145/168, train_loss: 0.4987\n",
            "146/168, train_loss: 1.0396\n",
            "147/168, train_loss: 0.3961\n",
            "148/168, train_loss: 0.4575\n",
            "149/168, train_loss: 0.4777\n",
            "150/168, train_loss: 0.4497\n",
            "151/168, train_loss: 0.3965\n",
            "152/168, train_loss: 0.5150\n",
            "153/168, train_loss: 1.1469\n",
            "154/168, train_loss: 0.5493\n",
            "155/168, train_loss: 0.4554\n",
            "156/168, train_loss: 0.4812\n",
            "157/168, train_loss: 0.5080\n",
            "158/168, train_loss: 0.4518\n",
            "159/168, train_loss: 0.4132\n",
            "160/168, train_loss: 0.5303\n",
            "161/168, train_loss: 0.4825\n",
            "162/168, train_loss: 0.7233\n",
            "163/168, train_loss: 0.4768\n",
            "164/168, train_loss: 0.4520\n",
            "165/168, train_loss: 0.7996\n",
            "166/168, train_loss: 0.4727\n",
            "167/168, train_loss: 1.0620\n",
            "168/168, train_loss: 0.4961\n",
            "169/168, train_loss: 0.4496\n",
            "epoch 3 average loss: 0.6390\n",
            "----------\n",
            "epoch 4/10\n",
            "1/168, train_loss: 0.6692\n",
            "2/168, train_loss: 0.4575\n",
            "3/168, train_loss: 1.0296\n",
            "4/168, train_loss: 0.4251\n",
            "5/168, train_loss: 0.7343\n",
            "6/168, train_loss: 0.3818\n",
            "7/168, train_loss: 0.4690\n",
            "8/168, train_loss: 1.0414\n",
            "9/168, train_loss: 0.5767\n",
            "10/168, train_loss: 0.4625\n",
            "11/168, train_loss: 0.4464\n",
            "12/168, train_loss: 0.8718\n",
            "13/168, train_loss: 0.9945\n",
            "14/168, train_loss: 0.1831\n",
            "15/168, train_loss: 0.1861\n",
            "16/168, train_loss: 0.2775\n",
            "17/168, train_loss: 0.4692\n",
            "18/168, train_loss: 0.6112\n",
            "19/168, train_loss: 0.4378\n",
            "20/168, train_loss: 0.4418\n",
            "21/168, train_loss: 0.9758\n",
            "22/168, train_loss: 0.3867\n",
            "23/168, train_loss: 0.9384\n",
            "24/168, train_loss: 0.5076\n",
            "25/168, train_loss: 0.7930\n",
            "26/168, train_loss: 0.4574\n",
            "27/168, train_loss: 0.4672\n",
            "28/168, train_loss: 0.3135\n",
            "29/168, train_loss: 0.3498\n",
            "30/168, train_loss: 0.4602\n",
            "31/168, train_loss: 0.4061\n",
            "32/168, train_loss: 0.4716\n",
            "33/168, train_loss: 1.1110\n",
            "34/168, train_loss: 0.7449\n",
            "35/168, train_loss: 0.4679\n",
            "36/168, train_loss: 0.4068\n",
            "37/168, train_loss: 0.4626\n",
            "38/168, train_loss: 0.3986\n",
            "39/168, train_loss: 0.3147\n",
            "40/168, train_loss: 0.8054\n",
            "41/168, train_loss: 0.4267\n",
            "42/168, train_loss: 0.4848\n",
            "43/168, train_loss: 0.4598\n",
            "44/168, train_loss: 0.7473\n",
            "45/168, train_loss: 0.5681\n",
            "46/168, train_loss: 0.4130\n",
            "47/168, train_loss: 1.0043\n",
            "48/168, train_loss: 0.4058\n",
            "49/168, train_loss: 0.7545\n",
            "50/168, train_loss: 1.0047\n",
            "51/168, train_loss: 1.0138\n",
            "52/168, train_loss: 0.4366\n",
            "53/168, train_loss: 0.4509\n",
            "54/168, train_loss: 0.4028\n",
            "55/168, train_loss: 0.5643\n",
            "56/168, train_loss: 0.4454\n",
            "57/168, train_loss: 0.5088\n",
            "58/168, train_loss: 0.3898\n",
            "59/168, train_loss: 0.7627\n",
            "60/168, train_loss: 0.7901\n",
            "61/168, train_loss: 0.5183\n",
            "62/168, train_loss: 0.6775\n",
            "63/168, train_loss: 0.7659\n",
            "64/168, train_loss: 0.2040\n",
            "65/168, train_loss: 0.4671\n",
            "66/168, train_loss: 0.4834\n",
            "67/168, train_loss: 0.6617\n",
            "68/168, train_loss: 0.4621\n",
            "69/168, train_loss: 0.4899\n",
            "70/168, train_loss: 0.5208\n",
            "71/168, train_loss: 0.4478\n",
            "72/168, train_loss: 1.0667\n",
            "73/168, train_loss: 0.5156\n",
            "74/168, train_loss: 0.4471\n",
            "75/168, train_loss: 0.8460\n",
            "76/168, train_loss: 0.5470\n",
            "77/168, train_loss: 0.7166\n",
            "78/168, train_loss: 0.7641\n",
            "79/168, train_loss: 1.0723\n",
            "80/168, train_loss: 0.6890\n",
            "81/168, train_loss: 0.4531\n",
            "82/168, train_loss: 0.3927\n",
            "83/168, train_loss: 0.3897\n",
            "84/168, train_loss: 0.5619\n",
            "85/168, train_loss: 0.7667\n",
            "86/168, train_loss: 1.1589\n",
            "87/168, train_loss: 0.4612\n",
            "88/168, train_loss: 0.4697\n",
            "89/168, train_loss: 1.0875\n",
            "90/168, train_loss: 1.0427\n",
            "91/168, train_loss: 0.6418\n",
            "92/168, train_loss: 0.5423\n",
            "93/168, train_loss: 0.7522\n",
            "94/168, train_loss: 0.5557\n",
            "95/168, train_loss: 0.5593\n",
            "96/168, train_loss: 0.4629\n",
            "97/168, train_loss: 0.1955\n",
            "98/168, train_loss: 0.4182\n",
            "99/168, train_loss: 0.4871\n",
            "100/168, train_loss: 0.4567\n",
            "101/168, train_loss: 0.5196\n",
            "102/168, train_loss: 0.4077\n",
            "103/168, train_loss: 0.7398\n",
            "104/168, train_loss: 0.4327\n",
            "105/168, train_loss: 1.0547\n",
            "106/168, train_loss: 0.4622\n",
            "107/168, train_loss: 0.6810\n",
            "108/168, train_loss: 0.6854\n",
            "109/168, train_loss: 0.2218\n",
            "110/168, train_loss: 0.7642\n",
            "111/168, train_loss: 0.7639\n",
            "112/168, train_loss: 0.8655\n",
            "113/168, train_loss: 0.4723\n",
            "114/168, train_loss: 0.8801\n",
            "115/168, train_loss: 0.7539\n",
            "116/168, train_loss: 1.2980\n",
            "117/168, train_loss: 1.1168\n",
            "118/168, train_loss: 0.4566\n",
            "119/168, train_loss: 0.7478\n",
            "120/168, train_loss: 0.4383\n",
            "121/168, train_loss: 1.0112\n",
            "122/168, train_loss: 0.4475\n",
            "123/168, train_loss: 1.0408\n",
            "124/168, train_loss: 0.6474\n",
            "125/168, train_loss: 0.4980\n",
            "126/168, train_loss: 0.4593\n",
            "127/168, train_loss: 0.4581\n",
            "128/168, train_loss: 0.7238\n",
            "129/168, train_loss: 0.9635\n",
            "130/168, train_loss: 0.4728\n",
            "131/168, train_loss: 0.4834\n",
            "132/168, train_loss: 0.4817\n",
            "133/168, train_loss: 1.0449\n",
            "134/168, train_loss: 0.4966\n",
            "135/168, train_loss: 0.4773\n",
            "136/168, train_loss: 0.5014\n",
            "137/168, train_loss: 0.3698\n",
            "138/168, train_loss: 0.7566\n",
            "139/168, train_loss: 0.7090\n",
            "140/168, train_loss: 0.5611\n",
            "141/168, train_loss: 1.0081\n",
            "142/168, train_loss: 0.4524\n",
            "143/168, train_loss: 0.4525\n",
            "144/168, train_loss: 1.0486\n",
            "145/168, train_loss: 0.4631\n",
            "146/168, train_loss: 0.5448\n",
            "147/168, train_loss: 1.0035\n",
            "148/168, train_loss: 0.4912\n",
            "149/168, train_loss: 0.4945\n",
            "150/168, train_loss: 0.7104\n",
            "151/168, train_loss: 0.5223\n",
            "152/168, train_loss: 0.7133\n",
            "153/168, train_loss: 0.4706\n",
            "154/168, train_loss: 0.4568\n",
            "155/168, train_loss: 0.4386\n",
            "156/168, train_loss: 0.4420\n",
            "157/168, train_loss: 0.4865\n",
            "158/168, train_loss: 0.6684\n",
            "159/168, train_loss: 1.0742\n",
            "160/168, train_loss: 0.4568\n",
            "161/168, train_loss: 0.4447\n",
            "162/168, train_loss: 0.4267\n",
            "163/168, train_loss: 0.4267\n",
            "164/168, train_loss: 0.4402\n",
            "165/168, train_loss: 0.3197\n",
            "166/168, train_loss: 0.2741\n",
            "167/168, train_loss: 0.7027\n",
            "168/168, train_loss: 0.4361\n",
            "169/168, train_loss: 1.0086\n",
            "epoch 4 average loss: 0.6030\n",
            "current epoch: 4 current accuracy: 0.6000 best accuracy: 0.6235 at epoch 2\n",
            "----------\n",
            "epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.1726\n",
            "2/168, train_loss: 0.2958\n",
            "3/168, train_loss: 0.4195\n",
            "4/168, train_loss: 0.8138\n",
            "5/168, train_loss: 1.0870\n",
            "6/168, train_loss: 0.4793\n",
            "7/168, train_loss: 0.5539\n",
            "8/168, train_loss: 0.4575\n",
            "9/168, train_loss: 0.4281\n",
            "10/168, train_loss: 0.5743\n",
            "11/168, train_loss: 0.7275\n",
            "12/168, train_loss: 0.3060\n",
            "13/168, train_loss: 0.4236\n",
            "14/168, train_loss: 0.4560\n",
            "15/168, train_loss: 0.4455\n",
            "16/168, train_loss: 0.7256\n",
            "17/168, train_loss: 0.4458\n",
            "18/168, train_loss: 0.3045\n",
            "19/168, train_loss: 0.2903\n",
            "20/168, train_loss: 0.9715\n",
            "21/168, train_loss: 0.5390\n",
            "22/168, train_loss: 1.1899\n",
            "23/168, train_loss: 0.7143\n",
            "24/168, train_loss: 0.4304\n",
            "25/168, train_loss: 0.4427\n",
            "26/168, train_loss: 0.9901\n",
            "27/168, train_loss: 1.0061\n",
            "28/168, train_loss: 0.6291\n",
            "29/168, train_loss: 0.6088\n",
            "30/168, train_loss: 0.2026\n",
            "31/168, train_loss: 0.5273\n",
            "32/168, train_loss: 0.2511\n",
            "33/168, train_loss: 0.5958\n",
            "34/168, train_loss: 0.8843\n",
            "35/168, train_loss: 0.3224\n",
            "36/168, train_loss: 0.7894\n",
            "37/168, train_loss: 0.4492\n",
            "38/168, train_loss: 0.4360\n",
            "39/168, train_loss: 1.1899\n",
            "40/168, train_loss: 1.0648\n",
            "41/168, train_loss: 0.6735\n",
            "42/168, train_loss: 0.5212\n",
            "43/168, train_loss: 0.4505\n",
            "44/168, train_loss: 0.4414\n",
            "45/168, train_loss: 0.4811\n",
            "46/168, train_loss: 0.3219\n",
            "47/168, train_loss: 0.5480\n",
            "48/168, train_loss: 0.2922\n",
            "49/168, train_loss: 0.5529\n",
            "50/168, train_loss: 0.4645\n",
            "51/168, train_loss: 0.6155\n",
            "52/168, train_loss: 0.6301\n",
            "53/168, train_loss: 0.4945\n",
            "54/168, train_loss: 0.6715\n",
            "55/168, train_loss: 0.7221\n",
            "56/168, train_loss: 1.0270\n",
            "57/168, train_loss: 1.3169\n",
            "58/168, train_loss: 0.6040\n",
            "59/168, train_loss: 0.7265\n",
            "60/168, train_loss: 1.0434\n",
            "61/168, train_loss: 0.5679\n",
            "62/168, train_loss: 0.3104\n",
            "63/168, train_loss: 0.5542\n",
            "64/168, train_loss: 0.5563\n",
            "65/168, train_loss: 1.0351\n",
            "66/168, train_loss: 1.0114\n",
            "67/168, train_loss: 0.9080\n",
            "68/168, train_loss: 0.4307\n",
            "69/168, train_loss: 0.5591\n",
            "70/168, train_loss: 0.4746\n",
            "71/168, train_loss: 0.5032\n",
            "72/168, train_loss: 0.4395\n",
            "73/168, train_loss: 0.6376\n",
            "74/168, train_loss: 0.4810\n",
            "75/168, train_loss: 0.3782\n",
            "76/168, train_loss: 0.5373\n",
            "77/168, train_loss: 0.9782\n",
            "78/168, train_loss: 0.9791\n",
            "79/168, train_loss: 0.7222\n",
            "80/168, train_loss: 0.4533\n",
            "81/168, train_loss: 0.4599\n",
            "82/168, train_loss: 0.4545\n",
            "83/168, train_loss: 0.4672\n",
            "84/168, train_loss: 0.5303\n",
            "85/168, train_loss: 0.6376\n",
            "86/168, train_loss: 0.6495\n",
            "87/168, train_loss: 0.3113\n",
            "88/168, train_loss: 0.1803\n",
            "89/168, train_loss: 0.4509\n",
            "90/168, train_loss: 0.4526\n",
            "91/168, train_loss: 0.4547\n",
            "92/168, train_loss: 0.7149\n",
            "93/168, train_loss: 0.4645\n",
            "94/168, train_loss: 0.6264\n",
            "95/168, train_loss: 0.4560\n",
            "96/168, train_loss: 0.6671\n",
            "97/168, train_loss: 0.4929\n",
            "98/168, train_loss: 0.5443\n",
            "99/168, train_loss: 0.5147\n",
            "100/168, train_loss: 0.5058\n",
            "101/168, train_loss: 0.5512\n",
            "102/168, train_loss: 1.4660\n",
            "103/168, train_loss: 0.4433\n",
            "104/168, train_loss: 0.4918\n",
            "105/168, train_loss: 0.4417\n",
            "106/168, train_loss: 0.2675\n",
            "107/168, train_loss: 0.3448\n",
            "108/168, train_loss: 0.4583\n",
            "109/168, train_loss: 0.5116\n",
            "110/168, train_loss: 1.4823\n",
            "111/168, train_loss: 0.7170\n",
            "112/168, train_loss: 0.7180\n",
            "113/168, train_loss: 0.7943\n",
            "114/168, train_loss: 1.1556\n",
            "115/168, train_loss: 1.2281\n",
            "116/168, train_loss: 0.2032\n",
            "117/168, train_loss: 0.2514\n",
            "118/168, train_loss: 0.7286\n",
            "119/168, train_loss: 0.1900\n",
            "120/168, train_loss: 0.4405\n",
            "121/168, train_loss: 0.7194\n",
            "122/168, train_loss: 0.7190\n",
            "123/168, train_loss: 0.4936\n",
            "124/168, train_loss: 0.4283\n",
            "125/168, train_loss: 0.7200\n",
            "126/168, train_loss: 0.7203\n",
            "127/168, train_loss: 0.4364\n",
            "128/168, train_loss: 1.0381\n",
            "129/168, train_loss: 0.4494\n",
            "130/168, train_loss: 1.0999\n",
            "131/168, train_loss: 0.2172\n",
            "132/168, train_loss: 0.7973\n",
            "133/168, train_loss: 0.6937\n",
            "134/168, train_loss: 0.6363\n",
            "135/168, train_loss: 0.4380\n",
            "136/168, train_loss: 1.0273\n",
            "137/168, train_loss: 0.4470\n",
            "138/168, train_loss: 0.7181\n",
            "139/168, train_loss: 0.6701\n",
            "140/168, train_loss: 0.7171\n",
            "141/168, train_loss: 0.3436\n",
            "142/168, train_loss: 0.3063\n",
            "143/168, train_loss: 0.4296\n",
            "144/168, train_loss: 0.4715\n",
            "145/168, train_loss: 1.1091\n",
            "146/168, train_loss: 0.5329\n",
            "147/168, train_loss: 0.4497\n",
            "148/168, train_loss: 0.5109\n",
            "149/168, train_loss: 0.4179\n",
            "150/168, train_loss: 0.5050\n",
            "151/168, train_loss: 0.4042\n",
            "152/168, train_loss: 0.4401\n",
            "153/168, train_loss: 0.4710\n",
            "154/168, train_loss: 0.6395\n",
            "155/168, train_loss: 0.4198\n",
            "156/168, train_loss: 1.0234\n",
            "157/168, train_loss: 0.6028\n",
            "158/168, train_loss: 0.4476\n",
            "159/168, train_loss: 0.4793\n",
            "160/168, train_loss: 0.4262\n",
            "161/168, train_loss: 0.5131\n",
            "162/168, train_loss: 0.4644\n",
            "163/168, train_loss: 0.4256\n",
            "164/168, train_loss: 0.9421\n",
            "165/168, train_loss: 0.3316\n",
            "166/168, train_loss: 1.0293\n",
            "167/168, train_loss: 0.7194\n",
            "168/168, train_loss: 0.4773\n",
            "169/168, train_loss: 1.0392\n",
            "epoch 5 average loss: 0.5990\n",
            "----------\n",
            "epoch 6/10\n",
            "1/168, train_loss: 0.3171\n",
            "2/168, train_loss: 0.4165\n",
            "3/168, train_loss: 0.4316\n",
            "4/168, train_loss: 0.3402\n",
            "5/168, train_loss: 1.0372\n",
            "6/168, train_loss: 0.4356\n",
            "7/168, train_loss: 1.0220\n",
            "8/168, train_loss: 0.4187\n",
            "9/168, train_loss: 0.4499\n",
            "10/168, train_loss: 0.4411\n",
            "11/168, train_loss: 0.3168\n",
            "12/168, train_loss: 1.0222\n",
            "13/168, train_loss: 0.4782\n",
            "14/168, train_loss: 1.1065\n",
            "15/168, train_loss: 0.4731\n",
            "16/168, train_loss: 1.0727\n",
            "17/168, train_loss: 0.4046\n",
            "18/168, train_loss: 0.4831\n",
            "19/168, train_loss: 0.4748\n",
            "20/168, train_loss: 0.9816\n",
            "21/168, train_loss: 0.4770\n",
            "22/168, train_loss: 0.4545\n",
            "23/168, train_loss: 0.4674\n",
            "24/168, train_loss: 0.5104\n",
            "25/168, train_loss: 0.4225\n",
            "26/168, train_loss: 0.4299\n",
            "27/168, train_loss: 0.6063\n",
            "28/168, train_loss: 0.3883\n",
            "29/168, train_loss: 0.3988\n",
            "30/168, train_loss: 0.5064\n",
            "31/168, train_loss: 0.4294\n",
            "32/168, train_loss: 0.2501\n",
            "33/168, train_loss: 0.4184\n",
            "34/168, train_loss: 0.4377\n",
            "35/168, train_loss: 0.2973\n",
            "36/168, train_loss: 0.4240\n",
            "37/168, train_loss: 0.4877\n",
            "38/168, train_loss: 0.7594\n",
            "39/168, train_loss: 0.9724\n",
            "40/168, train_loss: 1.2157\n",
            "41/168, train_loss: 0.4303\n",
            "42/168, train_loss: 1.1150\n",
            "43/168, train_loss: 0.7235\n",
            "44/168, train_loss: 0.3846\n",
            "45/168, train_loss: 0.4242\n",
            "46/168, train_loss: 0.2254\n",
            "47/168, train_loss: 0.7687\n",
            "48/168, train_loss: 0.4210\n",
            "49/168, train_loss: 1.0245\n",
            "50/168, train_loss: 0.3136\n",
            "51/168, train_loss: 0.4327\n",
            "52/168, train_loss: 0.4790\n",
            "53/168, train_loss: 0.3997\n",
            "54/168, train_loss: 1.0494\n",
            "55/168, train_loss: 0.4065\n",
            "56/168, train_loss: 0.4270\n",
            "57/168, train_loss: 0.3660\n",
            "58/168, train_loss: 0.1705\n",
            "59/168, train_loss: 0.3836\n",
            "60/168, train_loss: 0.2517\n",
            "61/168, train_loss: 0.4007\n",
            "62/168, train_loss: 0.2457\n",
            "63/168, train_loss: 0.4326\n",
            "64/168, train_loss: 0.4551\n",
            "65/168, train_loss: 0.4144\n",
            "66/168, train_loss: 1.2744\n",
            "67/168, train_loss: 0.3388\n",
            "68/168, train_loss: 0.6612\n",
            "69/168, train_loss: 0.4367\n",
            "70/168, train_loss: 0.7698\n",
            "71/168, train_loss: 0.7685\n",
            "72/168, train_loss: 0.1922\n",
            "73/168, train_loss: 1.0825\n",
            "74/168, train_loss: 0.4830\n",
            "75/168, train_loss: 0.7595\n",
            "76/168, train_loss: 1.1550\n",
            "77/168, train_loss: 1.0649\n",
            "78/168, train_loss: 1.0286\n",
            "79/168, train_loss: 0.4309\n",
            "80/168, train_loss: 0.7467\n",
            "81/168, train_loss: 0.8205\n",
            "82/168, train_loss: 0.6104\n",
            "83/168, train_loss: 0.6164\n",
            "84/168, train_loss: 0.7023\n",
            "85/168, train_loss: 0.4250\n",
            "86/168, train_loss: 0.9962\n",
            "87/168, train_loss: 0.4838\n",
            "88/168, train_loss: 0.5539\n",
            "89/168, train_loss: 0.4268\n",
            "90/168, train_loss: 0.5985\n",
            "91/168, train_loss: 0.4712\n",
            "92/168, train_loss: 0.5187\n",
            "93/168, train_loss: 0.7056\n",
            "94/168, train_loss: 0.3353\n",
            "95/168, train_loss: 1.0098\n",
            "96/168, train_loss: 0.9632\n",
            "97/168, train_loss: 0.4857\n",
            "98/168, train_loss: 0.7239\n",
            "99/168, train_loss: 0.4416\n",
            "100/168, train_loss: 0.9692\n",
            "101/168, train_loss: 0.4907\n",
            "102/168, train_loss: 0.7190\n",
            "103/168, train_loss: 0.9703\n",
            "104/168, train_loss: 0.4411\n",
            "105/168, train_loss: 0.4871\n",
            "106/168, train_loss: 1.0322\n",
            "107/168, train_loss: 0.4638\n",
            "108/168, train_loss: 0.9598\n",
            "109/168, train_loss: 0.5166\n",
            "110/168, train_loss: 0.4653\n",
            "111/168, train_loss: 0.7174\n",
            "112/168, train_loss: 0.6600\n",
            "113/168, train_loss: 0.4669\n",
            "114/168, train_loss: 0.5000\n",
            "115/168, train_loss: 0.5017\n",
            "116/168, train_loss: 0.4241\n",
            "117/168, train_loss: 0.4342\n",
            "118/168, train_loss: 0.4844\n",
            "119/168, train_loss: 0.4335\n",
            "120/168, train_loss: 0.3791\n",
            "121/168, train_loss: 0.4228\n",
            "122/168, train_loss: 0.1236\n",
            "123/168, train_loss: 0.2532\n",
            "124/168, train_loss: 0.2583\n",
            "125/168, train_loss: 0.8346\n",
            "126/168, train_loss: 0.4631\n",
            "127/168, train_loss: 0.6400\n",
            "128/168, train_loss: 0.6779\n",
            "129/168, train_loss: 0.2528\n",
            "130/168, train_loss: 0.7269\n",
            "131/168, train_loss: 0.4665\n",
            "132/168, train_loss: 0.4627\n",
            "133/168, train_loss: 0.4680\n",
            "134/168, train_loss: 0.5087\n",
            "135/168, train_loss: 0.4923\n",
            "136/168, train_loss: 0.5217\n",
            "137/168, train_loss: 0.4375\n",
            "138/168, train_loss: 0.4309\n",
            "139/168, train_loss: 0.2912\n",
            "140/168, train_loss: 1.0151\n",
            "141/168, train_loss: 0.3007\n",
            "142/168, train_loss: 0.4175\n",
            "143/168, train_loss: 1.0938\n",
            "144/168, train_loss: 0.5213\n",
            "145/168, train_loss: 0.6196\n",
            "146/168, train_loss: 1.1070\n",
            "147/168, train_loss: 0.4226\n",
            "148/168, train_loss: 0.3867\n",
            "149/168, train_loss: 0.4087\n",
            "150/168, train_loss: 0.4815\n",
            "151/168, train_loss: 1.1212\n",
            "152/168, train_loss: 0.3959\n",
            "153/168, train_loss: 0.7513\n",
            "154/168, train_loss: 0.4082\n",
            "155/168, train_loss: 0.3049\n",
            "156/168, train_loss: 0.4587\n",
            "157/168, train_loss: 0.1640\n",
            "158/168, train_loss: 0.2920\n",
            "159/168, train_loss: 1.0482\n",
            "160/168, train_loss: 0.5920\n",
            "161/168, train_loss: 0.7417\n",
            "162/168, train_loss: 0.5946\n",
            "163/168, train_loss: 0.4465\n",
            "164/168, train_loss: 0.5662\n",
            "165/168, train_loss: 1.0667\n",
            "166/168, train_loss: 0.4487\n",
            "167/168, train_loss: 0.4884\n",
            "168/168, train_loss: 0.4162\n",
            "169/168, train_loss: 0.4277\n",
            "epoch 6 average loss: 0.5708\n",
            "saved new best metric model\n",
            "current epoch: 6 current accuracy: 0.6588 best accuracy: 0.6588 at epoch 6\n",
            "----------\n",
            "epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.4194\n",
            "2/168, train_loss: 0.1786\n",
            "3/168, train_loss: 0.3738\n",
            "4/168, train_loss: 1.0171\n",
            "5/168, train_loss: 0.4539\n",
            "6/168, train_loss: 1.0966\n",
            "7/168, train_loss: 0.4761\n",
            "8/168, train_loss: 0.4040\n",
            "9/168, train_loss: 1.1102\n",
            "10/168, train_loss: 0.5392\n",
            "11/168, train_loss: 0.4152\n",
            "12/168, train_loss: 0.6062\n",
            "13/168, train_loss: 1.0478\n",
            "14/168, train_loss: 0.4975\n",
            "15/168, train_loss: 0.7424\n",
            "16/168, train_loss: 0.4263\n",
            "17/168, train_loss: 0.6550\n",
            "18/168, train_loss: 0.8953\n",
            "19/168, train_loss: 0.3480\n",
            "20/168, train_loss: 0.9211\n",
            "21/168, train_loss: 1.0865\n",
            "22/168, train_loss: 0.4806\n",
            "23/168, train_loss: 0.1107\n",
            "24/168, train_loss: 0.5368\n",
            "25/168, train_loss: 1.0114\n",
            "26/168, train_loss: 0.4840\n",
            "27/168, train_loss: 0.5071\n",
            "28/168, train_loss: 0.7249\n",
            "29/168, train_loss: 0.4422\n",
            "30/168, train_loss: 0.4617\n",
            "31/168, train_loss: 0.9551\n",
            "32/168, train_loss: 0.5382\n",
            "33/168, train_loss: 0.4244\n",
            "34/168, train_loss: 0.4610\n",
            "35/168, train_loss: 0.1356\n",
            "36/168, train_loss: 0.5256\n",
            "37/168, train_loss: 0.1609\n",
            "38/168, train_loss: 0.5204\n",
            "39/168, train_loss: 0.4337\n",
            "40/168, train_loss: 0.8864\n",
            "41/168, train_loss: 0.5484\n",
            "42/168, train_loss: 0.5031\n",
            "43/168, train_loss: 0.2121\n",
            "44/168, train_loss: 1.0129\n",
            "45/168, train_loss: 0.4847\n",
            "46/168, train_loss: 0.8322\n",
            "47/168, train_loss: 0.4421\n",
            "48/168, train_loss: 0.4377\n",
            "49/168, train_loss: 0.8480\n",
            "50/168, train_loss: 0.4661\n",
            "51/168, train_loss: 0.4451\n",
            "52/168, train_loss: 0.4213\n",
            "53/168, train_loss: 0.8098\n",
            "54/168, train_loss: 0.9978\n",
            "55/168, train_loss: 1.1458\n",
            "56/168, train_loss: 1.1952\n",
            "57/168, train_loss: 0.2207\n",
            "58/168, train_loss: 0.4445\n",
            "59/168, train_loss: 0.8385\n",
            "60/168, train_loss: 0.5030\n",
            "61/168, train_loss: 0.3017\n",
            "62/168, train_loss: 1.0142\n",
            "63/168, train_loss: 0.6240\n",
            "64/168, train_loss: 0.4896\n",
            "65/168, train_loss: 0.4953\n",
            "66/168, train_loss: 0.4769\n",
            "67/168, train_loss: 0.6997\n",
            "68/168, train_loss: 0.5409\n",
            "69/168, train_loss: 0.1808\n",
            "70/168, train_loss: 1.0323\n",
            "71/168, train_loss: 0.7301\n",
            "72/168, train_loss: 0.4733\n",
            "73/168, train_loss: 0.4346\n",
            "74/168, train_loss: 0.7030\n",
            "75/168, train_loss: 1.0086\n",
            "76/168, train_loss: 0.4487\n",
            "77/168, train_loss: 0.4646\n",
            "78/168, train_loss: 0.8516\n",
            "79/168, train_loss: 0.1586\n",
            "80/168, train_loss: 0.4599\n",
            "81/168, train_loss: 0.4515\n",
            "82/168, train_loss: 0.4151\n",
            "83/168, train_loss: 0.4590\n",
            "84/168, train_loss: 0.4492\n",
            "85/168, train_loss: 0.4171\n",
            "86/168, train_loss: 0.5608\n",
            "87/168, train_loss: 0.4458\n",
            "88/168, train_loss: 0.4208\n",
            "89/168, train_loss: 0.1407\n",
            "90/168, train_loss: 0.4585\n",
            "91/168, train_loss: 0.4784\n",
            "92/168, train_loss: 0.5080\n",
            "93/168, train_loss: 1.1028\n",
            "94/168, train_loss: 0.4425\n",
            "95/168, train_loss: 0.8735\n",
            "96/168, train_loss: 0.2689\n",
            "97/168, train_loss: 0.4704\n",
            "98/168, train_loss: 0.2420\n",
            "99/168, train_loss: 0.3906\n",
            "100/168, train_loss: 1.1519\n",
            "101/168, train_loss: 0.2115\n",
            "102/168, train_loss: 0.2755\n",
            "103/168, train_loss: 0.1924\n",
            "104/168, train_loss: 0.1853\n",
            "105/168, train_loss: 0.6999\n",
            "106/168, train_loss: 0.4063\n",
            "107/168, train_loss: 0.4135\n",
            "108/168, train_loss: 0.1490\n",
            "109/168, train_loss: 0.4314\n",
            "110/168, train_loss: 0.2477\n",
            "111/168, train_loss: 0.8089\n",
            "112/168, train_loss: 0.4066\n",
            "113/168, train_loss: 0.4616\n",
            "114/168, train_loss: 0.2287\n",
            "115/168, train_loss: 0.1431\n",
            "116/168, train_loss: 0.2046\n",
            "117/168, train_loss: 0.5363\n",
            "118/168, train_loss: 0.4756\n",
            "119/168, train_loss: 0.6953\n",
            "120/168, train_loss: 0.4375\n",
            "121/168, train_loss: 1.2148\n",
            "122/168, train_loss: 1.1694\n",
            "123/168, train_loss: 1.2753\n",
            "124/168, train_loss: 1.0334\n",
            "125/168, train_loss: 0.7136\n",
            "126/168, train_loss: 0.3988\n",
            "127/168, train_loss: 0.4431\n",
            "128/168, train_loss: 0.4272\n",
            "129/168, train_loss: 1.1459\n",
            "130/168, train_loss: 0.4843\n",
            "131/168, train_loss: 1.1800\n",
            "132/168, train_loss: 0.6360\n",
            "133/168, train_loss: 0.6230\n",
            "134/168, train_loss: 0.6986\n",
            "135/168, train_loss: 0.4295\n",
            "136/168, train_loss: 0.5048\n",
            "137/168, train_loss: 0.4794\n",
            "138/168, train_loss: 0.5028\n",
            "139/168, train_loss: 0.2141\n",
            "140/168, train_loss: 0.6104\n",
            "141/168, train_loss: 0.4402\n",
            "142/168, train_loss: 0.1515\n",
            "143/168, train_loss: 0.2160\n",
            "144/168, train_loss: 0.9332\n",
            "145/168, train_loss: 0.5704\n",
            "146/168, train_loss: 0.5195\n",
            "147/168, train_loss: 0.5945\n",
            "148/168, train_loss: 0.4238\n",
            "149/168, train_loss: 0.5250\n",
            "150/168, train_loss: 0.5607\n",
            "151/168, train_loss: 1.3121\n",
            "152/168, train_loss: 1.1322\n",
            "153/168, train_loss: 0.4852\n",
            "154/168, train_loss: 0.1786\n",
            "155/168, train_loss: 0.4362\n",
            "156/168, train_loss: 0.4461\n",
            "157/168, train_loss: 0.7177\n",
            "158/168, train_loss: 1.1686\n",
            "159/168, train_loss: 0.4947\n",
            "160/168, train_loss: 0.4067\n",
            "161/168, train_loss: 0.4111\n",
            "162/168, train_loss: 0.4026\n",
            "163/168, train_loss: 0.7399\n",
            "164/168, train_loss: 0.5453\n",
            "165/168, train_loss: 0.4294\n",
            "166/168, train_loss: 1.2040\n",
            "167/168, train_loss: 1.1493\n",
            "168/168, train_loss: 0.4116\n",
            "169/168, train_loss: 0.4617\n",
            "epoch 7 average loss: 0.5759\n",
            "----------\n",
            "epoch 8/10\n",
            "1/168, train_loss: 1.1595\n",
            "2/168, train_loss: 1.1378\n",
            "3/168, train_loss: 0.4364\n",
            "4/168, train_loss: 0.4437\n",
            "5/168, train_loss: 0.4252\n",
            "6/168, train_loss: 0.4678\n",
            "7/168, train_loss: 0.4037\n",
            "8/168, train_loss: 1.0666\n",
            "9/168, train_loss: 1.0599\n",
            "10/168, train_loss: 0.2298\n",
            "11/168, train_loss: 0.4707\n",
            "12/168, train_loss: 0.7322\n",
            "13/168, train_loss: 0.7294\n",
            "14/168, train_loss: 0.2999\n",
            "15/168, train_loss: 0.4005\n",
            "16/168, train_loss: 0.6163\n",
            "17/168, train_loss: 0.2325\n",
            "18/168, train_loss: 1.1149\n",
            "19/168, train_loss: 0.3899\n",
            "20/168, train_loss: 0.3154\n",
            "21/168, train_loss: 0.4334\n",
            "22/168, train_loss: 1.1439\n",
            "23/168, train_loss: 0.2549\n",
            "24/168, train_loss: 0.3039\n",
            "25/168, train_loss: 0.5072\n",
            "26/168, train_loss: 0.8011\n",
            "27/168, train_loss: 0.2165\n",
            "28/168, train_loss: 0.3453\n",
            "29/168, train_loss: 0.5010\n",
            "30/168, train_loss: 1.1207\n",
            "31/168, train_loss: 1.0759\n",
            "32/168, train_loss: 0.4893\n",
            "33/168, train_loss: 0.3755\n",
            "34/168, train_loss: 0.6793\n",
            "35/168, train_loss: 0.4484\n",
            "36/168, train_loss: 0.4526\n",
            "37/168, train_loss: 1.0238\n",
            "38/168, train_loss: 0.4366\n",
            "39/168, train_loss: 0.4572\n",
            "40/168, train_loss: 0.4437\n",
            "41/168, train_loss: 0.5598\n",
            "42/168, train_loss: 0.2673\n",
            "43/168, train_loss: 0.3653\n",
            "44/168, train_loss: 1.0146\n",
            "45/168, train_loss: 0.4551\n",
            "46/168, train_loss: 0.5049\n",
            "47/168, train_loss: 0.4677\n",
            "48/168, train_loss: 0.4914\n",
            "49/168, train_loss: 0.5857\n",
            "50/168, train_loss: 0.4065\n",
            "51/168, train_loss: 0.3847\n",
            "52/168, train_loss: 1.4207\n",
            "53/168, train_loss: 0.8252\n",
            "54/168, train_loss: 0.4709\n",
            "55/168, train_loss: 0.7298\n",
            "56/168, train_loss: 0.6444\n",
            "57/168, train_loss: 0.4705\n",
            "58/168, train_loss: 0.5615\n",
            "59/168, train_loss: 1.0182\n",
            "60/168, train_loss: 0.5782\n",
            "61/168, train_loss: 0.4299\n",
            "62/168, train_loss: 1.0565\n",
            "63/168, train_loss: 0.4258\n",
            "64/168, train_loss: 0.3528\n",
            "65/168, train_loss: 0.7272\n",
            "66/168, train_loss: 0.3989\n",
            "67/168, train_loss: 0.3963\n",
            "68/168, train_loss: 0.4057\n",
            "69/168, train_loss: 0.4206\n",
            "70/168, train_loss: 0.1086\n",
            "71/168, train_loss: 0.2299\n",
            "72/168, train_loss: 0.3670\n",
            "73/168, train_loss: 0.4311\n",
            "74/168, train_loss: 0.4242\n",
            "75/168, train_loss: 0.4719\n",
            "76/168, train_loss: 1.1060\n",
            "77/168, train_loss: 0.4265\n",
            "78/168, train_loss: 0.2546\n",
            "79/168, train_loss: 0.2402\n",
            "80/168, train_loss: 0.5899\n",
            "81/168, train_loss: 0.4828\n",
            "82/168, train_loss: 0.4094\n",
            "83/168, train_loss: 0.4403\n",
            "84/168, train_loss: 0.4103\n",
            "85/168, train_loss: 0.7318\n",
            "86/168, train_loss: 1.1701\n",
            "87/168, train_loss: 0.9880\n",
            "88/168, train_loss: 0.5816\n",
            "89/168, train_loss: 0.4780\n",
            "90/168, train_loss: 1.0984\n",
            "91/168, train_loss: 0.1774\n",
            "92/168, train_loss: 0.4129\n",
            "93/168, train_loss: 0.4160\n",
            "94/168, train_loss: 0.4525\n",
            "95/168, train_loss: 0.4468\n",
            "96/168, train_loss: 0.4158\n",
            "97/168, train_loss: 0.4728\n",
            "98/168, train_loss: 0.7366\n",
            "99/168, train_loss: 0.4267\n",
            "100/168, train_loss: 0.1975\n",
            "101/168, train_loss: 0.4046\n",
            "102/168, train_loss: 0.4248\n",
            "103/168, train_loss: 0.5211\n",
            "104/168, train_loss: 1.1543\n",
            "105/168, train_loss: 0.3773\n",
            "106/168, train_loss: 1.0529\n",
            "107/168, train_loss: 0.4504\n",
            "108/168, train_loss: 0.3887\n",
            "109/168, train_loss: 0.4264\n",
            "110/168, train_loss: 0.1999\n",
            "111/168, train_loss: 0.7342\n",
            "112/168, train_loss: 0.1284\n",
            "113/168, train_loss: 0.1227\n",
            "114/168, train_loss: 0.4019\n",
            "115/168, train_loss: 0.3550\n",
            "116/168, train_loss: 0.2299\n",
            "117/168, train_loss: 0.7292\n",
            "118/168, train_loss: 1.6063\n",
            "119/168, train_loss: 0.7274\n",
            "120/168, train_loss: 0.3787\n",
            "121/168, train_loss: 1.1340\n",
            "122/168, train_loss: 0.7910\n",
            "123/168, train_loss: 0.1718\n",
            "124/168, train_loss: 0.3797\n",
            "125/168, train_loss: 0.3695\n",
            "126/168, train_loss: 0.7218\n",
            "127/168, train_loss: 1.1568\n",
            "128/168, train_loss: 0.6653\n",
            "129/168, train_loss: 0.7194\n",
            "130/168, train_loss: 0.8822\n",
            "131/168, train_loss: 0.4785\n",
            "132/168, train_loss: 0.3732\n",
            "133/168, train_loss: 0.3877\n",
            "134/168, train_loss: 1.0980\n",
            "135/168, train_loss: 0.4355\n",
            "136/168, train_loss: 0.4481\n",
            "137/168, train_loss: 0.3468\n",
            "138/168, train_loss: 0.3944\n",
            "139/168, train_loss: 0.4137\n",
            "140/168, train_loss: 0.4543\n",
            "141/168, train_loss: 0.4770\n",
            "142/168, train_loss: 0.6657\n",
            "143/168, train_loss: 0.2070\n",
            "144/168, train_loss: 0.7134\n",
            "145/168, train_loss: 0.3799\n",
            "146/168, train_loss: 0.3829\n",
            "147/168, train_loss: 0.1906\n",
            "148/168, train_loss: 0.4753\n",
            "149/168, train_loss: 0.3729\n",
            "150/168, train_loss: 1.0830\n",
            "151/168, train_loss: 0.3922\n",
            "152/168, train_loss: 0.3501\n",
            "153/168, train_loss: 0.1064\n",
            "154/168, train_loss: 0.4304\n",
            "155/168, train_loss: 0.5840\n",
            "156/168, train_loss: 0.6263\n",
            "157/168, train_loss: 0.5293\n",
            "158/168, train_loss: 0.6991\n",
            "159/168, train_loss: 0.9354\n",
            "160/168, train_loss: 0.9258\n",
            "161/168, train_loss: 0.5360\n",
            "162/168, train_loss: 0.7108\n",
            "163/168, train_loss: 0.4476\n",
            "164/168, train_loss: 0.5738\n",
            "165/168, train_loss: 0.4176\n",
            "166/168, train_loss: 1.2005\n",
            "167/168, train_loss: 0.3918\n",
            "168/168, train_loss: 0.7480\n",
            "169/168, train_loss: 0.5581\n",
            "epoch 8 average loss: 0.5611\n",
            "current epoch: 8 current accuracy: 0.5412 best accuracy: 0.6588 at epoch 6\n",
            "----------\n",
            "epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/168, train_loss: 0.4313\n",
            "2/168, train_loss: 0.4506\n",
            "3/168, train_loss: 0.2256\n",
            "4/168, train_loss: 0.4232\n",
            "5/168, train_loss: 0.5552\n",
            "6/168, train_loss: 0.3971\n",
            "7/168, train_loss: 0.5241\n",
            "8/168, train_loss: 0.4060\n",
            "9/168, train_loss: 0.7089\n",
            "10/168, train_loss: 0.8875\n",
            "11/168, train_loss: 0.4643\n",
            "12/168, train_loss: 0.5480\n",
            "13/168, train_loss: 0.1824\n",
            "14/168, train_loss: 0.5496\n",
            "15/168, train_loss: 0.4014\n",
            "16/168, train_loss: 0.3957\n",
            "17/168, train_loss: 0.7076\n",
            "18/168, train_loss: 0.3844\n",
            "19/168, train_loss: 1.0748\n",
            "20/168, train_loss: 0.3924\n",
            "21/168, train_loss: 0.7728\n",
            "22/168, train_loss: 1.0687\n",
            "23/168, train_loss: 0.5784\n",
            "24/168, train_loss: 0.4415\n",
            "25/168, train_loss: 0.4375\n",
            "26/168, train_loss: 0.8904\n",
            "27/168, train_loss: 0.4492\n",
            "28/168, train_loss: 0.5042\n",
            "29/168, train_loss: 0.2214\n",
            "30/168, train_loss: 0.2943\n",
            "31/168, train_loss: 0.4166\n",
            "32/168, train_loss: 1.0611\n",
            "33/168, train_loss: 0.7041\n",
            "34/168, train_loss: 0.8650\n",
            "35/168, train_loss: 0.3621\n",
            "36/168, train_loss: 1.1308\n",
            "37/168, train_loss: 0.4047\n",
            "38/168, train_loss: 0.4164\n",
            "39/168, train_loss: 0.4020\n",
            "40/168, train_loss: 0.4047\n",
            "41/168, train_loss: 0.7015\n",
            "42/168, train_loss: 0.4588\n",
            "43/168, train_loss: 1.1350\n",
            "44/168, train_loss: 0.3967\n",
            "45/168, train_loss: 0.1923\n",
            "46/168, train_loss: 1.1707\n",
            "47/168, train_loss: 0.5797\n",
            "48/168, train_loss: 0.7009\n",
            "49/168, train_loss: 0.8280\n",
            "50/168, train_loss: 0.6955\n",
            "51/168, train_loss: 0.4329\n",
            "52/168, train_loss: 0.4441\n",
            "53/168, train_loss: 0.3885\n",
            "54/168, train_loss: 0.3826\n",
            "55/168, train_loss: 0.5281\n",
            "56/168, train_loss: 0.6989\n",
            "57/168, train_loss: 0.3666\n",
            "58/168, train_loss: 0.3092\n",
            "59/168, train_loss: 0.3984\n",
            "60/168, train_loss: 0.3606\n",
            "61/168, train_loss: 1.1139\n",
            "62/168, train_loss: 0.7666\n",
            "63/168, train_loss: 0.3725\n",
            "64/168, train_loss: 0.3580\n",
            "65/168, train_loss: 0.9458\n",
            "66/168, train_loss: 0.4128\n",
            "67/168, train_loss: 0.3694\n",
            "68/168, train_loss: 0.7961\n",
            "69/168, train_loss: 1.2336\n",
            "70/168, train_loss: 0.6976\n",
            "71/168, train_loss: 0.4224\n",
            "72/168, train_loss: 0.8109\n",
            "73/168, train_loss: 0.1017\n",
            "74/168, train_loss: 0.3970\n",
            "75/168, train_loss: 1.0500\n",
            "76/168, train_loss: 0.4144\n",
            "77/168, train_loss: 0.5489\n",
            "78/168, train_loss: 1.1041\n",
            "79/168, train_loss: 0.6254\n",
            "80/168, train_loss: 0.4342\n",
            "81/168, train_loss: 0.6948\n",
            "82/168, train_loss: 0.3313\n",
            "83/168, train_loss: 0.4035\n",
            "84/168, train_loss: 0.1213\n",
            "85/168, train_loss: 0.4415\n",
            "86/168, train_loss: 0.7103\n",
            "87/168, train_loss: 1.1131\n",
            "88/168, train_loss: 0.5307\n",
            "89/168, train_loss: 0.5295\n",
            "90/168, train_loss: 0.4844\n",
            "91/168, train_loss: 0.6946\n",
            "92/168, train_loss: 0.3996\n",
            "93/168, train_loss: 0.2588\n",
            "94/168, train_loss: 0.4572\n",
            "95/168, train_loss: 0.4335\n",
            "96/168, train_loss: 0.3939\n",
            "97/168, train_loss: 0.6948\n",
            "98/168, train_loss: 0.6948\n",
            "99/168, train_loss: 0.3652\n",
            "100/168, train_loss: 0.6948\n",
            "101/168, train_loss: 0.4031\n",
            "102/168, train_loss: 0.6472\n",
            "103/168, train_loss: 0.2462\n",
            "104/168, train_loss: 0.4694\n",
            "105/168, train_loss: 0.1712\n",
            "106/168, train_loss: 0.9520\n",
            "107/168, train_loss: 0.6384\n",
            "108/168, train_loss: 0.4132\n",
            "109/168, train_loss: 0.1615\n",
            "110/168, train_loss: 0.6244\n",
            "111/168, train_loss: 0.9210\n",
            "112/168, train_loss: 1.1064\n",
            "113/168, train_loss: 0.4531\n",
            "114/168, train_loss: 0.2165\n",
            "115/168, train_loss: 0.4583\n",
            "116/168, train_loss: 0.0881\n",
            "117/168, train_loss: 0.3531\n",
            "118/168, train_loss: 0.7021\n",
            "119/168, train_loss: 0.4300\n",
            "120/168, train_loss: 0.3895\n",
            "121/168, train_loss: 0.7449\n",
            "122/168, train_loss: 1.2029\n",
            "123/168, train_loss: 0.4802\n",
            "124/168, train_loss: 0.7045\n",
            "125/168, train_loss: 0.3809\n",
            "126/168, train_loss: 0.7044\n",
            "127/168, train_loss: 0.6700\n",
            "128/168, train_loss: 0.3889\n",
            "129/168, train_loss: 0.4855\n",
            "130/168, train_loss: 0.4990\n",
            "131/168, train_loss: 0.4051\n",
            "132/168, train_loss: 0.4250\n",
            "133/168, train_loss: 0.5607\n",
            "134/168, train_loss: 0.1690\n",
            "135/168, train_loss: 0.2502\n",
            "136/168, train_loss: 0.5447\n",
            "137/168, train_loss: 0.3721\n",
            "138/168, train_loss: 0.4236\n",
            "139/168, train_loss: 0.4075\n",
            "140/168, train_loss: 1.3026\n",
            "141/168, train_loss: 0.8333\n",
            "142/168, train_loss: 1.2572\n",
            "143/168, train_loss: 1.9058\n",
            "144/168, train_loss: 0.3704\n",
            "145/168, train_loss: 0.1147\n",
            "146/168, train_loss: 0.8726\n",
            "147/168, train_loss: 0.2909\n",
            "148/168, train_loss: 1.1214\n",
            "149/168, train_loss: 0.7226\n",
            "150/168, train_loss: 1.1123\n",
            "151/168, train_loss: 0.4096\n",
            "152/168, train_loss: 0.2336\n",
            "153/168, train_loss: 0.4840\n",
            "154/168, train_loss: 0.4786\n",
            "155/168, train_loss: 0.2530\n",
            "156/168, train_loss: 0.4173\n",
            "157/168, train_loss: 0.4701\n",
            "158/168, train_loss: 0.5085\n",
            "159/168, train_loss: 0.4439\n",
            "160/168, train_loss: 0.5300\n",
            "161/168, train_loss: 1.0833\n",
            "162/168, train_loss: 0.4943\n",
            "163/168, train_loss: 1.1143\n",
            "164/168, train_loss: 0.2192\n",
            "165/168, train_loss: 0.3455\n",
            "166/168, train_loss: 0.5017\n",
            "167/168, train_loss: 1.1089\n",
            "168/168, train_loss: 1.0638\n",
            "169/168, train_loss: 0.4329\n",
            "epoch 9 average loss: 0.5721\n",
            "----------\n",
            "epoch 10/10\n",
            "1/168, train_loss: 0.7412\n",
            "2/168, train_loss: 0.1483\n",
            "3/168, train_loss: 0.7429\n",
            "4/168, train_loss: 0.1827\n",
            "5/168, train_loss: 0.4323\n",
            "6/168, train_loss: 0.4254\n",
            "7/168, train_loss: 0.4089\n",
            "8/168, train_loss: 0.7461\n",
            "9/168, train_loss: 0.4164\n",
            "10/168, train_loss: 0.9861\n",
            "11/168, train_loss: 0.0644\n",
            "12/168, train_loss: 0.3307\n",
            "13/168, train_loss: 0.3948\n",
            "14/168, train_loss: 0.3942\n",
            "15/168, train_loss: 0.1300\n",
            "16/168, train_loss: 0.4972\n",
            "17/168, train_loss: 0.5243\n",
            "18/168, train_loss: 0.4780\n",
            "19/168, train_loss: 0.3986\n",
            "20/168, train_loss: 0.4885\n",
            "21/168, train_loss: 0.4173\n",
            "22/168, train_loss: 0.4806\n",
            "23/168, train_loss: 0.3772\n",
            "24/168, train_loss: 0.3755\n",
            "25/168, train_loss: 0.5014\n",
            "26/168, train_loss: 0.4270\n",
            "27/168, train_loss: 1.0911\n",
            "28/168, train_loss: 0.3642\n",
            "29/168, train_loss: 0.7786\n",
            "30/168, train_loss: 1.2231\n",
            "31/168, train_loss: 0.4592\n",
            "32/168, train_loss: 0.6305\n",
            "33/168, train_loss: 0.5975\n",
            "34/168, train_loss: 0.3995\n",
            "35/168, train_loss: 0.4236\n",
            "36/168, train_loss: 0.4956\n",
            "37/168, train_loss: 0.1046\n",
            "38/168, train_loss: 0.4565\n",
            "39/168, train_loss: 1.0924\n",
            "40/168, train_loss: 0.3988\n",
            "41/168, train_loss: 0.4981\n",
            "42/168, train_loss: 0.3923\n",
            "43/168, train_loss: 0.4103\n",
            "44/168, train_loss: 0.2990\n",
            "45/168, train_loss: 0.1809\n",
            "46/168, train_loss: 0.3893\n",
            "47/168, train_loss: 1.1806\n",
            "48/168, train_loss: 0.4806\n",
            "49/168, train_loss: 0.0813\n",
            "50/168, train_loss: 0.8972\n",
            "51/168, train_loss: 0.7568\n",
            "52/168, train_loss: 0.3234\n",
            "53/168, train_loss: 0.6419\n",
            "54/168, train_loss: 0.1422\n",
            "55/168, train_loss: 0.1471\n",
            "56/168, train_loss: 0.3708\n",
            "57/168, train_loss: 0.7421\n",
            "58/168, train_loss: 0.9954\n",
            "59/168, train_loss: 1.0648\n",
            "60/168, train_loss: 0.4987\n",
            "61/168, train_loss: 0.4069\n",
            "62/168, train_loss: 0.6203\n",
            "63/168, train_loss: 0.4890\n",
            "64/168, train_loss: 0.4701\n",
            "65/168, train_loss: 0.4092\n",
            "66/168, train_loss: 0.1323\n",
            "67/168, train_loss: 0.4191\n",
            "68/168, train_loss: 1.4027\n",
            "69/168, train_loss: 0.4543\n",
            "70/168, train_loss: 0.3190\n",
            "71/168, train_loss: 0.4007\n",
            "72/168, train_loss: 0.1496\n",
            "73/168, train_loss: 0.4926\n",
            "74/168, train_loss: 0.7367\n",
            "75/168, train_loss: 0.5293\n",
            "76/168, train_loss: 1.0352\n",
            "77/168, train_loss: 1.0766\n",
            "78/168, train_loss: 0.3702\n",
            "79/168, train_loss: 0.4560\n",
            "80/168, train_loss: 0.4338\n",
            "81/168, train_loss: 0.5188\n",
            "82/168, train_loss: 0.5535\n",
            "83/168, train_loss: 0.3663\n",
            "84/168, train_loss: 0.3307\n",
            "85/168, train_loss: 0.5024\n",
            "86/168, train_loss: 1.1120\n",
            "87/168, train_loss: 0.7226\n",
            "88/168, train_loss: 0.3470\n",
            "89/168, train_loss: 0.1415\n",
            "90/168, train_loss: 0.5026\n",
            "91/168, train_loss: 0.7192\n",
            "92/168, train_loss: 1.0556\n",
            "93/168, train_loss: 0.0773\n",
            "94/168, train_loss: 0.3816\n",
            "95/168, train_loss: 0.7161\n",
            "96/168, train_loss: 1.2821\n",
            "97/168, train_loss: 0.4057\n",
            "98/168, train_loss: 0.5168\n",
            "99/168, train_loss: 0.7430\n",
            "100/168, train_loss: 0.1725\n",
            "101/168, train_loss: 0.6704\n",
            "102/168, train_loss: 0.4522\n",
            "103/168, train_loss: 1.1633\n",
            "104/168, train_loss: 0.4441\n",
            "105/168, train_loss: 0.7115\n",
            "106/168, train_loss: 0.4257\n",
            "107/168, train_loss: 0.3197\n",
            "108/168, train_loss: 0.1824\n",
            "109/168, train_loss: 0.3966\n",
            "110/168, train_loss: 0.1309\n",
            "111/168, train_loss: 0.7099\n",
            "112/168, train_loss: 1.0589\n",
            "113/168, train_loss: 0.4132\n",
            "114/168, train_loss: 0.1450\n",
            "115/168, train_loss: 0.0880\n",
            "116/168, train_loss: 0.7081\n",
            "117/168, train_loss: 0.4161\n",
            "118/168, train_loss: 0.4419\n",
            "119/168, train_loss: 0.0606\n",
            "120/168, train_loss: 0.4376\n",
            "121/168, train_loss: 0.7067\n",
            "122/168, train_loss: 0.4452\n",
            "123/168, train_loss: 0.1278\n",
            "124/168, train_loss: 0.1835\n",
            "125/168, train_loss: 0.2551\n",
            "126/168, train_loss: 0.5900\n",
            "127/168, train_loss: 0.7052\n",
            "128/168, train_loss: 0.2746\n",
            "129/168, train_loss: 0.2918\n",
            "130/168, train_loss: 0.4134\n",
            "131/168, train_loss: 0.3998\n",
            "132/168, train_loss: 0.3930\n",
            "133/168, train_loss: 1.2743\n",
            "134/168, train_loss: 0.4037\n",
            "135/168, train_loss: 1.1470\n",
            "136/168, train_loss: 0.3801\n",
            "137/168, train_loss: 0.5426\n",
            "138/168, train_loss: 0.3937\n",
            "139/168, train_loss: 0.4888\n",
            "140/168, train_loss: 0.1293\n",
            "141/168, train_loss: 0.7030\n",
            "142/168, train_loss: 0.7029\n",
            "143/168, train_loss: 0.1725\n",
            "144/168, train_loss: 0.3969\n",
            "145/168, train_loss: 0.3905\n",
            "146/168, train_loss: 0.1418\n",
            "147/168, train_loss: 0.6956\n",
            "148/168, train_loss: 0.4008\n",
            "149/168, train_loss: 0.3619\n",
            "150/168, train_loss: 0.4175\n",
            "151/168, train_loss: 0.6058\n",
            "152/168, train_loss: 1.2128\n",
            "153/168, train_loss: 0.8513\n",
            "154/168, train_loss: 0.2413\n",
            "155/168, train_loss: 0.4711\n",
            "156/168, train_loss: 1.1187\n",
            "157/168, train_loss: 0.2079\n",
            "158/168, train_loss: 1.2637\n",
            "159/168, train_loss: 0.4817\n",
            "160/168, train_loss: 0.2684\n",
            "161/168, train_loss: 0.4094\n",
            "162/168, train_loss: 0.4169\n",
            "163/168, train_loss: 0.8292\n",
            "164/168, train_loss: 0.5051\n",
            "165/168, train_loss: 0.5861\n",
            "166/168, train_loss: 0.5860\n",
            "167/168, train_loss: 0.4487\n",
            "168/168, train_loss: 0.8316\n",
            "169/168, train_loss: 0.8323\n",
            "epoch 10 average loss: 0.5205\n",
            "current epoch: 10 current accuracy: 0.5647 best accuracy: 0.6588 at epoch 6\n",
            "train completed, best_metric: 0.6588 at epoch: 6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}